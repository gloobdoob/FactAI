{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Rating</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>cleaned tokenized</th>\n",
       "      <th>period%</th>\n",
       "      <th>comma%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>question mark%</th>\n",
       "      <th>exclamation mark%</th>\n",
       "      <th>dash%</th>\n",
       "      <th>apostrophe%</th>\n",
       "      <th>close parenthesis%</th>\n",
       "      <th>capitalized%</th>\n",
       "      <th>slang words%</th>\n",
       "      <th>curse words%</th>\n",
       "      <th>with numericals%</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lalaki patay sa pamamaril sa Tondo, Maynila</td>\n",
       "      <td>Real</td>\n",
       "      <td>lalaki patay pamamaril tondo maynila</td>\n",
       "      <td>[lalaki, patay, pamamaril, tondo, ,, maynila]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[lalaki patay, patay pamamaril, pamamaril tond...</td>\n",
       "      <td>[lalaki patay pamamaril, patay pamamaril tondo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50 Pinoy na naipit sa kaguluhan sa Sudan, nail...</td>\n",
       "      <td>Real</td>\n",
       "      <td>50 pinoy naipit kaguluhan sudan nailikas</td>\n",
       "      <td>[50, pinoy, naipit, kaguluhan, sudan, ,, naili...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>[50 pinoy, pinoy naipit, naipit kaguluhan, kag...</td>\n",
       "      <td>[50 pinoy naipit, pinoy naipit kaguluhan, naip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#BoyingResign: Netizens galit kay DOJ Chief Re...</td>\n",
       "      <td>Real</td>\n",
       "      <td>boyingresign netizens galit kay doj chief remu...</td>\n",
       "      <td>[#, boyingresign, :, netizens, galit, kay, doj...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[boyingresign netizens, netizens galit, galit ...</td>\n",
       "      <td>[boyingresign netizens galit, netizens galit k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‘Backdoor entry’ ng Grab sa motorcycle taxi pi...</td>\n",
       "      <td>Real</td>\n",
       "      <td>backdoor entry grab motorcycle taxi pilot pina...</td>\n",
       "      <td>[‘, backdoor, entry, ’, grab, motorcycle, taxi...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[backdoor entry, entry grab, grab motorcycle, ...</td>\n",
       "      <td>[backdoor entry grab, entry grab motorcycle, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doktor nangangamba na na-mild stroke si De Lima</td>\n",
       "      <td>Real</td>\n",
       "      <td>doktor nangangamba mild stroke de</td>\n",
       "      <td>[doktor, nangangamba, na-mild, stroke, de]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[doktor nangangamba, nangangamba mild, mild st...</td>\n",
       "      <td>[doktor nangangamba mild, nangangamba mild str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>Remembering Marky Cielo, Proud Igorot and Star...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>remembering marky cielo proud igorot and stars...</td>\n",
       "      <td>[remembering, marky, cielo, ,, proud, igorot, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[remembering marky, marky cielo, cielo proud, ...</td>\n",
       "      <td>[remembering marky cielo, marky cielo proud, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>Netizen, inilabas ang sekretong galit dahil la...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>netizen inilabas sekretong galit lang my day k...</td>\n",
       "      <td>[netizen, ,, inilabas, sekretong, galit, lang,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[netizen inilabas, inilabas sekretong, sekreto...</td>\n",
       "      <td>[netizen inilabas sekretong, inilabas sekreton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>Pinoy Vlogger na tumulong kay Nas Daily noon, ...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>pinoy vlogger tumulong kay nas daily nagsalita...</td>\n",
       "      <td>[pinoy, vlogger, tumulong, kay, nas, daily, ,,...</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[pinoy vlogger, vlogger tumulong, tumulong kay...</td>\n",
       "      <td>[pinoy vlogger tumulong, vlogger tumulong kay,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>GRABE! UNANG GABI NG LAMAY SA BUROL NI KRIS AQ...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>grabe unang gabi lamay burol kris aquino dinag...</td>\n",
       "      <td>[grabe, !, unang, gabi, lamay, burol, kris, aq...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[grabe unang, unang gabi, gabi lamay, lamay bu...</td>\n",
       "      <td>[grabe unang gabi, unang gabi lamay, gabi lama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>Isang tattoo artist may walong asawa at magka...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>tattoo artist walong asawa magkakasamang nanin...</td>\n",
       "      <td>[tattoo, artist, walong, asawa, magkakasamang,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[tattoo artist, artist walong, walong asawa, a...</td>\n",
       "      <td>[tattoo artist walong, artist walong asawa, wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3452 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Statement Rating  \\\n",
       "0           Lalaki patay sa pamamaril sa Tondo, Maynila   Real   \n",
       "1     50 Pinoy na naipit sa kaguluhan sa Sudan, nail...   Real   \n",
       "2     #BoyingResign: Netizens galit kay DOJ Chief Re...   Real   \n",
       "3     ‘Backdoor entry’ ng Grab sa motorcycle taxi pi...   Real   \n",
       "4       Doktor nangangamba na na-mild stroke si De Lima   Real   \n",
       "...                                                 ...    ...   \n",
       "3447  Remembering Marky Cielo, Proud Igorot and Star...   Fake   \n",
       "3448  Netizen, inilabas ang sekretong galit dahil la...   Fake   \n",
       "3449  Pinoy Vlogger na tumulong kay Nas Daily noon, ...   Fake   \n",
       "3450  GRABE! UNANG GABI NG LAMAY SA BUROL NI KRIS AQ...   Fake   \n",
       "3451   Isang tattoo artist may walong asawa at magka...   Fake   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0                  lalaki patay pamamaril tondo maynila   \n",
       "1              50 pinoy naipit kaguluhan sudan nailikas   \n",
       "2     boyingresign netizens galit kay doj chief remu...   \n",
       "3     backdoor entry grab motorcycle taxi pilot pina...   \n",
       "4                     doktor nangangamba mild stroke de   \n",
       "...                                                 ...   \n",
       "3447  remembering marky cielo proud igorot and stars...   \n",
       "3448  netizen inilabas sekretong galit lang my day k...   \n",
       "3449  pinoy vlogger tumulong kay nas daily nagsalita...   \n",
       "3450  grabe unang gabi lamay burol kris aquino dinag...   \n",
       "3451  tattoo artist walong asawa magkakasamang nanin...   \n",
       "\n",
       "                                      cleaned tokenized   period%    comma%  \\\n",
       "0         [lalaki, patay, pamamaril, tondo, ,, maynila]  0.000000  0.125000   \n",
       "1     [50, pinoy, naipit, kaguluhan, sudan, ,, naili...  0.000000  0.090909   \n",
       "2     [#, boyingresign, :, netizens, galit, kay, doj...  0.000000  0.000000   \n",
       "3     [‘, backdoor, entry, ’, grab, motorcycle, taxi...  0.000000  0.000000   \n",
       "4            [doktor, nangangamba, na-mild, stroke, de]  0.000000  0.000000   \n",
       "...                                                 ...       ...       ...   \n",
       "3447  [remembering, marky, cielo, ,, proud, igorot, ...  0.000000  0.083333   \n",
       "3448  [netizen, ,, inilabas, sekretong, galit, lang,...  0.000000  0.055556   \n",
       "3449  [pinoy, vlogger, tumulong, kay, nas, daily, ,,...  0.041667  0.083333   \n",
       "3450  [grabe, !, unang, gabi, lamay, burol, kris, aq...  0.050000  0.050000   \n",
       "3451  [tattoo, artist, walong, asawa, magkakasamang,...  0.000000  0.000000   \n",
       "\n",
       "        colon%  semicolon%  question mark%  exclamation mark%     dash%  \\\n",
       "0     0.000000    0.000000             0.0           0.000000  0.000000   \n",
       "1     0.000000    0.000000             0.0           0.000000  0.000000   \n",
       "2     0.055556    0.000000             0.0           0.000000  0.055556   \n",
       "3     0.000000    0.000000             0.0           0.000000  0.000000   \n",
       "4     0.000000    0.000000             0.0           0.000000  0.125000   \n",
       "...        ...         ...             ...                ...       ...   \n",
       "3447  0.000000    0.000000             0.0           0.083333  0.000000   \n",
       "3448  0.000000    0.055556             0.0           0.000000  0.000000   \n",
       "3449  0.041667    0.000000             0.0           0.000000  0.000000   \n",
       "3450  0.000000    0.000000             0.0           0.050000  0.050000   \n",
       "3451  0.000000    0.000000             0.0           0.000000  0.000000   \n",
       "\n",
       "      apostrophe%  close parenthesis%  capitalized%  slang words%  \\\n",
       "0             0.0                 0.0      0.000000           0.0   \n",
       "1             0.0                 0.0      0.000000           0.0   \n",
       "2             0.0                 0.0      0.055556           0.0   \n",
       "3             0.0                 0.0      0.000000           0.0   \n",
       "4             0.0                 0.0      0.000000           0.0   \n",
       "...           ...                 ...           ...           ...   \n",
       "3447          0.0                 0.0      0.000000           0.0   \n",
       "3448          0.0                 0.0      0.000000           0.0   \n",
       "3449          0.0                 0.0      0.000000           0.0   \n",
       "3450          0.0                 0.0      0.850000           0.0   \n",
       "3451          0.0                 0.0      0.000000           0.0   \n",
       "\n",
       "      curse words%  with numericals%  \\\n",
       "0              0.0          0.000000   \n",
       "1              0.0          0.090909   \n",
       "2              0.0          0.000000   \n",
       "3              0.0          0.000000   \n",
       "4              0.0          0.000000   \n",
       "...            ...               ...   \n",
       "3447           0.0          0.000000   \n",
       "3448           0.0          0.000000   \n",
       "3449           0.0          0.000000   \n",
       "3450           0.0          0.000000   \n",
       "3451           0.0          0.000000   \n",
       "\n",
       "                                                bigrams  \\\n",
       "0     [lalaki patay, patay pamamaril, pamamaril tond...   \n",
       "1     [50 pinoy, pinoy naipit, naipit kaguluhan, kag...   \n",
       "2     [boyingresign netizens, netizens galit, galit ...   \n",
       "3     [backdoor entry, entry grab, grab motorcycle, ...   \n",
       "4     [doktor nangangamba, nangangamba mild, mild st...   \n",
       "...                                                 ...   \n",
       "3447  [remembering marky, marky cielo, cielo proud, ...   \n",
       "3448  [netizen inilabas, inilabas sekretong, sekreto...   \n",
       "3449  [pinoy vlogger, vlogger tumulong, tumulong kay...   \n",
       "3450  [grabe unang, unang gabi, gabi lamay, lamay bu...   \n",
       "3451  [tattoo artist, artist walong, walong asawa, a...   \n",
       "\n",
       "                                               trigrams  \n",
       "0     [lalaki patay pamamaril, patay pamamaril tondo...  \n",
       "1     [50 pinoy naipit, pinoy naipit kaguluhan, naip...  \n",
       "2     [boyingresign netizens galit, netizens galit k...  \n",
       "3     [backdoor entry grab, entry grab motorcycle, g...  \n",
       "4     [doktor nangangamba mild, nangangamba mild str...  \n",
       "...                                                 ...  \n",
       "3447  [remembering marky cielo, marky cielo proud, c...  \n",
       "3448  [netizen inilabas sekretong, inilabas sekreton...  \n",
       "3449  [pinoy vlogger tumulong, vlogger tumulong kay,...  \n",
       "3450  [grabe unang gabi, unang gabi lamay, gabi lama...  \n",
       "3451  [tattoo artist walong, artist walong asawa, wa...  \n",
       "\n",
       "[3452 rows x 19 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('Tagalog_Headlines_EngFeatures')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['Rating'] = le.fit_transform(df.Rating.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['Statement'].values\n",
    "y = df['Rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Lalaki patay sa pamamaril sa Tondo, Maynila',\n",
       "       '50 Pinoy na naipit sa kaguluhan sa Sudan, nailikas na',\n",
       "       '#BoyingResign: Netizens galit kay DOJ Chief Remulla matapos mahulihan ng ‘high-grade’ marijuana ang anak',\n",
       "       ...,\n",
       "       'Pinoy Vlogger na tumulong kay Nas Daily noon, nagsalita na: Parang ginamit lang ako, naabuso. Hindi ko na sya kilala',\n",
       "       'GRABE! UNANG GABI NG LAMAY SA BUROL NI KRIS AQUINO DINAGSA NG MGA TAO, KAIBIGAN AT KAMAG-ANAK .',\n",
       "       ' Isang tattoo artist may walong asawa at magkakasamang naninirahan sa iisang bubong'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.2, random_state=49, stratify=y)\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 572, 3, 33, 445, 2, 3835, 66, 13, 1, 2, 121, 345],\n",
       " [795, 24, 565, 2, 2238, 3229, 415, 566, 1698, 1],\n",
       " [3179, 3180, 4015, 4, 26, 161, 9, 3725, 4, 2214, 2, 15, 84],\n",
       " [150, 1347, 747, 1, 77, 2, 7, 67, 1, 3237, 152],\n",
       " [634, 3272, 3, 1, 3, 270, 2, 50, 3083],\n",
       " [7, 845, 1, 1287, 4284, 30, 2, 78, 9, 4046],\n",
       " [4885,\n",
       "  415,\n",
       "  4886,\n",
       "  211,\n",
       "  2,\n",
       "  517,\n",
       "  30,\n",
       "  525,\n",
       "  1,\n",
       "  3043,\n",
       "  3044,\n",
       "  1,\n",
       "  2007,\n",
       "  4887,\n",
       "  628,\n",
       "  106],\n",
       " [92, 1695, 4, 2167, 2, 1696, 2, 39, 3223, 331, 55],\n",
       " [1752, 796, 482, 744, 636, 1362, 2, 2, 301, 11, 3274, 2, 226],\n",
       " [27, 1557, 1, 2471, 8, 989, 2472, 3631],\n",
       " [2915, 916, 1, 254, 636, 3046, 3, 5, 1826, 2852],\n",
       " [586,\n",
       "  220,\n",
       "  677,\n",
       "  4,\n",
       "  12,\n",
       "  3,\n",
       "  16,\n",
       "  631,\n",
       "  602,\n",
       "  1343,\n",
       "  95,\n",
       "  18,\n",
       "  1007,\n",
       "  465,\n",
       "  91,\n",
       "  58,\n",
       "  1078],\n",
       " [2070, 2503, 1, 5, 3, 2480, 1, 192, 263, 1242, 1, 212],\n",
       " [1, 955, 2, 357, 4, 568, 357, 163, 3, 1135, 3, 2344, 2, 2138],\n",
       " [212, 3, 4, 1355, 365, 2008, 314, 19, 1239, 98],\n",
       " [7, 583, 403, 11, 3738, 2, 1272, 1, 664, 2, 1943, 4013],\n",
       " [243, 227, 1090, 1, 5, 1089, 3, 45, 1113, 952, 1359, 1],\n",
       " [674, 3, 3403, 2, 1577, 1470, 13, 2956, 4, 2, 800],\n",
       " [842, 9, 19, 110, 316, 2, 135, 1436, 6, 215, 16, 110, 1923],\n",
       " [4, 2, 853, 519, 2976, 1064, 6, 3750, 1, 1, 139, 128],\n",
       " [1039, 3709, 2366, 167, 3, 1, 140, 3],\n",
       " [7, 61, 1010, 2, 32, 1, 4044, 2, 4235, 13, 1, 248, 399],\n",
       " [3497, 1, 74, 648, 2409, 1320, 3, 3498, 2413],\n",
       " [7, 583, 1505, 4, 15, 5, 4036, 1173, 23, 1, 135, 9, 1, 667, 1440, 1, 57, 133],\n",
       " [162, 4, 314, 3, 1937, 1, 4371, 93, 11, 204, 13, 1, 545, 2],\n",
       " [1537, 822, 542, 3, 88, 423],\n",
       " [442, 18, 315, 27, 210, 184, 1, 1142, 571, 258, 2326, 3],\n",
       " [1882, 1467, 1188, 783, 8, 669, 868, 1, 7, 3044, 1, 15, 1127],\n",
       " [42, 54, 1, 591, 1, 792, 152],\n",
       " [688, 86, 1, 780, 65, 1, 1929, 438, 3, 346, 2, 387, 1395, 4, 15, 5, 66, 284],\n",
       " [595, 489, 1108, 2, 252, 4, 825, 3132, 950, 3132, 950],\n",
       " [51, 133, 1, 280, 6, 690, 1309, 1, 349, 198, 40, 13, 1, 186, 172],\n",
       " [4, 197, 3, 2752, 235, 986, 3, 16, 3813, 6, 4, 15, 14, 3, 469],\n",
       " [4376, 1487, 3, 1392, 4, 2596, 1363, 324, 504, 23, 19, 965, 1127],\n",
       " [65, 18, 4138, 79, 1454, 175, 1151, 4, 196, 247, 627],\n",
       " [68, 54, 111, 351, 1, 591, 1, 715, 80],\n",
       " [243, 227, 219, 3, 1, 599, 1338, 4, 3, 393, 339],\n",
       " [977,\n",
       "  101,\n",
       "  41,\n",
       "  24,\n",
       "  233,\n",
       "  2,\n",
       "  761,\n",
       "  347,\n",
       "  25,\n",
       "  736,\n",
       "  2,\n",
       "  3531,\n",
       "  713,\n",
       "  1935,\n",
       "  90,\n",
       "  3604,\n",
       "  17,\n",
       "  18],\n",
       " [12, 2, 439, 3, 2517, 6, 1174, 2, 5, 65, 1226, 4, 61, 337, 2, 843, 2, 77],\n",
       " [40, 351, 1, 2387, 3, 1, 632],\n",
       " [1, 7, 122, 395, 1821, 804, 11, 2, 5],\n",
       " [92, 3410, 3, 2459, 4, 3546, 2, 1738, 2, 37, 6, 3, 23, 1, 1396],\n",
       " [3771, 10, 718, 1, 2342, 3, 396, 4, 12, 347, 3, 85, 457, 457],\n",
       " [16, 14, 3, 6, 1009, 3, 101, 4, 15, 32, 43],\n",
       " [147,\n",
       "  3,\n",
       "  97,\n",
       "  64,\n",
       "  4,\n",
       "  66,\n",
       "  32,\n",
       "  2,\n",
       "  33,\n",
       "  998,\n",
       "  3,\n",
       "  16,\n",
       "  756,\n",
       "  6,\n",
       "  2,\n",
       "  125,\n",
       "  247,\n",
       "  447,\n",
       "  3,\n",
       "  102,\n",
       "  2,\n",
       "  1955],\n",
       " [2782,\n",
       "  508,\n",
       "  1102,\n",
       "  3938,\n",
       "  500,\n",
       "  58,\n",
       "  139,\n",
       "  128,\n",
       "  232,\n",
       "  22,\n",
       "  540,\n",
       "  253,\n",
       "  2782,\n",
       "  435,\n",
       "  2639,\n",
       "  1979,\n",
       "  253,\n",
       "  2955,\n",
       "  4063,\n",
       "  548,\n",
       "  2782,\n",
       "  255,\n",
       "  1725,\n",
       "  170,\n",
       "  154],\n",
       " [48, 24, 4402, 4402, 1203, 2, 3947, 13, 1, 248, 873, 325, 2, 7, 1079],\n",
       " [4053, 322, 3903, 109, 93, 403, 4, 1450, 1, 120, 668, 2, 3737, 1, 1924],\n",
       " [42, 348, 209, 3, 4006, 4, 662, 1, 15, 2625],\n",
       " [1164, 246, 767, 854, 4, 4113, 4113, 1234, 67, 1, 1488],\n",
       " [1559, 2, 3012, 10, 3403, 97, 64, 1, 1671],\n",
       " [618, 2291, 1, 80, 1689, 534, 1698, 2, 692, 313, 3, 1, 193],\n",
       " [25, 125, 874, 20, 1, 659, 4, 7, 984, 601, 681],\n",
       " [4, 8, 82, 2, 1, 2236, 2, 885, 8, 4214, 4215, 3, 16, 1499, 756],\n",
       " [42, 1, 114, 106, 11, 204, 4, 150, 3, 1042, 166, 140],\n",
       " [95, 1183, 2, 7, 94, 3, 1, 142],\n",
       " [227, 1, 599, 185, 393, 339, 799, 1, 377, 297, 2, 37],\n",
       " [2767, 320, 3, 16, 3600, 3601, 987, 2739, 66, 1091],\n",
       " [293,\n",
       "  70,\n",
       "  1966,\n",
       "  81,\n",
       "  1581,\n",
       "  1967,\n",
       "  4105,\n",
       "  21,\n",
       "  283,\n",
       "  1424,\n",
       "  49,\n",
       "  1199,\n",
       "  511,\n",
       "  1750,\n",
       "  2445,\n",
       "  3119,\n",
       "  283,\n",
       "  22,\n",
       "  2566,\n",
       "  928,\n",
       "  253,\n",
       "  666,\n",
       "  2413,\n",
       "  49,\n",
       "  1018,\n",
       "  1424,\n",
       "  666,\n",
       "  335,\n",
       "  3877,\n",
       "  91,\n",
       "  4105,\n",
       "  49,\n",
       "  441,\n",
       "  319,\n",
       "  1424,\n",
       "  2142],\n",
       " [36, 101, 4, 5, 203, 41, 123, 112, 503, 4201, 2, 67, 4, 25, 125],\n",
       " [239, 2, 408, 2, 179, 266],\n",
       " [60, 2677, 2, 1836, 1, 7, 100, 472, 7, 2802, 6, 2562, 4, 478],\n",
       " [430, 3167, 411, 2274, 2108, 922, 3225, 2102, 193, 306, 2, 3226, 55, 497],\n",
       " [7, 462, 4109, 2, 4110, 6, 1661, 578, 1, 15],\n",
       " [4, 119, 6, 9, 1, 26, 2713, 43, 1, 834, 207],\n",
       " [147,\n",
       "  97,\n",
       "  64,\n",
       "  4,\n",
       "  295,\n",
       "  3,\n",
       "  177,\n",
       "  19,\n",
       "  1598,\n",
       "  1667,\n",
       "  358,\n",
       "  217,\n",
       "  4,\n",
       "  366,\n",
       "  177,\n",
       "  1,\n",
       "  32,\n",
       "  8,\n",
       "  362,\n",
       "  989],\n",
       " [1825, 915, 2, 62, 665, 1, 39, 1430, 2, 486],\n",
       " [498, 2, 299, 2, 648, 1, 355, 80],\n",
       " [506, 3, 2570, 2, 60, 345, 1, 15, 167, 3, 655, 501, 870, 1, 5, 34],\n",
       " [1709, 500, 8, 241, 918, 3, 3498, 24, 314, 2, 40, 554, 1937, 29, 215],\n",
       " [7, 464, 271, 272, 625, 1, 2753, 146, 2, 1298, 14, 2680, 3463, 18],\n",
       " [214, 6, 217, 41, 1277, 4, 5, 66, 76, 1, 32, 8, 2774],\n",
       " [89, 4819, 4820, 4821, 929, 4822, 1, 296, 2050, 8, 92],\n",
       " [28, 129, 341, 3, 94, 1172, 3, 769, 4, 3836, 23, 17, 527, 173],\n",
       " [286, 31, 3, 65, 325, 11, 1165, 4, 369, 3, 1, 26, 149, 100, 969],\n",
       " [3968, 8, 1333, 2701, 101, 2, 7, 36, 10, 5, 261, 3, 957, 4, 117, 273],\n",
       " [857, 66, 582, 11, 204, 1913, 2, 7, 205, 116, 2, 15, 131],\n",
       " [2, 1585, 2, 455, 1, 388, 543],\n",
       " [5, 371, 1, 142, 2239, 203, 2, 384, 1, 39],\n",
       " [323, 2, 1300, 6, 10, 1482, 1, 245, 231, 19, 721, 1928, 3, 483, 81, 58],\n",
       " [383, 1354, 411, 1047, 1690, 1373, 1, 911, 116, 2],\n",
       " [413, 250, 5, 1152, 1, 45, 45],\n",
       " [993, 3813, 6, 631, 2558, 30, 2, 5, 36, 1, 5, 12, 8, 195, 3, 1145, 2559, 6],\n",
       " [440, 124, 1463, 16, 110, 1183, 2, 7, 36, 9, 1, 120, 674, 3, 507, 2, 84],\n",
       " [893, 1, 801, 2, 222, 265, 14, 3, 1377, 470, 4, 2063],\n",
       " [33, 866, 2812, 1, 654, 2, 2320, 1014, 287, 278, 1, 4380, 834, 207],\n",
       " [3880,\n",
       "  31,\n",
       "  3,\n",
       "  110,\n",
       "  854,\n",
       "  4,\n",
       "  1420,\n",
       "  2,\n",
       "  387,\n",
       "  1846,\n",
       "  3881,\n",
       "  3882,\n",
       "  1903,\n",
       "  996,\n",
       "  70,\n",
       "  608],\n",
       " [181, 3706, 328, 2, 192, 263, 1, 223, 3],\n",
       " [1467,\n",
       "  3935,\n",
       "  3936,\n",
       "  4352,\n",
       "  1,\n",
       "  1342,\n",
       "  13,\n",
       "  1,\n",
       "  3996,\n",
       "  3,\n",
       "  4411,\n",
       "  1,\n",
       "  132,\n",
       "  17,\n",
       "  4,\n",
       "  4360,\n",
       "  117,\n",
       "  18,\n",
       "  4,\n",
       "  1249,\n",
       "  117],\n",
       " [208,\n",
       "  2,\n",
       "  52,\n",
       "  1773,\n",
       "  6,\n",
       "  2803,\n",
       "  1,\n",
       "  73,\n",
       "  191,\n",
       "  963,\n",
       "  3,\n",
       "  14,\n",
       "  1246,\n",
       "  64,\n",
       "  1957,\n",
       "  4,\n",
       "  208,\n",
       "  95],\n",
       " [123, 229, 117, 4299, 2, 7, 3230, 205, 4, 203],\n",
       " [27, 733, 3, 3060, 821, 4918, 4919, 1224, 41, 4920, 4921, 548, 522],\n",
       " [2193, 3722, 519, 3723, 6, 3724, 1, 2, 834, 768],\n",
       " [4984, 8, 3042, 2148, 1, 5, 447, 3023, 519, 1333, 635],\n",
       " [2578, 3, 8, 33, 1598, 1667, 358, 6, 240, 16, 1154, 1, 245],\n",
       " [442,\n",
       "  18,\n",
       "  941,\n",
       "  92,\n",
       "  6,\n",
       "  1479,\n",
       "  3788,\n",
       "  4,\n",
       "  251,\n",
       "  495,\n",
       "  257,\n",
       "  410,\n",
       "  8,\n",
       "  1578,\n",
       "  343,\n",
       "  2,\n",
       "  1755],\n",
       " [583, 1, 1107, 3, 33, 2530, 6, 103, 3, 88, 3998],\n",
       " [42, 4, 2, 39, 1, 389, 381],\n",
       " [604, 8, 847, 3832, 8, 10, 269, 3370, 2004, 7, 45, 11, 81],\n",
       " [2111, 2112, 3, 684, 30, 1, 384, 8, 720, 549, 54, 1, 1035, 80],\n",
       " [25, 125, 2732, 2, 34, 11, 3814, 2, 1143, 6, 4, 3, 2659, 1, 849],\n",
       " [635, 404, 3, 677, 4, 26, 1253, 8, 669, 868, 1, 5, 3478, 2],\n",
       " [36, 514, 862, 1, 7, 2745, 370, 86, 3, 2299, 6, 2746, 2, 267, 1, 15, 579],\n",
       " [36, 352, 1, 676, 11, 2250, 4, 402, 14, 175, 2, 390, 159],\n",
       " [4675, 4676, 2931, 2039, 103, 20, 1585, 85, 4677],\n",
       " [3677, 913, 79, 1, 1072, 80],\n",
       " [320,\n",
       "  3,\n",
       "  16,\n",
       "  1500,\n",
       "  1501,\n",
       "  24,\n",
       "  1,\n",
       "  1138,\n",
       "  2,\n",
       "  701,\n",
       "  2,\n",
       "  557,\n",
       "  29,\n",
       "  3536,\n",
       "  3,\n",
       "  16,\n",
       "  773,\n",
       "  774],\n",
       " [2491, 2492, 14, 16, 254, 178, 350, 98],\n",
       " [2052, 2, 1589, 1, 715, 80, 100, 3, 46],\n",
       " [186, 1685, 909, 56, 2825, 108, 164, 1427, 737, 1, 37],\n",
       " [1211, 3, 127, 2, 9, 1, 2205, 3, 294, 1010, 2, 32, 11, 4122],\n",
       " [7, 400, 23, 1, 33, 662, 3, 2107, 159, 2, 14, 1302, 4],\n",
       " [60, 24, 1282, 2, 1960, 1, 662, 1, 141, 11, 18, 4, 89, 182],\n",
       " [76, 1, 618, 10, 2626, 3905, 2, 131, 6, 1490, 3, 322, 1914, 2407, 1, 5, 34],\n",
       " [857, 855, 2, 14, 264, 3, 5, 1, 1287],\n",
       " [70, 1081, 736, 578, 21, 81, 283, 1007, 1001, 2518, 3875, 511, 1196, 667],\n",
       " [590,\n",
       "  97,\n",
       "  64,\n",
       "  16,\n",
       "  4214,\n",
       "  4215,\n",
       "  6,\n",
       "  123,\n",
       "  32,\n",
       "  17,\n",
       "  46,\n",
       "  75,\n",
       "  1,\n",
       "  2598,\n",
       "  2,\n",
       "  245,\n",
       "  90,\n",
       "  2481,\n",
       "  159,\n",
       "  3,\n",
       "  11,\n",
       "  167],\n",
       " [442, 18, 1, 1503, 1479, 1930, 3, 571, 258, 556, 1656, 2690, 1118, 3, 184],\n",
       " [257, 31, 3, 76, 1, 3603, 549, 54, 6, 10, 5, 130, 28, 48, 76, 46],\n",
       " [2882, 4579],\n",
       " [186, 172, 103, 20, 227],\n",
       " [96, 6, 4, 176, 323, 18, 2, 1565, 2727, 18],\n",
       " [4043, 1862, 951, 3890, 2, 649, 1, 849, 99, 2, 1565, 69, 1, 5, 808],\n",
       " [5,\n",
       "  1539,\n",
       "  1,\n",
       "  1540,\n",
       "  1245,\n",
       "  606,\n",
       "  2,\n",
       "  267,\n",
       "  4525,\n",
       "  607,\n",
       "  2013,\n",
       "  1246,\n",
       "  3,\n",
       "  124,\n",
       "  4526,\n",
       "  3,\n",
       "  18,\n",
       "  124],\n",
       " [796,\n",
       "  482,\n",
       "  4790,\n",
       "  4791,\n",
       "  337,\n",
       "  2,\n",
       "  927,\n",
       "  1297,\n",
       "  1,\n",
       "  4792,\n",
       "  1298,\n",
       "  1612,\n",
       "  2,\n",
       "  464,\n",
       "  271,\n",
       "  272,\n",
       "  625],\n",
       " [1622, 4, 2, 160, 3, 5, 1576, 1, 228, 138, 35, 2322, 1, 22, 1125, 650],\n",
       " [2403, 2404, 19, 3727, 81, 2504, 3728, 602, 206, 2505],\n",
       " [1607, 163, 1806, 352, 2475, 10, 1065, 199, 1, 525, 3, 600, 89, 449, 302],\n",
       " [12, 2, 7, 943, 6, 277, 346, 35, 464, 271, 272],\n",
       " [12, 2, 1912, 1829, 4, 15, 172, 1504, 75, 2, 26, 161],\n",
       " [52, 10, 3, 1, 529, 1200, 4, 116, 2, 2630, 4, 3422, 125],\n",
       " [25, 172, 582, 2751, 1, 292, 1972, 2, 69, 2, 7, 36],\n",
       " [113,\n",
       "  3,\n",
       "  2290,\n",
       "  2,\n",
       "  131,\n",
       "  1,\n",
       "  5,\n",
       "  651,\n",
       "  113,\n",
       "  652,\n",
       "  2,\n",
       "  237,\n",
       "  1,\n",
       "  568,\n",
       "  2962,\n",
       "  1,\n",
       "  632,\n",
       "  80,\n",
       "  695],\n",
       " [3817, 275, 62, 3, 2, 828, 1, 262],\n",
       " [277, 2217, 35, 169, 2, 827, 4298, 4, 7, 1966],\n",
       " [635, 404, 1747, 16, 586, 35, 208, 581, 4, 6, 1, 659, 721, 1, 1156],\n",
       " [362, 112, 2, 5, 4, 112, 970, 3, 1553, 3, 3341],\n",
       " [249, 193, 306, 1, 2294, 55, 1360, 3271, 2, 430, 1060],\n",
       " [1491, 4, 757, 758, 370, 3, 3114, 1, 2499, 1, 805, 3957],\n",
       " [195,\n",
       "  1460,\n",
       "  1,\n",
       "  1513,\n",
       "  8,\n",
       "  884,\n",
       "  3,\n",
       "  896,\n",
       "  2,\n",
       "  242,\n",
       "  4,\n",
       "  84,\n",
       "  6,\n",
       "  85,\n",
       "  1462,\n",
       "  2778,\n",
       "  588,\n",
       "  108],\n",
       " [33, 4071, 991, 8, 669, 868, 3, 16, 47, 3, 102, 4, 15, 32, 43],\n",
       " [28, 304, 1763, 2, 600, 2866, 211, 2, 925, 517, 205, 3, 977, 2, 130, 106, 46],\n",
       " [631, 2676, 4, 4241, 231, 1, 1600, 1234, 1140, 1, 172, 16, 586, 220],\n",
       " [3193, 3194, 2, 160, 1370, 3, 113, 1, 535, 536],\n",
       " [2161, 3, 396, 3, 449, 449, 4, 2675, 856, 545, 3, 1496, 10, 340],\n",
       " [2900, 255, 309],\n",
       " [25, 240, 1, 956, 99, 3508, 2, 50, 67, 1, 5, 808, 73, 1, 139, 128],\n",
       " [181, 920, 3, 4, 2155, 1, 5, 371, 3, 185, 1815, 1, 1093, 3, 279],\n",
       " [52, 2010, 2, 65, 6, 1678, 2, 1, 3404, 1, 2358, 3405, 106],\n",
       " [51, 498, 2, 221, 1, 251, 3, 3578, 4, 489, 1469, 2, 5, 1447],\n",
       " [76, 2228, 1, 640, 13, 1, 248, 354, 2, 6, 26, 2, 1976, 1609],\n",
       " [996,\n",
       "  58,\n",
       "  710,\n",
       "  10,\n",
       "  314,\n",
       "  3,\n",
       "  19,\n",
       "  1442,\n",
       "  3,\n",
       "  229,\n",
       "  1262,\n",
       "  4,\n",
       "  402,\n",
       "  1756,\n",
       "  126,\n",
       "  23,\n",
       "  1,\n",
       "  707],\n",
       " [447, 2608, 1, 15, 1385, 11, 1724, 4, 402, 3761, 107, 82],\n",
       " [12, 2, 1912, 6, 2531, 372, 2, 1488, 1407],\n",
       " [176, 2, 537, 9, 1319, 2, 4372, 2765, 2, 131, 2, 1110],\n",
       " [241, 490, 399, 3, 30, 1, 964, 3, 1, 3390, 2, 327],\n",
       " [2137, 3440, 2, 357],\n",
       " [2, 327, 1, 2, 37, 3, 4, 200, 418, 812, 38, 3160, 44, 2235],\n",
       " [397, 1030, 225, 839, 134, 1425, 2179, 778, 2547, 2729, 3, 268, 23, 1, 7, 53],\n",
       " [3, 24, 565, 2, 322, 1151, 90, 1772, 17, 2, 353, 2, 51, 182],\n",
       " [2271, 733, 2, 456, 1, 15, 296, 3247],\n",
       " [28, 1, 111, 448, 3235, 2278, 3, 2, 210],\n",
       " [53, 4131, 1376, 1, 7, 1, 392, 7, 45, 144, 4, 121, 649],\n",
       " [2710,\n",
       "  2711,\n",
       "  10,\n",
       "  1253,\n",
       "  19,\n",
       "  515,\n",
       "  38,\n",
       "  1,\n",
       "  3450,\n",
       "  4,\n",
       "  5,\n",
       "  12,\n",
       "  117,\n",
       "  108,\n",
       "  17,\n",
       "  987,\n",
       "  987,\n",
       "  1876],\n",
       " [169, 145, 2241, 2],\n",
       " [9, 1, 610, 28, 72, 1674, 1, 571, 2318, 3402, 3],\n",
       " [51, 1, 2304, 1, 5, 845],\n",
       " [349, 198, 40, 799, 17, 46, 1, 297, 2, 280, 902, 2008, 1243, 1, 51, 1032],\n",
       " [259, 2, 25, 401, 587, 1, 1927, 2, 292, 4, 142, 2, 5, 34],\n",
       " [4307, 18, 102, 2577, 28, 129, 6, 4, 4312, 366, 1475],\n",
       " [4625, 1, 5, 56, 2907, 1, 532, 1261, 251, 2908, 2, 4626],\n",
       " [27, 2043, 2, 2893, 4605, 4606, 3, 4607, 1, 702],\n",
       " [523, 3, 2563, 9, 1, 649, 2, 7, 3819, 224, 173, 467, 1, 568, 45, 2, 109],\n",
       " [3, 2077, 2, 813, 1, 1735, 2, 1287, 221, 2, 69, 99, 1771, 3, 1, 15, 37],\n",
       " [2046, 4614, 4, 705, 2898, 2, 5, 168, 1, 331, 4615],\n",
       " [120, 42, 1918, 6, 2292, 4, 15, 1214, 3622, 1, 594, 2, 1049, 1159],\n",
       " [405, 2, 201, 928, 1, 736, 613, 3, 8, 143, 27],\n",
       " [52, 1, 7, 391, 378, 4, 392, 59],\n",
       " [13, 274, 8, 977, 3, 4, 761, 59],\n",
       " [954,\n",
       "  29,\n",
       "  605,\n",
       "  814,\n",
       "  2639,\n",
       "  650,\n",
       "  238,\n",
       "  608,\n",
       "  1919,\n",
       "  293,\n",
       "  174,\n",
       "  666,\n",
       "  3925,\n",
       "  148,\n",
       "  3926,\n",
       "  22,\n",
       "  2318,\n",
       "  608,\n",
       "  2467,\n",
       "  508,\n",
       "  134,\n",
       "  285,\n",
       "  1489,\n",
       "  72,\n",
       "  63,\n",
       "  253,\n",
       "  666,\n",
       "  335,\n",
       "  511,\n",
       "  2432,\n",
       "  730,\n",
       "  49,\n",
       "  335,\n",
       "  2640,\n",
       "  508,\n",
       "  134,\n",
       "  285,\n",
       "  2640,\n",
       "  425,\n",
       "  148,\n",
       "  134,\n",
       "  29,\n",
       "  3927,\n",
       "  157,\n",
       "  879,\n",
       "  29,\n",
       "  879,\n",
       "  745,\n",
       "  756,\n",
       "  2568,\n",
       "  1713,\n",
       "  87,\n",
       "  183,\n",
       "  151,\n",
       "  348,\n",
       "  497,\n",
       "  44,\n",
       "  257,\n",
       "  104],\n",
       " [583, 1, 716, 101, 4, 1495, 41, 123, 56, 986, 4, 15],\n",
       " [363,\n",
       "  4102,\n",
       "  375,\n",
       "  162,\n",
       "  3,\n",
       "  10,\n",
       "  3,\n",
       "  4,\n",
       "  32,\n",
       "  24,\n",
       "  565,\n",
       "  2,\n",
       "  875,\n",
       "  1,\n",
       "  15,\n",
       "  338,\n",
       "  2696,\n",
       "  1,\n",
       "  984],\n",
       " [825, 1028, 3, 38, 3512, 1, 262, 3030, 3],\n",
       " [1533, 353, 2920, 4659, 6, 1577, 2921, 3, 9, 4660, 5, 328, 8, 4661],\n",
       " [997, 2537, 6, 1867, 99, 1771, 1, 1032, 2539, 2, 273, 562],\n",
       " [3170, 3272, 3, 1, 5, 1126, 1, 1069, 1070, 3273, 155, 1, 87],\n",
       " [2783,\n",
       "  174,\n",
       "  881,\n",
       "  58,\n",
       "  22,\n",
       "  1846,\n",
       "  29,\n",
       "  33,\n",
       "  4272,\n",
       "  70,\n",
       "  1211,\n",
       "  1327,\n",
       "  253,\n",
       "  4273,\n",
       "  209,\n",
       "  1841,\n",
       "  1393],\n",
       " [468, 169, 2, 3458, 122, 141, 17, 54, 1, 1229, 1, 2388, 3459],\n",
       " [183, 2293, 3661, 127, 2, 1111, 278, 1, 114, 2086, 40, 2035, 2367],\n",
       " [819, 1595, 1289, 3, 2, 210, 9, 806, 4, 83, 269, 118, 2, 188, 1, 254, 178],\n",
       " [3195, 23, 2324, 3, 88, 3248],\n",
       " [1590, 959, 10, 697, 9, 3702, 4, 5, 1086],\n",
       " [3757, 3758, 578, 21, 1853, 1178, 1260, 81, 22, 2524, 21, 174],\n",
       " [5,\n",
       "  153,\n",
       "  2,\n",
       "  7,\n",
       "  129,\n",
       "  341,\n",
       "  3,\n",
       "  94,\n",
       "  2,\n",
       "  81,\n",
       "  1282,\n",
       "  4,\n",
       "  59,\n",
       "  1,\n",
       "  150,\n",
       "  318,\n",
       "  6,\n",
       "  103,\n",
       "  56,\n",
       "  1385],\n",
       " [710, 1, 5, 133, 3, 2164, 1, 3059, 4917, 3, 546, 1, 459, 433],\n",
       " [3, 1406, 108, 3920, 3, 125, 6, 61, 4, 366, 404, 1, 748, 2, 3704],\n",
       " [893, 801, 2, 222, 265, 412, 1025, 17, 46],\n",
       " [1862,\n",
       "  951,\n",
       "  11,\n",
       "  4,\n",
       "  26,\n",
       "  4212,\n",
       "  6,\n",
       "  26,\n",
       "  2,\n",
       "  3192,\n",
       "  1,\n",
       "  1954,\n",
       "  1823,\n",
       "  3,\n",
       "  46,\n",
       "  4,\n",
       "  3468,\n",
       "  2914,\n",
       "  1,\n",
       "  688],\n",
       " [60,\n",
       "  2,\n",
       "  584,\n",
       "  180,\n",
       "  761,\n",
       "  3681,\n",
       "  2,\n",
       "  3992,\n",
       "  2477,\n",
       "  743,\n",
       "  2,\n",
       "  180,\n",
       "  761,\n",
       "  3,\n",
       "  2724,\n",
       "  35,\n",
       "  144,\n",
       "  47,\n",
       "  3965,\n",
       "  32],\n",
       " [225, 494, 1, 19, 3514, 35, 1652, 2, 181],\n",
       " [2176, 3072, 358, 50, 74, 163],\n",
       " [138, 1121, 733, 746, 2125, 1691],\n",
       " [889,\n",
       "  2,\n",
       "  4507,\n",
       "  6,\n",
       "  4508,\n",
       "  2005,\n",
       "  2845,\n",
       "  4509,\n",
       "  4510,\n",
       "  3,\n",
       "  4511,\n",
       "  2,\n",
       "  2846,\n",
       "  2,\n",
       "  2006,\n",
       "  1,\n",
       "  37],\n",
       " [3307, 1, 2309, 14, 1, 1254, 11, 2, 5, 113, 106, 1, 520, 504],\n",
       " [4, 1495, 8, 2495, 1961, 217, 4, 366, 177, 1, 15, 32],\n",
       " [431, 160, 1757, 3, 4, 2322, 1, 450, 6, 1076, 2085],\n",
       " [597, 281, 3099, 4, 3412, 2, 5, 3413, 688],\n",
       " [3048, 2122, 55, 1, 2157, 104, 62, 274, 2158, 115, 1, 72, 63],\n",
       " [454, 3, 139, 3622, 1, 808, 3, 2204, 10, 697],\n",
       " [1698, 2, 223, 192, 263, 185, 1383, 2, 331, 1305],\n",
       " [1801, 2, 2195, 365, 3610, 1, 3611],\n",
       " [89, 3985, 3, 3977, 8, 2661, 2662, 235, 939, 177, 97, 64],\n",
       " [1127, 3609, 1052, 1, 936, 2395, 2268],\n",
       " [7, 120, 4, 5, 1512, 1, 568, 469, 325, 2, 5, 34],\n",
       " [290, 438, 103, 20, 1082, 3, 4093, 1202],\n",
       " [42, 54, 20, 3279, 2, 1323, 1, 716],\n",
       " [520, 3, 10, 2870, 3, 44, 1320],\n",
       " [68, 831, 2, 925, 3024, 3, 3586, 525, 1, 389],\n",
       " [28, 1038, 1, 553, 80, 6, 805, 1, 3653, 193, 306, 1, 3226, 55, 497],\n",
       " [180, 359, 163, 1569, 411, 1047, 167, 3, 1, 140, 915],\n",
       " [1083, 521, 264, 4, 3486, 2, 189, 9, 1, 1125, 650],\n",
       " [1227, 899, 2, 1532, 3, 3, 806, 4, 384, 2, 2501, 16, 1664],\n",
       " [61, 2, 130, 1, 1547, 1794, 54, 11, 1232, 1, 2424],\n",
       " [61,\n",
       "  10,\n",
       "  56,\n",
       "  1462,\n",
       "  19,\n",
       "  198,\n",
       "  28,\n",
       "  1,\n",
       "  365,\n",
       "  108,\n",
       "  342,\n",
       "  3827,\n",
       "  2,\n",
       "  769,\n",
       "  9,\n",
       "  1,\n",
       "  135,\n",
       "  12],\n",
       " [136, 1, 4160, 2596, 4161, 2, 7, 50, 392, 13, 1, 4162],\n",
       " [376, 4, 1, 181, 11, 3117, 35],\n",
       " [14, 2937, 222, 2068, 919, 83, 4689, 2069, 17, 46],\n",
       " [2914, 1, 355, 80, 4644, 2, 321, 86, 13, 146, 1, 1048],\n",
       " [308, 3, 4218, 2, 3, 10, 14, 2, 12, 13, 14, 270, 4219],\n",
       " [1156, 1790, 1112, 27, 2, 431, 312, 4, 1054, 155],\n",
       " [7, 96, 2655, 4, 3366, 231, 1, 688, 86, 3, 107],\n",
       " [1314, 2144, 198, 2145, 1315, 4870, 2, 322, 3036, 1316],\n",
       " [11, 4, 958, 209, 2643, 4037, 3, 97, 64, 1, 1948, 1949],\n",
       " [590, 97, 64, 16, 4244, 1, 366, 32, 6, 147, 4, 3, 1210, 2, 3860, 197, 3, 320],\n",
       " [805, 1259, 80, 169, 3623, 50, 228, 138, 2210, 368],\n",
       " [1, 45, 2, 15, 421, 207, 4066, 677, 4, 1832, 81, 1904],\n",
       " [52, 6, 3615, 1, 955, 2, 54, 1, 591, 1, 715],\n",
       " [644, 745, 1304, 2032, 701, 3, 806, 4, 384, 1, 1344, 246, 13, 30, 1, 619],\n",
       " [1111, 31, 3, 53, 994, 1019, 99, 3827, 2, 4094, 6, 770, 2, 12, 3, 3, 1023],\n",
       " [86, 24, 1660, 2, 353, 424, 35, 1801, 11, 1416, 1, 4380, 834, 207],\n",
       " [719, 3, 596, 35, 2],\n",
       " [50, 294, 1, 80, 311, 2, 5, 4150, 211, 2, 5, 2078, 6, 1120, 35, 400],\n",
       " [3425, 2, 5, 50, 704, 1362, 3, 2, 209, 44, 357],\n",
       " [3, 4, 1, 205, 136, 1, 1397, 435, 4154, 941, 4, 580, 20, 53, 347],\n",
       " [256, 2457, 24, 602, 19, 332, 333, 1419, 861, 38, 4, 73, 657],\n",
       " [3473, 2466, 1, 2419, 3, 4, 26, 3651, 1, 1836, 2, 2858],\n",
       " [326, 2, 715, 80, 1751, 1250, 1421, 3, 1, 171],\n",
       " [60, 1476, 1, 121, 568, 294, 4090, 97, 64, 2, 1804, 344, 662],\n",
       " [447, 437, 718, 1, 2916, 3, 1167, 2, 149, 1448],\n",
       " [1939, 31, 3, 137, 219, 3, 1019, 1, 1484, 2, 1943, 9, 1, 15, 84],\n",
       " [1481, 6, 3856, 140, 2081, 6, 2229, 4062, 1, 100, 33],\n",
       " [463, 3863, 1736, 147, 97, 64, 4, 6, 123, 229, 864],\n",
       " [1412, 2, 4127, 277, 66, 1452, 1, 69, 2, 25, 125],\n",
       " [172, 2731, 4, 77, 1, 1218, 4158, 2, 12, 116, 2, 446, 20, 474, 4, 509],\n",
       " [7, 760, 31, 3, 230, 23, 1, 1752, 305, 2, 55, 416, 1, 190],\n",
       " [12, 3, 14, 4, 1490, 6, 1932, 2, 61, 1, 15, 660, 603, 2, 5, 880],\n",
       " [181, 2055, 2, 160, 40, 1272, 3, 118, 2, 2915, 235, 1573],\n",
       " [202, 953, 1, 382, 6, 622, 11, 4, 89, 170, 3, 226],\n",
       " [2644, 3, 1002, 2, 7, 162, 11, 2, 109, 93, 4, 15, 314],\n",
       " [30, 54, 11, 1, 5, 1826, 1, 2453],\n",
       " [76, 3264, 1, 7, 532, 1, 563, 68, 94, 1236, 1, 55, 3, 544],\n",
       " [5, 3261, 406, 1, 751, 1691, 2, 210],\n",
       " [109, 93, 17, 46, 1, 1807, 2, 797, 704, 79, 47, 1, 1561, 2, 7, 859, 1],\n",
       " [48, 4020, 5, 1207, 1, 15, 852, 3, 5, 153],\n",
       " [28, 168, 316, 8, 1675, 1112, 2, 3164, 1, 535],\n",
       " [42, 1894, 4, 390, 119, 9, 127, 2, 837, 4, 240, 369, 1, 1034],\n",
       " [47, 3, 102, 4, 32, 8, 43, 4, 33, 197, 3, 877, 3, 507, 4, 26, 4404],\n",
       " [4350, 729, 3, 4351, 4352, 11, 483, 4353, 4354, 4, 15, 7, 12],\n",
       " [551, 31, 3, 2175, 617, 739, 2641, 9, 3928, 2021],\n",
       " [1833, 3144, 2, 3, 406, 1, 1121, 1, 296, 7, 2, 1347],\n",
       " [1276, 206, 1352, 197, 85, 97, 108, 141, 4, 1, 117],\n",
       " [36, 101, 4, 1905, 3, 585, 2, 48, 582, 26, 105, 66, 2555, 95, 116, 2, 1825],\n",
       " [3718, 2, 423, 379, 1, 5, 1033, 3268, 3598, 1537],\n",
       " [50,\n",
       "  144,\n",
       "  3420,\n",
       "  1928,\n",
       "  3,\n",
       "  79,\n",
       "  1496,\n",
       "  3952,\n",
       "  661,\n",
       "  3953,\n",
       "  78,\n",
       "  380,\n",
       "  206,\n",
       "  988,\n",
       "  894,\n",
       "  502,\n",
       "  91,\n",
       "  49,\n",
       "  851,\n",
       "  91,\n",
       "  3805,\n",
       "  894,\n",
       "  502,\n",
       "  2623,\n",
       "  2650,\n",
       "  3954,\n",
       "  386,\n",
       "  1868,\n",
       "  2371,\n",
       "  3955,\n",
       "  425,\n",
       "  1001,\n",
       "  335,\n",
       "  22,\n",
       "  650,\n",
       "  21,\n",
       "  3373,\n",
       "  3954,\n",
       "  1180,\n",
       "  148,\n",
       "  148,\n",
       "  2650,\n",
       "  2639,\n",
       "  881,\n",
       "  148,\n",
       "  894,\n",
       "  502,\n",
       "  124,\n",
       "  3956,\n",
       "  894,\n",
       "  502,\n",
       "  3956],\n",
       " [98, 1, 210],\n",
       " [104, 78, 1539, 1, 3280, 91, 3281, 1, 553],\n",
       " [74,\n",
       "  24,\n",
       "  1525,\n",
       "  1273,\n",
       "  179,\n",
       "  3186,\n",
       "  3,\n",
       "  113,\n",
       "  9,\n",
       "  81,\n",
       "  4,\n",
       "  5,\n",
       "  532,\n",
       "  6,\n",
       "  5,\n",
       "  133,\n",
       "  3,\n",
       "  328,\n",
       "  2,\n",
       "  1760,\n",
       "  1,\n",
       "  1107],\n",
       " [155, 896, 2, 5, 1662, 4, 4, 3348, 1, 3240],\n",
       " [1111,\n",
       "  3,\n",
       "  133,\n",
       "  3,\n",
       "  328,\n",
       "  2,\n",
       "  192,\n",
       "  263,\n",
       "  1105,\n",
       "  2,\n",
       "  179,\n",
       "  3159,\n",
       "  193,\n",
       "  3,\n",
       "  69,\n",
       "  23,\n",
       "  1,\n",
       "  826,\n",
       "  6,\n",
       "  3036,\n",
       "  2846],\n",
       " [4, 1833, 871, 2, 197, 3, 320, 82, 3, 16, 3267, 3, 2, 15, 53],\n",
       " [7, 28, 31, 3, 94, 54, 11, 4, 1, 653, 2562],\n",
       " [1537, 10, 353, 354, 1, 5, 2853, 2854, 3, 284, 1, 1033],\n",
       " [4286,\n",
       "  3,\n",
       "  4286,\n",
       "  3,\n",
       "  259,\n",
       "  30,\n",
       "  8,\n",
       "  1472,\n",
       "  2473,\n",
       "  242,\n",
       "  4,\n",
       "  651,\n",
       "  42,\n",
       "  1638,\n",
       "  3,\n",
       "  1638,\n",
       "  1,\n",
       "  139,\n",
       "  128],\n",
       " [12, 2228, 1, 640, 11, 2, 1475, 5, 153, 291],\n",
       " [2974,\n",
       "  4757,\n",
       "  2975,\n",
       "  3,\n",
       "  4,\n",
       "  1600,\n",
       "  126,\n",
       "  8,\n",
       "  2976,\n",
       "  1064,\n",
       "  4758,\n",
       "  386,\n",
       "  363,\n",
       "  1601,\n",
       "  49,\n",
       "  363,\n",
       "  4759,\n",
       "  605,\n",
       "  6,\n",
       "  4760],\n",
       " [94, 994, 3864, 2, 162, 3, 3865, 2, 3866, 1, 3867, 2, 45],\n",
       " [105,\n",
       "  1,\n",
       "  4300,\n",
       "  1477,\n",
       "  847,\n",
       "  1,\n",
       "  646,\n",
       "  46,\n",
       "  174,\n",
       "  881,\n",
       "  1580,\n",
       "  134,\n",
       "  70,\n",
       "  58,\n",
       "  397,\n",
       "  1734,\n",
       "  148,\n",
       "  1877,\n",
       "  335,\n",
       "  22,\n",
       "  954,\n",
       "  429,\n",
       "  81,\n",
       "  2772,\n",
       "  881,\n",
       "  81,\n",
       "  283,\n",
       "  2041,\n",
       "  21,\n",
       "  1044,\n",
       "  1178,\n",
       "  22,\n",
       "  29,\n",
       "  397,\n",
       "  887,\n",
       "  425,\n",
       "  16,\n",
       "  945,\n",
       "  883,\n",
       "  1877,\n",
       "  1,\n",
       "  3696,\n",
       "  107,\n",
       "  1,\n",
       "  2402,\n",
       "  49,\n",
       "  21,\n",
       "  22,\n",
       "  887,\n",
       "  88,\n",
       "  850,\n",
       "  63],\n",
       " [183, 3515, 2, 51, 394, 1, 1658, 9, 1, 380],\n",
       " [2038, 4, 792, 3450, 4, 539, 1776, 28, 300, 2359, 19, 169, 723, 4, 117, 3],\n",
       " [110, 3, 215, 93, 1, 394, 2, 119, 645, 2, 69],\n",
       " [61, 8, 288, 16, 4239, 3, 2630, 4, 12, 95, 3778, 2441],\n",
       " [2954, 6, 2089, 2090, 1, 5, 1235, 722, 1, 236, 37, 4726, 3],\n",
       " [3691, 2129, 1, 3064, 690, 9, 1, 1801, 2, 2500],\n",
       " [521, 3145, 1, 1619, 3146, 687],\n",
       " [38, 852, 372, 3, 4176, 271, 272, 1, 1058],\n",
       " [160, 1114, 1, 907, 1241, 2, 2900, 299],\n",
       " [7, 84, 396, 3, 4039, 4, 5, 1793, 985, 49, 451, 4040, 3, 953, 1, 659, 1, 731],\n",
       " [176, 14, 313, 1, 660, 11, 3755, 2, 375, 2, 5, 3756, 9, 1852, 17, 46],\n",
       " [3656, 106, 11, 2, 452, 4, 2, 130],\n",
       " [136, 337, 2, 843, 2, 77, 1, 5, 129, 14, 4392, 25, 2, 4274],\n",
       " [321, 86, 24, 2, 2795, 2925, 1, 15, 284, 510, 2, 880],\n",
       " [522, 959, 14, 742, 2, 3258, 9, 3259, 2, 37, 4, 821, 1278, 2966],\n",
       " [194, 8, 234, 237, 1, 3342, 24, 242, 4, 2314, 18, 146, 2],\n",
       " [104, 78, 2879, 1908, 2, 7, 61, 1, 12, 2637, 2, 34],\n",
       " [1861, 2, 1622, 4, 1454, 3, 627, 3, 1049, 2, 3773, 3773, 3, 131],\n",
       " [7, 2586, 2600, 439, 112, 1895, 393, 1896, 1, 236, 37, 605, 363, 3868],\n",
       " [33, 197, 3, 2572, 8, 965, 1127, 3, 16, 147, 3, 282, 4, 32, 43],\n",
       " [217, 4, 5, 4168, 2, 1736, 3, 371, 3],\n",
       " [7, 593, 841, 1446, 4, 4172, 3, 7, 136, 3, 2, 78, 476, 276, 6, 267],\n",
       " [4056, 1034, 2, 439, 2, 1, 630, 967, 2, 34],\n",
       " [583, 1464, 1, 5, 3948, 4, 347, 1, 15, 12],\n",
       " [7, 162, 4, 6, 2, 3996, 4, 7, 681, 2, 2334, 3997],\n",
       " [1345, 2200, 2, 5, 618, 2201, 23, 639, 55, 156, 257],\n",
       " [1491, 4, 4266, 560, 67, 8, 4220, 1215, 1515, 24, 18, 1, 39],\n",
       " [7, 137, 671, 20, 1906, 2, 783, 783, 4, 402, 3889, 23, 1, 298, 572],\n",
       " [249, 803, 4474, 4475, 1, 4476, 2, 4477, 4478],\n",
       " [178, 223, 896, 4487, 2, 2842],\n",
       " [181, 376, 1, 245, 3, 2129, 20, 10, 4, 5, 1126, 1, 5, 705, 3, 1434, 2, 69],\n",
       " [915, 129, 341, 3, 137, 323, 2, 1048, 1, 7, 592, 93, 1006, 2, 15],\n",
       " [4008,\n",
       "  162,\n",
       "  47,\n",
       "  4,\n",
       "  310,\n",
       "  101,\n",
       "  2,\n",
       "  7,\n",
       "  1453,\n",
       "  903,\n",
       "  1,\n",
       "  1541,\n",
       "  11,\n",
       "  1453,\n",
       "  855,\n",
       "  4,\n",
       "  7,\n",
       "  162],\n",
       " [25,\n",
       "  240,\n",
       "  1912,\n",
       "  3,\n",
       "  28,\n",
       "  1497,\n",
       "  2,\n",
       "  14,\n",
       "  2364,\n",
       "  325,\n",
       "  2,\n",
       "  273,\n",
       "  562,\n",
       "  90,\n",
       "  1788,\n",
       "  743,\n",
       "  3,\n",
       "  18,\n",
       "  4,\n",
       "  513,\n",
       "  2,\n",
       "  2702],\n",
       " [1604,\n",
       "  2103,\n",
       "  2343,\n",
       "  3628,\n",
       "  1349,\n",
       "  4,\n",
       "  3621,\n",
       "  2,\n",
       "  757,\n",
       "  758,\n",
       "  3629,\n",
       "  202,\n",
       "  3594,\n",
       "  1,\n",
       "  434,\n",
       "  280],\n",
       " [24,\n",
       "  1175,\n",
       "  643,\n",
       "  1,\n",
       "  310,\n",
       "  529,\n",
       "  3,\n",
       "  40,\n",
       "  18,\n",
       "  4,\n",
       "  31,\n",
       "  3,\n",
       "  137,\n",
       "  4,\n",
       "  1011,\n",
       "  3,\n",
       "  588,\n",
       "  561,\n",
       "  169,\n",
       "  6,\n",
       "  141,\n",
       "  17],\n",
       " [5, 3, 2137, 5, 875, 102, 2, 5, 2828, 3, 10, 1482, 1],\n",
       " [230, 1, 1929, 100, 2572, 1, 780, 2742, 2, 1324, 3, 67, 6, 5, 1914],\n",
       " [13, 23, 1, 3891, 2437, 7, 9, 19, 1198, 4, 1062, 2, 2910, 6, 587, 84],\n",
       " [155, 281, 932, 4, 1, 2251, 1, 830],\n",
       " [4985, 3084, 1, 703, 3085, 620, 8, 169, 2037, 612, 941, 1099, 4986, 2, 4987],\n",
       " [2073, 1590, 213, 27, 85, 2941, 2942, 3, 1, 2943, 57, 4700, 4701],\n",
       " [25, 240, 1, 2525, 2, 2120, 6, 2631, 156, 1, 1108, 2, 135, 67],\n",
       " [422, 1384, 1131, 40, 278, 40, 83, 262, 3],\n",
       " [308, 1, 703, 2897, 2, 4612, 204, 135, 12],\n",
       " [1862, 951, 2, 1060, 9, 4, 5, 6, 2528, 4, 26, 1458, 2],\n",
       " [876, 2, 1897, 1898, 1898, 4, 121, 40, 129, 341, 3, 12, 79, 1148, 32],\n",
       " [4817, 2, 39, 1, 4818, 2122, 2123, 312, 3],\n",
       " [3263, 822, 953, 1, 5, 1235, 23, 1, 1021],\n",
       " [4814, 3, 1585, 191, 9, 1, 222, 265, 24, 73],\n",
       " [1820, 72, 63, 2, 39, 281, 313, 1, 803],\n",
       " [439, 1, 73, 604, 191, 24, 602, 3],\n",
       " [691, 1762, 35, 50],\n",
       " [61, 3, 740, 38, 1941, 3, 119, 6, 67],\n",
       " [212, 376, 4, 360, 1, 892, 410, 2, 555],\n",
       " [136, 1, 1129, 1879, 2532, 11, 6, 4, 4054, 1, 566],\n",
       " [4744, 8, 723, 4745, 2965, 905, 4746, 463, 14, 105, 66, 4747],\n",
       " [124, 2536, 722, 1, 2651, 699, 1, 4073, 3, 4074, 2],\n",
       " [73, 604, 191, 2, 7, 96, 1636, 2, 251],\n",
       " [1253, 8, 213, 1, 5, 2874, 14, 105, 4557, 3, 4558, 905, 4559],\n",
       " [48,\n",
       "  1926,\n",
       "  3995,\n",
       "  18,\n",
       "  4,\n",
       "  2719,\n",
       "  3839,\n",
       "  4,\n",
       "  142,\n",
       "  2,\n",
       "  5,\n",
       "  36,\n",
       "  721,\n",
       "  2087,\n",
       "  663,\n",
       "  1,\n",
       "  5,\n",
       "  153,\n",
       "  477],\n",
       " [644, 10, 489, 25, 1065, 199, 1, 526, 2, 600, 89, 449, 525, 3, 302],\n",
       " [3232, 2, 3233, 1699, 1376],\n",
       " [1953,\n",
       "  1003,\n",
       "  2475,\n",
       "  14,\n",
       "  75,\n",
       "  295,\n",
       "  3,\n",
       "  12,\n",
       "  8,\n",
       "  1472,\n",
       "  1003,\n",
       "  14,\n",
       "  124,\n",
       "  105,\n",
       "  1088,\n",
       "  2,\n",
       "  208,\n",
       "  95],\n",
       " [72, 63, 488, 816, 28, 1045, 1, 1025, 3, 133, 228, 138, 83, 488, 816, 40],\n",
       " [848, 2, 1, 116, 2, 3356, 3, 4, 116, 3, 1566],\n",
       " [4447, 4448, 1, 2826, 2827, 800, 2, 37, 281, 408],\n",
       " [762,\n",
       "  175,\n",
       "  3824,\n",
       "  2,\n",
       "  2838,\n",
       "  108,\n",
       "  706,\n",
       "  175,\n",
       "  124,\n",
       "  581,\n",
       "  1,\n",
       "  189,\n",
       "  675,\n",
       "  1876,\n",
       "  175,\n",
       "  950,\n",
       "  2315,\n",
       "  706,\n",
       "  124,\n",
       "  2567,\n",
       "  3825,\n",
       "  3,\n",
       "  4,\n",
       "  189,\n",
       "  124,\n",
       "  71,\n",
       "  14,\n",
       "  2315,\n",
       "  6,\n",
       "  663,\n",
       "  18,\n",
       "  13,\n",
       "  14,\n",
       "  124,\n",
       "  935,\n",
       "  785,\n",
       "  1377,\n",
       "  953,\n",
       "  108,\n",
       "  706,\n",
       "  124,\n",
       "  950,\n",
       "  25,\n",
       "  1643,\n",
       "  2,\n",
       "  676,\n",
       "  13,\n",
       "  14,\n",
       "  18,\n",
       "  175,\n",
       "  663,\n",
       "  4,\n",
       "  157,\n",
       "  91,\n",
       "  22,\n",
       "  44,\n",
       "  528,\n",
       "  1146,\n",
       "  1643,\n",
       "  32,\n",
       "  301,\n",
       "  1612,\n",
       "  2568,\n",
       "  104,\n",
       "  87],\n",
       " [176, 1000, 116, 2, 119, 1, 26, 161, 372, 2, 464, 271, 272, 1, 872, 1067],\n",
       " [560, 4293, 76, 3, 1458, 1, 2817, 1, 2500],\n",
       " [275, 45, 2, 693, 3432, 154, 66, 978],\n",
       " [1978, 24, 642, 1, 292, 51, 1539, 1, 4390, 3, 4383, 642],\n",
       " [3161, 85, 64, 1, 3162, 3163, 47, 41, 2, 1055, 1674],\n",
       " [81, 666, 1572, 319, 149, 13, 2360, 175, 4, 2, 252, 44, 723],\n",
       " [352, 1, 2691, 3, 3039, 30, 1, 3251, 3, 2692],\n",
       " [254, 976, 641, 1421, 3, 1, 1055, 2075, 1, 212],\n",
       " [33, 4911, 1642, 1052, 2, 4912, 1324, 1, 700, 190, 55, 933],\n",
       " [4124,\n",
       "  175,\n",
       "  1,\n",
       "  345,\n",
       "  205,\n",
       "  136,\n",
       "  2,\n",
       "  121,\n",
       "  2421,\n",
       "  2,\n",
       "  2906,\n",
       "  149,\n",
       "  2,\n",
       "  160,\n",
       "  2303,\n",
       "  78,\n",
       "  476,\n",
       "  276],\n",
       " [375, 1020, 1007, 4132, 4243],\n",
       " [3170, 3, 1, 1365, 1, 3171, 1366, 2, 1679, 4, 1329, 1367],\n",
       " [4173, 1164, 246, 603, 2, 880, 6, 4174, 11, 4, 402, 747, 1, 1214, 2698],\n",
       " [7, 76, 396, 3, 11, 81, 657, 4, 542, 1, 15, 649],\n",
       " [4289, 3, 4289, 3, 391, 2787, 4, 20, 4, 3847, 2, 15, 345],\n",
       " [2, 2341, 13, 1, 2813, 2758, 2],\n",
       " [1163, 3, 560, 2, 262, 106, 1, 1035],\n",
       " [3189, 14, 17, 46, 4, 797, 891, 1, 1024],\n",
       " [451, 484, 603, 2, 880, 13, 1, 219, 3, 119, 1, 4325, 2, 1676, 6, 493],\n",
       " [217, 4752, 244, 4753, 1, 4754],\n",
       " [298, 2, 5, 33, 456, 44, 1712],\n",
       " [3470, 2397, 1799, 2, 4, 36, 3, 1, 659, 2, 15, 12, 3, 25, 172],\n",
       " [1464, 9, 1, 15, 1467, 2619, 25, 2, 620, 115, 1, 240, 205],\n",
       " [25, 12, 1010, 2, 32, 11, 2305, 20, 4, 616, 3, 491, 3747],\n",
       " [2254, 2, 42, 1, 25, 240, 289, 458, 818, 385, 629, 1, 2243],\n",
       " [454, 3195, 1, 466, 3196, 1688, 8, 612],\n",
       " [435,\n",
       "  4,\n",
       "  991,\n",
       "  2,\n",
       "  87,\n",
       "  3,\n",
       "  16,\n",
       "  2609,\n",
       "  13,\n",
       "  1,\n",
       "  342,\n",
       "  503,\n",
       "  2325,\n",
       "  59,\n",
       "  231,\n",
       "  1,\n",
       "  1008,\n",
       "  2,\n",
       "  125],\n",
       " [752, 2, 2136, 78, 13, 1, 664, 2, 6, 3876, 1, 77, 127, 2, 972, 45],\n",
       " [521, 221, 19, 98, 3, 1599, 4, 5, 2227, 3, 24, 1, 132, 1, 188, 19, 300, 145],\n",
       " [202, 929, 2356, 2, 416, 1053, 1, 622],\n",
       " [593, 3, 2, 9, 1, 2833],\n",
       " [694, 1538, 2011, 1, 4523, 4524, 2, 2012, 2857],\n",
       " [7, 172, 10, 371, 461, 2, 69, 11, 1938, 2, 15, 5, 12],\n",
       " [151, 21, 711, 214, 17, 5, 42, 1, 32, 8, 4220, 1215, 1515],\n",
       " [824, 1957, 270, 4, 3, 8, 468, 672],\n",
       " [1731, 219, 3, 4, 252, 2237],\n",
       " [367, 2, 1163, 1, 5, 743, 2997, 552],\n",
       " [3451, 3540, 929, 3541, 2, 1656],\n",
       " [326, 1, 340, 2, 5, 230, 3, 18, 2, 456, 4, 1438],\n",
       " [22,\n",
       "  4423,\n",
       "  2449,\n",
       "  134,\n",
       "  1776,\n",
       "  29,\n",
       "  2819,\n",
       "  2321,\n",
       "  206,\n",
       "  4424,\n",
       "  55,\n",
       "  70,\n",
       "  364,\n",
       "  794,\n",
       "  373,\n",
       "  2272,\n",
       "  1178,\n",
       "  22,\n",
       "  1134,\n",
       "  22,\n",
       "  4423,\n",
       "  2449,\n",
       "  2762,\n",
       "  304,\n",
       "  58,\n",
       "  319,\n",
       "  2819,\n",
       "  1739,\n",
       "  931,\n",
       "  49,\n",
       "  936,\n",
       "  931,\n",
       "  22,\n",
       "  206,\n",
       "  22,\n",
       "  931,\n",
       "  29,\n",
       "  22,\n",
       "  388,\n",
       "  22,\n",
       "  187,\n",
       "  794,\n",
       "  373,\n",
       "  2451,\n",
       "  2819,\n",
       "  4343,\n",
       "  49,\n",
       "  1519,\n",
       "  314,\n",
       "  49,\n",
       "  58,\n",
       "  70,\n",
       "  2600,\n",
       "  49,\n",
       "  1186,\n",
       "  157,\n",
       "  187,\n",
       "  4425,\n",
       "  29,\n",
       "  726,\n",
       "  343,\n",
       "  1700,\n",
       "  726,\n",
       "  4425,\n",
       "  3100,\n",
       "  3569,\n",
       "  879,\n",
       "  1170,\n",
       "  206,\n",
       "  4424,\n",
       "  286,\n",
       "  154,\n",
       "  735,\n",
       "  286,\n",
       "  154],\n",
       " [4156, 101, 4, 15, 5, 1512, 1, 7, 667, 1440, 1, 539],\n",
       " [36, 3962, 20, 14, 30, 2, 7, 859, 4, 50, 1114, 13, 18, 1],\n",
       " [3971, 93, 507, 4, 314, 3, 99, 1463, 4, 7, 512],\n",
       " [882, 3, 7, 4, 3833, 3, 24, 2, 4219, 3, 354, 9, 1, 15, 572],\n",
       " [7, 234, 14, 2636, 79, 2, 191, 4, 5, 338, 3924, 2, 15, 125],\n",
       " [1015, 3, 322, 2, 248, 108, 3961, 18, 3, 1977, 199, 14, 615, 1445],\n",
       " [1697, 502, 2791, 4301, 1020, 70, 225, 3828, 2445, 441, 21, 4087, 49, 2640],\n",
       " [232,\n",
       "  283,\n",
       "  70,\n",
       "  851,\n",
       "  128,\n",
       "  174,\n",
       "  386,\n",
       "  2622,\n",
       "  386,\n",
       "  605,\n",
       "  851,\n",
       "  97,\n",
       "  105,\n",
       "  987,\n",
       "  863,\n",
       "  97,\n",
       "  105,\n",
       "  1,\n",
       "  987,\n",
       "  174,\n",
       "  1431,\n",
       "  363,\n",
       "  3938,\n",
       "  465,\n",
       "  29,\n",
       "  282,\n",
       "  95,\n",
       "  3,\n",
       "  863,\n",
       "  661,\n",
       "  117,\n",
       "  232,\n",
       "  283,\n",
       "  70,\n",
       "  730,\n",
       "  851,\n",
       "  128,\n",
       "  293,\n",
       "  2645,\n",
       "  21,\n",
       "  293,\n",
       "  2645,\n",
       "  21,\n",
       "  2297,\n",
       "  293,\n",
       "  2645,\n",
       "  21,\n",
       "  2400,\n",
       "  157,\n",
       "  307,\n",
       "  29,\n",
       "  22,\n",
       "  309,\n",
       "  3939,\n",
       "  29,\n",
       "  307,\n",
       "  368,\n",
       "  1259,\n",
       "  1411,\n",
       "  1444,\n",
       "  2646,\n",
       "  879,\n",
       "  1170,\n",
       "  1451,\n",
       "  89,\n",
       "  87],\n",
       " [4709, 4710, 538, 4711, 4712, 8, 4713],\n",
       " [3243, 9, 1, 911, 764, 2482, 900, 14, 1, 667, 1440],\n",
       " [123,\n",
       "  663,\n",
       "  729,\n",
       "  2480,\n",
       "  1832,\n",
       "  8,\n",
       "  166,\n",
       "  680,\n",
       "  1,\n",
       "  524,\n",
       "  1297,\n",
       "  1,\n",
       "  3483,\n",
       "  2,\n",
       "  327,\n",
       "  1,\n",
       "  206],\n",
       " [62, 418, 3428, 2264, 8, 92, 1, 610, 2363, 2027, 2, 45, 2],\n",
       " [3066,\n",
       "  725,\n",
       "  4929,\n",
       "  3,\n",
       "  4930,\n",
       "  2,\n",
       "  4931,\n",
       "  6,\n",
       "  4932,\n",
       "  1326,\n",
       "  4933,\n",
       "  1,\n",
       "  5,\n",
       "  129,\n",
       "  4934,\n",
       "  2,\n",
       "  619],\n",
       " [333,\n",
       "  2942,\n",
       "  11,\n",
       "  2,\n",
       "  28,\n",
       "  394,\n",
       "  634,\n",
       "  14,\n",
       "  270,\n",
       "  1,\n",
       "  1658,\n",
       "  723,\n",
       "  1659,\n",
       "  20,\n",
       "  38,\n",
       "  201,\n",
       "  928],\n",
       " [27, 765, 4, 2, 2, 448, 3548, 1998, 1, 37],\n",
       " [2, 5, 637, 4],\n",
       " [4797, 1, 74, 4798, 2, 2995, 1, 817, 426, 330, 1614, 1, 710],\n",
       " [480, 23, 1, 2948, 3820, 204, 782, 11, 1, 2184],\n",
       " [2, 1115, 1, 280, 1383, 1, 2169, 3],\n",
       " [33, 38, 164, 6, 668, 127, 2, 3951, 2469, 6, 3, 43],\n",
       " [3713, 8, 143, 213, 27, 164, 3, 950, 2022],\n",
       " [10, 2689, 1, 3872, 8, 1639, 35, 1091, 1136, 1333, 6, 1198],\n",
       " [92, 1479, 1930, 3, 258, 1, 1142, 2260, 91, 3693, 210],\n",
       " [1861,\n",
       "  3,\n",
       "  858,\n",
       "  127,\n",
       "  2,\n",
       "  3658,\n",
       "  4152,\n",
       "  4,\n",
       "  1487,\n",
       "  6,\n",
       "  3285,\n",
       "  369,\n",
       "  108,\n",
       "  607,\n",
       "  68,\n",
       "  416,\n",
       "  47],\n",
       " [234,\n",
       "  665,\n",
       "  2,\n",
       "  2470,\n",
       "  1,\n",
       "  574,\n",
       "  209,\n",
       "  3,\n",
       "  1626,\n",
       "  126,\n",
       "  8,\n",
       "  194,\n",
       "  11,\n",
       "  14,\n",
       "  75,\n",
       "  4,\n",
       "  61,\n",
       "  2,\n",
       "  57,\n",
       "  390,\n",
       "  12],\n",
       " [60, 1, 4405, 2709, 2, 4405, 590, 282, 4, 4364],\n",
       " [7, 76, 4, 66, 506, 11, 247, 2350, 2, 9, 1, 121, 267],\n",
       " [475, 2, 1211, 4224, 11, 2, 3517, 1, 1399, 463, 1445, 4, 1103, 3, 4290],\n",
       " [1, 140, 2, 947, 480, 2742, 2, 135, 67, 6, 789, 4267, 1, 5, 153, 35, 862],\n",
       " [901, 2, 1353, 2, 5, 113, 2218, 8, 1667, 19, 50, 74, 163, 1354],\n",
       " [60, 30, 4, 15, 5, 1004, 772, 305, 16, 856, 323, 2, 602],\n",
       " [68, 2327, 1, 2957, 3, 530, 2, 222, 2068],\n",
       " [68, 1306, 2, 716, 311, 2, 380, 23, 19, 413, 250],\n",
       " [138, 1223, 378, 2894, 1262, 4, 201, 928, 4813],\n",
       " [110, 1, 1924, 11, 14, 2077, 1, 67, 4, 240, 1149, 2, 541],\n",
       " [1201, 1982, 34, 2278, 1216, 1, 162, 13, 1, 531, 59, 1, 681, 3, 2, 354, 126],\n",
       " [974,\n",
       "  844,\n",
       "  3,\n",
       "  369,\n",
       "  4076,\n",
       "  2,\n",
       "  2627,\n",
       "  4,\n",
       "  4077,\n",
       "  904,\n",
       "  1272,\n",
       "  82,\n",
       "  1,\n",
       "  2702,\n",
       "  4078,\n",
       "  6,\n",
       "  83,\n",
       "  904,\n",
       "  111,\n",
       "  554,\n",
       "  673,\n",
       "  3],\n",
       " [4282, 2095, 1, 11, 204, 2, 717, 12],\n",
       " [39, 3, 2445, 1121, 6],\n",
       " [3646, 2, 1, 103],\n",
       " [149, 1, 295, 3, 32, 1052, 35, 1, 324, 1061],\n",
       " [468, 672, 677, 4, 736, 578, 1, 132, 2, 7, 96],\n",
       " [1390, 1, 78, 3349, 971, 3350, 1, 1541, 80, 1, 1138, 2, 1718, 1719],\n",
       " [4, 165, 2, 2088, 2, 2, 7, 3],\n",
       " [104, 31, 3, 48, 2287, 1, 3265, 703, 406, 1714, 4, 40, 45, 1, 152],\n",
       " [1, 4006, 126, 4, 2239, 9, 1, 4007, 23, 1, 5, 233, 102, 883],\n",
       " [68, 23, 1101, 1230, 1, 72, 63, 260, 1, 181],\n",
       " [4183, 726, 3878, 3, 510, 2, 620, 2663, 800, 2, 5, 62],\n",
       " [968, 3, 8, 27, 1, 520, 3304, 2, 431],\n",
       " [855, 786, 1197, 915, 224, 173, 1900, 2560, 58, 961, 373, 319, 241, 1341],\n",
       " [199, 2248, 1616, 1617, 4, 2, 705, 537],\n",
       " [433, 224, 173, 3, 205, 1145, 641, 6, 3703, 1, 89],\n",
       " [2247, 222, 500, 265, 1, 2206, 44, 241, 490],\n",
       " [62, 1, 3203, 3204, 2264, 1, 2],\n",
       " [1083, 521, 1747, 16, 350, 1310, 98, 3, 1149, 3],\n",
       " [818, 3, 45, 45, 2, 110, 1, 903, 4267, 2, 445, 1, 512],\n",
       " [3772, 2, 2606, 39, 3, 4287, 4288, 2, 2516, 1, 51, 5, 34],\n",
       " [120, 10, 473, 3, 1050, 2, 2793, 645, 2, 69, 11, 81, 4386, 2, 162, 4, 314],\n",
       " [1677, 10, 848, 1, 245, 229, 3386, 1233, 4, 5, 1085],\n",
       " [47, 4, 8, 468, 672, 1, 7, 2363, 129, 341, 3, 137, 3, 346, 2, 466, 962],\n",
       " [1277, 1, 50, 1, 1332, 178],\n",
       " [193, 211, 2, 69, 1, 5, 2, 192, 263, 1, 223, 3],\n",
       " [286, 78, 1255, 2, 1553, 2876, 2877, 3, 1, 37],\n",
       " [92, 954, 3109, 1, 2471, 8, 989, 2472, 3631, 1],\n",
       " [1106, 1712, 4, 812, 811, 1294, 1295, 2, 74, 1, 792, 200, 1284],\n",
       " [1556, 2, 3564, 1, 1560, 1228, 17, 548],\n",
       " [2066, 314, 115, 1, 28, 374, 2, 3105, 3, 2, 216, 329],\n",
       " [4339, 4, 15, 298, 48, 53, 1, 140, 3, 574, 159, 341],\n",
       " [33, 858, 3, 43, 1, 100, 2, 4089, 78, 266, 182],\n",
       " [52, 1969, 6, 4, 60, 782, 90, 10, 510, 2, 1956],\n",
       " [1422,\n",
       "  1808,\n",
       "  471,\n",
       "  3,\n",
       "  2196,\n",
       "  1,\n",
       "  5,\n",
       "  4159,\n",
       "  4411,\n",
       "  1,\n",
       "  132,\n",
       "  81,\n",
       "  1864,\n",
       "  81,\n",
       "  2772,\n",
       "  4363,\n",
       "  148],\n",
       " [1, 26, 487, 2, 628, 2199, 1, 5, 118, 2],\n",
       " [120, 42, 1, 1060, 4, 4028, 1, 15, 1795, 1686, 13, 1438, 270, 47],\n",
       " [33, 741, 169, 167, 3],\n",
       " [740, 1184, 4, 402, 4419, 2, 512, 2764, 1, 99],\n",
       " [251, 3527, 4, 5, 1316, 3, 2, 630, 13, 1, 910],\n",
       " [36, 1, 370, 86, 3, 2746, 2, 1456, 344, 267, 4363, 1928, 4261],\n",
       " [181, 638, 1657, 2, 3, 50, 118, 2, 422, 2198],\n",
       " [2, 29, 1094],\n",
       " [275, 339, 2386, 3, 1, 364, 29],\n",
       " [522, 2, 555, 1, 737, 2, 693, 1],\n",
       " [4432, 34, 446, 19, 210, 163, 184, 11, 1517, 2, 2821, 4433, 1518, 4, 12],\n",
       " [32, 2, 1893, 3921, 2, 3, 16, 47, 102, 4, 1210, 2, 320, 82],\n",
       " [249, 617, 67, 1, 1551],\n",
       " [1222, 1281, 2, 1060],\n",
       " [508, 134, 4087, 2263, 43, 1, 499, 4088, 1397, 2707, 3440, 1684],\n",
       " [951, 861, 3699, 82, 20, 1855, 3721, 4],\n",
       " [118, 2, 72, 63, 1, 39, 4544, 3, 1, 249, 4545, 78],\n",
       " [3912,\n",
       "  3913,\n",
       "  281,\n",
       "  1827,\n",
       "  3,\n",
       "  23,\n",
       "  82,\n",
       "  71,\n",
       "  38,\n",
       "  3799,\n",
       "  3,\n",
       "  2627,\n",
       "  4,\n",
       "  150,\n",
       "  12,\n",
       "  107,\n",
       "  19,\n",
       "  1422],\n",
       " [1449, 6, 2556, 2, 7, 53, 1898, 4, 10, 4280, 12, 156, 372, 2, 387],\n",
       " [237, 2, 973, 215, 732, 376, 1, 2, 524],\n",
       " [111, 106, 11, 2334, 2335, 30, 2, 846, 224, 173, 3, 76],\n",
       " [1083, 521, 455, 4858, 4, 526, 2, 3029, 329, 19, 350, 1310, 98],\n",
       " [3243, 133, 2, 5, 50, 934, 1702, 229],\n",
       " [1288, 923, 488, 185, 2, 243, 1, 459],\n",
       " [7, 2635, 277, 2, 5, 1296, 2, 5, 4040, 11, 2694, 4, 15, 1901],\n",
       " [466,\n",
       "  829,\n",
       "  3,\n",
       "  321,\n",
       "  86,\n",
       "  101,\n",
       "  4,\n",
       "  1461,\n",
       "  3,\n",
       "  165,\n",
       "  2,\n",
       "  15,\n",
       "  32,\n",
       "  905,\n",
       "  25,\n",
       "  103,\n",
       "  6,\n",
       "  1476],\n",
       " [2, 3830, 1, 7, 120, 1888, 2, 135, 84],\n",
       " [13, 1, 2, 3253, 196, 4, 569, 1366, 2, 6, 671, 20, 1265, 2],\n",
       " [1097, 313, 1, 1694, 2, 171, 1, 1544, 1729],\n",
       " [7, 751, 670, 403, 13, 1, 2465, 3746, 1, 32, 1, 654, 2, 5, 3],\n",
       " [50, 72, 63, 422, 3147, 3, 83, 262, 3, 846, 1230],\n",
       " [3337, 1740, 738, 17, 46],\n",
       " [241, 220, 1226, 1, 5, 3478, 4, 719, 3, 26, 3479, 1, 1781, 2, 2402],\n",
       " [1310, 98, 3, 3625, 2, 1284, 1, 15, 67, 3, 2936, 75],\n",
       " [1869, 352, 1, 579, 3, 2466, 2, 95, 1452, 772, 2542, 652, 2, 15, 838],\n",
       " [655, 391, 4, 3528, 1, 871, 2, 15, 345, 6, 3864, 127, 1, 59],\n",
       " [7,\n",
       "  4293,\n",
       "  4294,\n",
       "  4294,\n",
       "  385,\n",
       "  1,\n",
       "  4295,\n",
       "  313,\n",
       "  2,\n",
       "  3134,\n",
       "  4116,\n",
       "  4296,\n",
       "  2,\n",
       "  972,\n",
       "  394,\n",
       "  144,\n",
       "  23,\n",
       "  1,\n",
       "  948],\n",
       " [35, 2, 5, 2937, 9, 1, 87, 1077, 313, 3, 1],\n",
       " [42, 237, 1, 2875, 2, 1042, 166, 140, 1, 114],\n",
       " [734, 14, 328, 2, 2874],\n",
       " [7, 1147, 2996, 488, 3216, 3, 46, 1, 74],\n",
       " [74, 3194, 2, 1111, 78, 3, 5, 1296, 1, 310, 37, 9, 1, 423, 379],\n",
       " [147, 97, 64, 4, 1, 77, 2, 7, 7, 165, 23, 1, 33],\n",
       " [3232, 62, 112, 20, 1, 26, 1562, 976],\n",
       " [268, 8, 1457, 35, 208, 1, 912, 12, 1, 3],\n",
       " [147, 97, 4186, 177, 19, 11, 247, 565, 4, 529, 2, 577],\n",
       " [1018,\n",
       "  22,\n",
       "  936,\n",
       "  667,\n",
       "  4399,\n",
       "  70,\n",
       "  4400,\n",
       "  2507,\n",
       "  22,\n",
       "  2482,\n",
       "  900,\n",
       "  29,\n",
       "  22,\n",
       "  309,\n",
       "  293,\n",
       "  70,\n",
       "  726,\n",
       "  1414,\n",
       "  1389,\n",
       "  2761,\n",
       "  22,\n",
       "  1776,\n",
       "  232,\n",
       "  386,\n",
       "  2792,\n",
       "  58,\n",
       "  22,\n",
       "  309,\n",
       "  134,\n",
       "  253,\n",
       "  22,\n",
       "  2141,\n",
       "  1760,\n",
       "  134,\n",
       "  285,\n",
       "  70,\n",
       "  1414,\n",
       "  1389,\n",
       "  2650,\n",
       "  58,\n",
       "  3068,\n",
       "  22,\n",
       "  2576,\n",
       "  1389,\n",
       "  22,\n",
       "  936,\n",
       "  667,\n",
       "  1020,\n",
       "  4399,\n",
       "  70,\n",
       "  4400,\n",
       "  4332,\n",
       "  49,\n",
       "  1018,\n",
       "  1774,\n",
       "  253,\n",
       "  1219,\n",
       "  386,\n",
       "  70,\n",
       "  1414,\n",
       "  299,\n",
       "  157,\n",
       "  1878,\n",
       "  336,\n",
       "  89,\n",
       "  343,\n",
       "  886,\n",
       "  781,\n",
       "  2493,\n",
       "  293,\n",
       "  557,\n",
       "  238,\n",
       "  244,\n",
       "  2141,\n",
       "  1760,\n",
       "  1878,\n",
       "  1983,\n",
       "  492,\n",
       "  87,\n",
       "  183,\n",
       "  151,\n",
       "  40,\n",
       "  21,\n",
       "  40],\n",
       " [466, 829, 305, 2, 193, 1, 190, 55, 1374],\n",
       " [114, 540, 1133, 1381, 2, 51, 1078, 1133, 556, 100, 969, 13, 1, 453, 725],\n",
       " [1201,\n",
       "  1982,\n",
       "  1204,\n",
       "  522,\n",
       "  2,\n",
       "  1086,\n",
       "  3,\n",
       "  2613,\n",
       "  2,\n",
       "  4172,\n",
       "  882,\n",
       "  3,\n",
       "  1,\n",
       "  377,\n",
       "  211,\n",
       "  71,\n",
       "  1446,\n",
       "  47,\n",
       "  1,\n",
       "  10,\n",
       "  398],\n",
       " [52,\n",
       "  103,\n",
       "  1,\n",
       "  2484,\n",
       "  1057,\n",
       "  1,\n",
       "  188,\n",
       "  1,\n",
       "  7,\n",
       "  94,\n",
       "  6,\n",
       "  1642,\n",
       "  237,\n",
       "  11,\n",
       "  4,\n",
       "  160,\n",
       "  947,\n",
       "  159],\n",
       " [447, 3863, 4, 2728, 5, 770, 102, 4, 2, 153, 1, 5, 12],\n",
       " [90, 3, 219, 3, 1185, 1514, 16, 775, 776, 81, 4303, 81, 435, 2810, 91, 1853],\n",
       " [75, 102, 4, 509, 8, 669, 868, 6, 103, 1, 2511, 2, 15, 4179],\n",
       " [1893, 2, 1386, 168, 3, 16, 404, 1005, 102, 4, 295, 3, 177, 1, 15, 32],\n",
       " [26, 2174, 2, 7, 1883, 6, 31, 3, 137, 603, 2, 236, 1002],\n",
       " [439, 3, 1174, 2, 176, 4234, 344, 2727, 337, 2, 843, 2, 77],\n",
       " [1341, 1616, 1617, 4, 483, 3717, 745],\n",
       " [1315, 3216, 1, 254, 178, 714, 2, 5, 118, 2, 188],\n",
       " [3856, 53, 1285, 2, 9, 1, 12, 99, 2598, 2, 57, 584, 61],\n",
       " [1182, 788, 2, 7, 205, 3350, 18, 30, 8, 3897, 1, 45, 4034, 2, 57, 294],\n",
       " [1, 4209, 3, 127, 13, 1],\n",
       " [92, 376, 1, 245, 3, 3334, 4, 15, 3335, 3, 4, 2320, 1, 37],\n",
       " [1309, 3, 1],\n",
       " [897, 288, 17, 2091, 41, 4727, 3, 4, 72, 63, 35, 540, 225, 1593],\n",
       " [694, 634, 1649, 2178, 4953, 3, 4954, 4955, 134, 4956, 2179, 4957],\n",
       " [3589, 2, 1984, 2, 5, 109, 985, 13, 1, 3, 4326, 1, 659],\n",
       " [845, 1, 956, 20, 474, 4, 641, 11, 2364, 1, 2365, 976],\n",
       " [36, 461, 2, 69, 9, 1, 7, 120, 1943, 1786, 277],\n",
       " [3772, 2, 1859, 39, 10, 231, 1, 334, 2, 1860, 4, 366, 305],\n",
       " [4652, 2, 1575, 461, 2, 69, 9, 1, 5, 2057, 2, 186, 711],\n",
       " [28,\n",
       "  74,\n",
       "  6,\n",
       "  170,\n",
       "  141,\n",
       "  17,\n",
       "  1813,\n",
       "  29,\n",
       "  2,\n",
       "  644,\n",
       "  13,\n",
       "  1,\n",
       "  1065,\n",
       "  199,\n",
       "  1,\n",
       "  200,\n",
       "  238,\n",
       "  8,\n",
       "  695,\n",
       "  459],\n",
       " [52, 1, 73, 191, 8, 3677, 913, 471, 3, 14, 105, 2, 1300],\n",
       " [239, 2, 2763, 6, 141, 56, 5, 354, 1, 7, 197, 3, 1735, 2, 5, 34],\n",
       " [788, 14, 2604, 2, 1211, 1, 294, 3791, 116, 2, 6, 24, 3280, 465],\n",
       " [92, 2, 9, 1815, 4, 1305, 3, 1955, 8, 9, 1, 5, 168],\n",
       " [248, 405, 2, 267, 66, 1, 2, 111, 31, 3, 94],\n",
       " [2049, 3213, 3576, 3, 4, 3057, 1, 1080, 3, 1558, 115, 19, 258],\n",
       " [12, 2488, 4, 5, 268, 2, 121, 53, 1, 1046, 61],\n",
       " [10, 1851, 231, 1, 3752, 2, 240, 16, 4, 125, 3753],\n",
       " [2193, 3313, 2356, 2, 375],\n",
       " [33, 3900, 6, 1193, 1193, 277, 43, 71, 10, 67, 6, 2469, 1, 32],\n",
       " [103, 1, 5, 2, 22, 1088, 4, 1, 107, 2, 449, 449, 213],\n",
       " [249, 1, 104, 78, 50, 119, 2862, 9, 1, 5, 430, 715, 80],\n",
       " [1, 140, 3, 1122, 31, 7, 583, 1, 2990, 2, 15, 1945, 3, 12],\n",
       " [698, 4534, 2, 1248, 1, 1036, 68, 11, 2020, 4, 7, 284],\n",
       " [1122, 224, 173, 3, 42, 2706, 4, 269, 224, 173, 3, 76, 5, 153, 2, 76, 316, 2],\n",
       " [278, 3, 113, 3, 1027, 684, 1, 384, 8, 1551, 80, 169, 558, 3375, 3376],\n",
       " [7, 1326, 1389, 1, 741, 2106, 71, 353, 1031, 6, 354, 9, 1, 5, 1434],\n",
       " [52, 2780, 2, 1, 77, 2, 286, 209, 625, 3, 2, 387],\n",
       " [1079,\n",
       "  2417,\n",
       "  13,\n",
       "  164,\n",
       "  204,\n",
       "  1490,\n",
       "  3,\n",
       "  1412,\n",
       "  4,\n",
       "  424,\n",
       "  3,\n",
       "  50,\n",
       "  2671,\n",
       "  3,\n",
       "  318,\n",
       "  451,\n",
       "  484,\n",
       "  1137],\n",
       " [727, 921, 3, 663, 624, 1603],\n",
       " [101, 4, 66, 1, 132, 82],\n",
       " [53, 8, 1574, 4647, 4648, 4649, 221, 19, 213, 27, 83, 2916, 615],\n",
       " [1, 2777, 3, 569, 10, 2644, 3, 531, 1, 1816, 6, 42, 59, 79, 1159, 1, 57, 67],\n",
       " [1339, 1339, 3, 26, 2191, 1, 5, 1340, 1028, 916, 260, 1, 74],\n",
       " [2575,\n",
       "  95,\n",
       "  790,\n",
       "  2721,\n",
       "  4,\n",
       "  2117,\n",
       "  108,\n",
       "  342,\n",
       "  126,\n",
       "  282,\n",
       "  3466,\n",
       "  576,\n",
       "  1057,\n",
       "  126,\n",
       "  3,\n",
       "  16,\n",
       "  2478,\n",
       "  3596,\n",
       "  2460,\n",
       "  157,\n",
       "  343,\n",
       "  528,\n",
       "  1146,\n",
       "  2575,\n",
       "  95,\n",
       "  790,\n",
       "  2721,\n",
       "  4,\n",
       "  766,\n",
       "  348,\n",
       "  87],\n",
       " [94, 7, 356, 3, 270, 1925, 2, 3947, 9, 4, 15, 873],\n",
       " [602,\n",
       "  124,\n",
       "  675,\n",
       "  14,\n",
       "  124,\n",
       "  105,\n",
       "  3910,\n",
       "  7,\n",
       "  1461,\n",
       "  3,\n",
       "  875,\n",
       "  23,\n",
       "  1,\n",
       "  7,\n",
       "  679,\n",
       "  68,\n",
       "  246,\n",
       "  9,\n",
       "  1,\n",
       "  15,\n",
       "  5,\n",
       "  153],\n",
       " [807, 3, 65, 24, 2, 1849, 559, 99, 10, 1, 26, 161],\n",
       " [401,\n",
       "  136,\n",
       "  505,\n",
       "  2,\n",
       "  5,\n",
       "  12,\n",
       "  13,\n",
       "  1438,\n",
       "  4,\n",
       "  4114,\n",
       "  1969,\n",
       "  123,\n",
       "  38,\n",
       "  317,\n",
       "  6,\n",
       "  1,\n",
       "  26,\n",
       "  4115],\n",
       " [176, 10, 473, 1000, 4, 1484, 2, 5, 971, 6, 2723, 99, 4, 26, 161],\n",
       " [208, 3, 722, 119, 3, 2700, 4, 4274, 1, 12, 90, 17, 4, 5, 1004, 59],\n",
       " [506, 3, 8, 2809, 4374, 6, 2, 121, 617, 129, 341, 3, 96, 870, 1, 5, 34],\n",
       " [417, 608, 1543, 2861, 696, 23, 485, 55, 104],\n",
       " [122, 925, 200, 2097, 1, 541, 106, 179, 4755, 211, 2, 302, 525],\n",
       " [527, 31, 3, 3174, 119, 54, 11, 2, 815],\n",
       " [147, 97, 64, 4, 5, 762, 6, 14, 1377, 746, 1, 50, 331, 78, 3394],\n",
       " [257, 31, 3, 1872, 305, 2, 7, 50, 370, 1, 17, 8, 598, 689],\n",
       " [224, 173, 42, 25, 7, 3852, 1, 530, 2, 1531, 858, 127, 2, 266, 45],\n",
       " [180, 3330, 2544, 999, 10, 19, 195, 253, 195, 479, 4373, 21, 1864],\n",
       " [3941, 4, 5, 4150, 4151, 2203, 6, 4152, 3, 5, 9, 19, 1144, 2744, 404],\n",
       " [74, 2213, 3, 1, 555, 9, 4, 5, 1078, 2313, 1732, 1, 310, 37],\n",
       " [1400, 2, 5, 2464, 10, 2465, 118, 2, 422],\n",
       " [12, 8, 210, 163, 184, 2835, 2836, 1, 520, 802, 200, 2837],\n",
       " [433, 31, 3, 480, 753, 1217, 9, 1, 585, 344, 67, 9, 1, 57, 25, 61],\n",
       " [3189, 291, 115, 1, 3190, 435, 957, 1686, 743, 151, 3191, 1078, 1647, 839],\n",
       " [112, 322, 62, 2441, 20, 964, 88, 1681, 3558],\n",
       " [1002, 2, 7, 12, 1, 172, 752, 18, 2, 3840, 3841, 6, 2584, 1, 142, 2, 5, 36],\n",
       " [3364, 1745, 185, 88, 159],\n",
       " [3437, 612, 3, 2, 275, 2, 72, 63, 1104],\n",
       " [1494, 1390, 4, 1357, 117, 235, 10, 3624, 85, 3, 1, 22, 2446],\n",
       " [530, 2, 1701, 2127, 1, 1123, 2963, 19, 92, 44, 524],\n",
       " [518, 1597, 3, 4, 4943, 2, 5, 4944, 546, 1, 735, 40],\n",
       " [60, 14, 1325, 4370, 1, 2815, 3, 1, 566, 4001, 10, 242, 56, 266, 45],\n",
       " [598, 689, 10, 699, 1, 4338, 512, 502, 293, 22],\n",
       " [82, 215, 109, 601, 93, 43, 13, 1, 1396],\n",
       " [2995, 1, 145, 330, 2326, 1, 171],\n",
       " [12, 1213, 20, 474, 4, 295, 3, 203, 2, 61, 41, 123, 14, 47, 2, 2696, 1, 659],\n",
       " [110, 1213, 3, 127, 1, 531, 2, 42, 3, 575, 2, 15, 1512],\n",
       " [405, 8, 945, 158, 2, 45, 45, 1102, 255, 146, 260, 1, 332, 3100],\n",
       " [7, 53, 2354, 11, 3959, 1051, 2, 3840, 1, 649, 2, 12],\n",
       " [40, 78, 1306, 1, 80, 3008],\n",
       " [2230, 219, 3, 3153, 1, 1, 2231, 2, 2827, 800, 2232],\n",
       " [1764, 2, 160, 803, 304, 3327, 1, 1765, 364, 388, 199, 1766, 1, 39, 44, 1590],\n",
       " [2912, 62, 489, 2913, 8, 2054, 1271, 1, 310, 529],\n",
       " [111, 351, 11, 616, 4, 4720, 4721, 1, 1058],\n",
       " [55, 56, 130, 1, 145, 330, 1309, 3, 1, 3103, 1, 210],\n",
       " [442, 18, 1118, 3, 3724, 1, 1546, 14, 1376, 19, 315, 856, 171, 4284, 3],\n",
       " [2, 5, 1, 486, 687, 3379, 438, 233, 35, 7, 168],\n",
       " [374, 2, 2345, 2151, 920, 4, 39, 3, 2116, 1, 526, 2, 327],\n",
       " [3, 2, 74, 919],\n",
       " [897, 1527, 1, 4674, 225, 1280, 1, 382, 714, 2, 428, 89, 170, 3, 226],\n",
       " [595, 220, 1819, 3, 4, 3, 9, 440, 1, 5, 12, 13, 1, 2332],\n",
       " [103, 56, 12, 8, 2054, 1271, 471, 3],\n",
       " [2021, 4535, 4536, 41, 609, 149, 1249, 34, 4537, 1, 699],\n",
       " [1740, 2477, 2245, 8],\n",
       " [1491, 1516, 97, 64, 1985, 6, 85, 4, 180, 1750, 8, 195, 479],\n",
       " [359, 2246, 1, 3074, 2, 3176, 1, 1021],\n",
       " [569, 4, 32, 13, 1, 3404, 2, 344, 1394, 4272],\n",
       " [52, 130, 1, 1, 1758, 237, 1, 1116, 2349, 1645, 293, 80],\n",
       " [150, 908, 620, 1, 5, 130, 1, 720, 238, 4962, 91, 4963, 3, 2, 210],\n",
       " [1340, 378, 1391, 4, 3585, 2, 3490, 2, 62, 494, 1, 57, 37],\n",
       " [307, 2886, 1, 145, 2223, 1536, 3172, 3173, 2, 423, 379],\n",
       " [96, 1, 2510, 3, 2533, 2, 15, 5, 176, 328, 1, 2597, 2, 84],\n",
       " [1066, 10, 1730, 728, 88, 412],\n",
       " [898, 221, 1, 1810, 3, 1037, 2, 189, 1, 5, 118, 2, 915, 62, 3, 83, 828, 2154],\n",
       " [189, 378, 1391, 4, 3015, 414, 1, 3551, 2, 3633, 1, 5, 1162],\n",
       " [36, 310, 2905, 116, 2, 5, 109, 985, 81, 116, 2, 126],\n",
       " [2508, 3733, 4, 14, 503, 1468, 107, 1, 3815, 3816, 472, 1, 324, 1061],\n",
       " [7, 42, 2, 23, 1, 4079, 2, 2615, 645, 2, 5],\n",
       " [893, 313, 3, 1, 4, 2, 222, 3234, 1, 5, 647, 1700],\n",
       " [217, 239, 2, 5, 3067, 1, 4939, 1327, 6, 5, 633, 4940, 1, 4941, 365],\n",
       " [183, 846, 415, 820, 3518, 3, 115, 1, 72, 63],\n",
       " [540, 833, 1337, 1362, 2, 251, 2, 390, 1775, 800],\n",
       " [3675,\n",
       "  2,\n",
       "  7,\n",
       "  2521,\n",
       "  2757,\n",
       "  3,\n",
       "  596,\n",
       "  483,\n",
       "  2829,\n",
       "  721,\n",
       "  477,\n",
       "  124,\n",
       "  105,\n",
       "  81,\n",
       "  3998,\n",
       "  14,\n",
       "  175,\n",
       "  581,\n",
       "  116],\n",
       " [6, 1708, 1, 152],\n",
       " [1279, 3421, 4, 427, 427, 41, 38, 360, 8, 92],\n",
       " [362, 20, 2389, 4, 1394, 410, 9, 1, 5, 230, 2, 1330],\n",
       " [243, 227, 4740, 1596, 1, 37, 108, 14, 4741, 2, 493],\n",
       " [60, 14, 30, 478, 1, 119, 13, 1, 15, 995, 3751, 2, 331, 1],\n",
       " [5, 1705, 4, 592, 148, 1706],\n",
       " [7, 512, 1688, 4, 1901, 2, 3866, 13, 38, 7, 36, 24, 503, 77],\n",
       " [442, 18, 50, 701, 315, 27, 3297, 571, 258, 2693, 184, 2690, 1964],\n",
       " [485, 860, 1358, 35, 1801, 2, 2427],\n",
       " [993,\n",
       "  2558,\n",
       "  1118,\n",
       "  3,\n",
       "  14,\n",
       "  97,\n",
       "  233,\n",
       "  24,\n",
       "  3775,\n",
       "  2,\n",
       "  172,\n",
       "  16,\n",
       "  586,\n",
       "  81,\n",
       "  996,\n",
       "  1967,\n",
       "  21,\n",
       "  3932,\n",
       "  29,\n",
       "  18,\n",
       "  534,\n",
       "  151,\n",
       "  253,\n",
       "  3471],\n",
       " [6, 127, 4347, 4348, 786],\n",
       " [2, 48, 1049, 2, 1469, 4, 469, 6, 160, 4397, 3, 4, 310, 995, 217],\n",
       " [2807, 2808, 24, 79, 2794, 1, 1141, 2, 320, 2241, 2, 5, 34],\n",
       " [111, 1, 104, 3, 3093, 23, 1, 553, 3],\n",
       " [100, 787, 1010, 2, 32, 11, 14, 2, 469, 6, 2390, 4, 248, 399],\n",
       " [205, 3266, 1380, 3, 16, 166, 3267, 167, 3],\n",
       " [52, 30, 4, 1042, 166, 140, 3, 1758, 106, 11, 4, 68, 182],\n",
       " [723, 2360, 1267, 2150, 8, 158, 11, 2940, 16, 250],\n",
       " [7, 96, 45, 45, 3, 3507, 9, 2674, 4, 5, 2594, 2, 15, 5, 65],\n",
       " [867, 1169, 4, 7, 137, 3, 1510, 35, 7, 1],\n",
       " [2332, 2, 2006, 1, 1060, 1, 251],\n",
       " [1688, 4, 3475, 2, 1097, 6, 2, 104, 2284, 1783],\n",
       " [89, 45, 144, 222, 265, 801, 2063, 14, 17, 46, 4667],\n",
       " [464, 3165, 91, 2237, 932, 8, 241, 413, 250],\n",
       " [338, 875, 2, 1046, 125, 870, 1, 142, 2, 5, 34],\n",
       " [2494, 3, 2386, 1, 7, 1214, 2698, 4, 116, 2, 131, 1, 77, 59],\n",
       " [36, 461, 2, 69, 9, 1941, 4, 797, 717, 12, 3, 7, 986, 2, 621],\n",
       " [1822, 1251, 1, 2, 1029, 8, 27, 115, 19, 3626, 3627],\n",
       " [324, 1061, 80, 1261, 4, 115, 1, 5, 2494, 3, 2],\n",
       " [259, 2, 7, 401, 1888, 30, 2, 15, 84, 2776, 4, 142, 2, 5, 34],\n",
       " [31, 3, 42, 54, 1, 591, 2, 52, 369, 3322, 1, 3323],\n",
       " [2409, 455, 645, 1, 338, 45, 2, 2410, 1784],\n",
       " [3, 105, 575, 85, 9, 10, 2607, 105, 56, 1792, 2, 120, 219, 1195, 1],\n",
       " [4695, 1052, 2, 2938, 306, 810, 1, 190],\n",
       " [626, 2, 797, 2959, 406, 1, 1287],\n",
       " [688, 86, 851, 4157, 21, 18, 2686],\n",
       " [4874, 306, 2, 700, 190, 55, 933, 922, 3],\n",
       " [241, 2382, 264, 1312, 1, 116, 2, 2, 2006, 4],\n",
       " [2543, 3388, 987, 3, 561, 3, 3909, 4060, 13, 946, 4, 1932, 107, 1, 5, 153],\n",
       " [186, 3053, 1364, 1, 1575, 149, 3336, 2321, 349, 68, 185, 2911, 1, 280],\n",
       " [1583, 1, 1, 152],\n",
       " [1328, 4, 118, 2, 122, 303, 3, 2354, 1, 2355, 13, 1, 604, 191],\n",
       " [10, 4280, 61, 545, 3, 378, 4, 1006, 2, 559, 9, 1, 5, 12],\n",
       " [1880, 756, 4, 298, 861, 377, 203, 1, 2672, 126, 8, 2553, 2554],\n",
       " [50, 4166, 3, 840, 2, 5, 1835, 1, 1214, 101, 2, 36, 3, 671, 2, 1906],\n",
       " [1457, 6, 3768, 3769, 2, 904, 416, 211, 2, 1727, 402, 1388, 18, 1, 627],\n",
       " [907, 2210, 35, 2891, 16, 1574, 3128, 6, 299],\n",
       " [1754, 9, 1, 974, 496, 3392, 3, 88, 412],\n",
       " [1098, 1242, 2208, 13, 1, 2860, 1, 188, 19, 1664],\n",
       " [3353, 35, 3354, 2, 37],\n",
       " [194, 385, 4, 1495, 8, 234, 13, 1, 7, 1651],\n",
       " [70, 495, 70, 6, 2, 262, 10, 1183, 19, 3861, 395, 2714, 2715],\n",
       " [1338, 270, 4, 709, 108, 123, 97, 64, 587, 1785, 1, 5, 1209, 215, 2669],\n",
       " [102, 483, 1495, 47, 102, 4, 366, 177, 1, 32, 8, 33],\n",
       " [1057, 2, 55, 3, 130, 1, 1, 138, 749, 3],\n",
       " [241, 27, 2475, 75, 4, 203, 2, 1, 26, 3651, 2, 1097],\n",
       " [148,\n",
       "  335,\n",
       "  41,\n",
       "  10,\n",
       "  2898,\n",
       "  85,\n",
       "  3,\n",
       "  1965,\n",
       "  1,\n",
       "  5,\n",
       "  1316,\n",
       "  3,\n",
       "  1434,\n",
       "  2,\n",
       "  1931,\n",
       "  10,\n",
       "  697,\n",
       "  85,\n",
       "  3,\n",
       "  863,\n",
       "  919,\n",
       "  174,\n",
       "  283,\n",
       "  21,\n",
       "  1868,\n",
       "  3,\n",
       "  41,\n",
       "  85,\n",
       "  2,\n",
       "  1316,\n",
       "  1,\n",
       "  7,\n",
       "  133,\n",
       "  289,\n",
       "  3,\n",
       "  785,\n",
       "  41,\n",
       "  38,\n",
       "  339,\n",
       "  419,\n",
       "  4300,\n",
       "  3,\n",
       "  643,\n",
       "  3,\n",
       "  4427,\n",
       "  2,\n",
       "  339,\n",
       "  149,\n",
       "  4427,\n",
       "  2,\n",
       "  6,\n",
       "  233,\n",
       "  4,\n",
       "  1434,\n",
       "  157,\n",
       "  307,\n",
       "  29,\n",
       "  22,\n",
       "  309,\n",
       "  3939,\n",
       "  29,\n",
       "  307,\n",
       "  368,\n",
       "  1259,\n",
       "  1411,\n",
       "  1444,\n",
       "  2646,\n",
       "  879,\n",
       "  1170,\n",
       "  1451,\n",
       "  89,\n",
       "  87],\n",
       " [2224, 2225, 1243, 1, 51, 133, 1, 741, 1124, 2, 2487, 279],\n",
       " [48, 2681, 3, 3841, 1, 1948, 1949, 4043],\n",
       " [48, 807, 3, 79, 1159, 1, 1856, 2, 292, 1013, 1, 5, 34],\n",
       " [7, 252, 23, 1, 1348, 3, 757, 758, 1959, 2, 466, 4375, 1, 2785, 287, 17],\n",
       " [92, 1, 1247, 1, 245, 1641, 1134, 21, 3338, 2015, 1355, 21],\n",
       " [1644, 2, 7, 3228, 552, 4, 5, 2654, 930, 1, 2, 5, 79, 4, 759],\n",
       " [3727,\n",
       "  3748,\n",
       "  2,\n",
       "  520,\n",
       "  3481,\n",
       "  1,\n",
       "  33,\n",
       "  125,\n",
       "  3,\n",
       "  16,\n",
       "  2405,\n",
       "  3480,\n",
       "  6,\n",
       "  1,\n",
       "  334,\n",
       "  247,\n",
       "  16,\n",
       "  2403,\n",
       "  2404],\n",
       " [166, 631, 101, 41, 590, 590, 4, 15, 5, 66, 662, 1, 577],\n",
       " [3568, 87, 453, 3262, 29, 22, 224, 1, 332, 2447, 1426],\n",
       " [595, 220, 3462, 1, 374, 2, 359, 155, 105, 3578, 117, 3579],\n",
       " [2572, 6, 1836, 3729, 101, 4, 14, 503, 1468, 1, 3831, 2, 380, 1, 57, 450],\n",
       " [53, 8, 1188, 783, 352, 1, 2, 1859, 39, 87, 14, 784, 661, 41, 147, 4, 177],\n",
       " [195, 479, 3, 729, 14, 3, 107, 4, 609, 335, 2102, 58, 497, 209],\n",
       " [172, 668, 2, 2723, 3983, 1, 84, 6, 26, 161, 570, 3, 346, 1, 140, 3, 1803],\n",
       " [1216, 1005, 102, 1985, 16, 1770, 612, 762, 2, 1, 1948, 1949],\n",
       " [2710, 2711, 19, 598, 689, 1466, 31, 85, 3, 108, 164, 85, 17, 561],\n",
       " [893, 1588, 3, 567, 193, 17, 18, 4, 5, 222, 2068, 3, 293, 29, 735, 122],\n",
       " [3764, 14, 1, 24, 1631, 6, 1, 450],\n",
       " [885,\n",
       "  2,\n",
       "  7,\n",
       "  1209,\n",
       "  215,\n",
       "  14,\n",
       "  2636,\n",
       "  4,\n",
       "  2583,\n",
       "  2,\n",
       "  109,\n",
       "  93,\n",
       "  882,\n",
       "  3,\n",
       "  2674,\n",
       "  1975,\n",
       "  4,\n",
       "  314,\n",
       "  2,\n",
       "  162],\n",
       " [9, 1, 15, 2231, 1130, 4, 1926, 968, 8, 992, 3759, 6, 2, 15, 84],\n",
       " [940, 3102, 3, 2, 179, 1274, 554, 3, 5, 628, 2199, 88, 1573],\n",
       " [42, 878, 2, 503, 67, 1, 566, 793, 78, 673, 10, 17],\n",
       " [402, 1006, 2, 769, 6, 770, 2, 12, 3, 10, 371, 2682, 2, 740],\n",
       " [48, 4287, 923, 967, 2, 5, 3708, 24, 3096, 4368, 17, 1, 5, 864],\n",
       " [25, 172, 1873, 116, 2, 4372, 2, 240, 136, 2666, 2, 67, 6, 2756, 2, 392],\n",
       " [5, 62, 3, 10, 371, 1, 490, 260, 1],\n",
       " [1677, 2096, 2147, 4, 911, 115, 1, 530, 2, 2, 1529, 6, 458, 1264],\n",
       " [1442, 3239, 3679, 115, 1, 1367, 8, 27, 3, 1, 327],\n",
       " [326,\n",
       "  4532,\n",
       "  2014,\n",
       "  4,\n",
       "  2015,\n",
       "  2864,\n",
       "  35,\n",
       "  2865,\n",
       "  2,\n",
       "  1034,\n",
       "  1,\n",
       "  5,\n",
       "  94,\n",
       "  2016,\n",
       "  3,\n",
       "  1,\n",
       "  1544,\n",
       "  2017,\n",
       "  2,\n",
       "  212],\n",
       " [52, 461, 2, 56, 1818, 9, 1, 12, 1470, 20, 14, 1463, 2, 579],\n",
       " [951, 1236, 1, 1, 1682, 1100, 332, 2],\n",
       " [626, 2, 7, 1301, 246, 406, 1, 539],\n",
       " [28, 1050, 2, 647, 942, 2327],\n",
       " [350, 27, 2, 212, 3558],\n",
       " [246, 151, 256, 3917, 22, 29, 1205, 375, 779, 3992, 1937, 29, 151],\n",
       " [227, 1113, 952, 1359, 1, 37, 179, 1114, 820, 83, 537, 13, 1, 1676],\n",
       " [7, 480, 4, 135, 2490, 144, 167, 81, 1967, 21, 2890, 4147],\n",
       " [1511, 31, 3, 110, 2730, 1, 664, 2, 6, 4127, 9, 1, 15, 84],\n",
       " [1813, 242, 1, 3620, 2, 5, 462, 2, 7, 1821, 1, 256],\n",
       " [1, 4, 187, 71, 2, 499, 11, 2671, 1995, 2, 7, 448, 4, 1646, 3, 47, 71, 1, 2],\n",
       " [7, 2786, 277, 51, 129, 24, 6, 1014, 878, 3, 2, 15, 135, 67],\n",
       " [208, 3, 990, 86, 549, 38, 32, 79, 1838, 1, 15, 990],\n",
       " [3520, 476, 1417, 1, 1711, 236, 1313, 2, 37, 3521, 4, 3522, 2427],\n",
       " [52, 30, 1, 150, 2257, 2, 121, 12, 106, 1, 114],\n",
       " [217, 4, 32, 43, 2, 33, 2059, 4336, 81, 4143, 3, 16, 3780],\n",
       " [2282, 14, 2263, 1, 225, 1, 178, 223],\n",
       " [7, 1453, 1869, 10, 125, 6, 4211, 1, 1175, 1931],\n",
       " [1036, 68, 10, 353, 424, 1, 5, 4630, 1, 485, 104, 21, 257],\n",
       " [2443, 2, 2, 1056, 1056, 1421, 3, 1, 212],\n",
       " [22, 700, 980, 3887, 4, 997, 107, 82, 1],\n",
       " [795, 24, 2824, 1, 4441, 796, 482, 348, 133, 83, 594, 2, 349, 198, 40],\n",
       " [230, 3, 305, 2, 190, 1918, 2, 135, 717, 12],\n",
       " [362, 3075, 1330, 1570, 2, 3251, 831, 156, 1, 275, 356, 2],\n",
       " [52, 24, 2951, 54, 1, 2083, 1, 792],\n",
       " [528, 4418, 4228, 103, 1, 5, 203, 41, 123, 322, 1, 1769],\n",
       " [4016, 514, 862, 3, 3, 233, 2, 166, 4079, 20, 14, 24],\n",
       " [1043, 2, 1549, 1, 3120, 1133, 312, 3, 312, 3, 1, 485, 551, 3393],\n",
       " [1511,\n",
       "  31,\n",
       "  3,\n",
       "  110,\n",
       "  753,\n",
       "  1217,\n",
       "  1,\n",
       "  4356,\n",
       "  2,\n",
       "  990,\n",
       "  90,\n",
       "  4,\n",
       "  284,\n",
       "  742,\n",
       "  95,\n",
       "  675,\n",
       "  164,\n",
       "  1006,\n",
       "  2,\n",
       "  361],\n",
       " [1752, 2086, 350, 3388, 4, 946, 26, 3389, 2, 171, 1, 3390, 2, 1753],\n",
       " [287, 28, 2042, 2, 87, 421, 768, 1, 931, 2413],\n",
       " [644, 2292, 4, 1256, 1, 3268, 465, 2, 189, 115, 1, 685, 3, 517],\n",
       " [2033, 2, 1040, 1, 152, 4585, 13, 1, 2884, 4586, 1, 2885],\n",
       " [195, 479, 6, 342, 3615, 13, 1, 4304, 1, 132, 81, 658, 1180, 1205, 4304],\n",
       " [615,\n",
       "  3,\n",
       "  3599,\n",
       "  2,\n",
       "  5,\n",
       "  168,\n",
       "  71,\n",
       "  637,\n",
       "  4,\n",
       "  1764,\n",
       "  2,\n",
       "  3364,\n",
       "  1745,\n",
       "  772,\n",
       "  4,\n",
       "  5,\n",
       "  168,\n",
       "  41,\n",
       "  637,\n",
       "  126,\n",
       "  4,\n",
       "  7,\n",
       "  1126,\n",
       "  3,\n",
       "  2,\n",
       "  6,\n",
       "  3486,\n",
       "  1,\n",
       "  590,\n",
       "  4,\n",
       "  665,\n",
       "  2,\n",
       "  5,\n",
       "  2816,\n",
       "  409,\n",
       "  4,\n",
       "  5,\n",
       "  4341,\n",
       "  909,\n",
       "  3,\n",
       "  4,\n",
       "  1745,\n",
       "  950,\n",
       "  1437,\n",
       "  47],\n",
       " [33, 2548, 601, 681, 6, 3739, 670, 346, 6, 2713, 1, 4093, 1407, 207],\n",
       " [787, 2, 633, 3870, 101, 41, 419, 75, 858, 2, 160, 3871, 266, 356],\n",
       " [3178, 942, 2, 93, 3, 10, 94, 1, 77, 2, 109, 1629, 1668, 2, 357],\n",
       " [183, 33, 143, 158, 920, 4, 3, 1437, 16, 315, 27],\n",
       " [702, 481, 3, 3530],\n",
       " [7, 53, 291, 11, 4, 177, 1, 15, 12, 1, 2, 859],\n",
       " [98, 274, 1670, 2, 37, 13, 1, 1, 3577, 59, 6, 2, 15, 84],\n",
       " [438, 12, 4, 1420, 1, 387, 1, 401, 10, 1132, 1, 26, 1492],\n",
       " [33, 43, 71, 476, 261, 3, 4, 4234, 6, 9, 393, 978, 3, 1958],\n",
       " [155, 2034, 2, 7, 2035, 2886, 1, 1257, 2, 1043, 2, 907, 2887, 1258],\n",
       " [650, 4117, 206, 70, 3642, 1850],\n",
       " [205, 73, 11, 4, 179, 4260, 3, 478, 23, 1, 4389],\n",
       " [23, 1, 3, 3380, 41, 1506, 2, 5, 168, 4, 1711, 236, 371, 20, 38, 447],\n",
       " [1516,\n",
       "  1985,\n",
       "  16,\n",
       "  4379,\n",
       "  458,\n",
       "  419,\n",
       "  47,\n",
       "  1398,\n",
       "  2,\n",
       "  989,\n",
       "  29,\n",
       "  3655,\n",
       "  4309,\n",
       "  6,\n",
       "  123,\n",
       "  671,\n",
       "  3,\n",
       "  483,\n",
       "  3358,\n",
       "  82],\n",
       " [76, 1470, 11, 14, 1, 968, 3, 4034, 1, 132, 4, 5, 380],\n",
       " [1183, 2, 69, 9, 1, 120, 443, 1, 1924, 510, 2, 2155, 23, 1, 5, 34],\n",
       " [882, 3, 71, 4, 7, 583, 13, 1, 560, 3, 177, 1, 15, 2516],\n",
       " [3, 4315, 1, 7, 4, 2, 34, 100],\n",
       " [506, 3, 2570, 2, 7, 345, 1, 167, 3, 655, 501, 2584, 1, 142, 2, 5, 34],\n",
       " [176, 1417, 101, 4, 66, 1468, 1, 532],\n",
       " [169, 3506, 2417, 13, 1, 14, 901, 2, 1412, 79, 3507],\n",
       " [176, 4015, 4, 9, 1, 1488, 4223, 2, 153, 372, 3, 464, 271, 272],\n",
       " [179, 1114, 113, 1, 228, 138, 1, 50, 159, 5, 3689, 1689],\n",
       " [74,\n",
       "  3242,\n",
       "  3,\n",
       "  4,\n",
       "  122,\n",
       "  130,\n",
       "  1,\n",
       "  103,\n",
       "  56,\n",
       "  619,\n",
       "  3552,\n",
       "  3,\n",
       "  2,\n",
       "  7,\n",
       "  65,\n",
       "  2,\n",
       "  1344,\n",
       "  29,\n",
       "  256],\n",
       " [2679,\n",
       "  677,\n",
       "  4,\n",
       "  5,\n",
       "  34,\n",
       "  41,\n",
       "  590,\n",
       "  4,\n",
       "  112,\n",
       "  2721,\n",
       "  1,\n",
       "  659,\n",
       "  8,\n",
       "  2353,\n",
       "  1039,\n",
       "  4106,\n",
       "  4106],\n",
       " [2889, 1246, 3, 64, 41, 419, 4597, 4, 4598, 909, 3, 88, 798, 910],\n",
       " [332, 333, 634, 1690, 1373, 9, 1, 5, 2, 186, 1550, 765, 2, 34],\n",
       " [2011, 1, 979, 2, 2336, 88, 939, 1426],\n",
       " [970, 1920, 4, 436, 3, 953, 1, 132, 4184, 18, 615],\n",
       " [830, 914, 3, 2, 1761, 16, 2172, 2173],\n",
       " [632, 138, 553, 38, 407, 156, 639, 89, 44, 1303],\n",
       " [202, 1381, 2, 112, 503, 800, 2295, 1, 2296, 2, 496, 23, 1, 218],\n",
       " [170, 106, 1, 716, 3, 1050, 30, 2, 685, 3, 4688],\n",
       " [1075,\n",
       "  86,\n",
       "  2,\n",
       "  3049,\n",
       "  2156,\n",
       "  4897,\n",
       "  30,\n",
       "  1,\n",
       "  1321,\n",
       "  11,\n",
       "  1634,\n",
       "  1635,\n",
       "  1,\n",
       "  3050,\n",
       "  4,\n",
       "  484,\n",
       "  417,\n",
       "  24,\n",
       "  3051],\n",
       " [1884, 166, 3, 267, 196, 2, 7, 65, 9, 3904, 1635, 4, 15, 439],\n",
       " [614, 3209, 4, 2411, 3092, 1, 5, 3, 987, 71, 203, 9, 2, 1160],\n",
       " [191, 8, 1074, 4042, 1187, 1, 52, 1, 132, 6, 3874, 107, 1102, 255],\n",
       " [2, 3528, 20, 3, 4, 25, 3529, 107],\n",
       " [111, 3, 1140, 54, 11, 2, 1, 539],\n",
       " [1094, 3275, 1, 51, 133, 1303],\n",
       " [130, 1, 188, 1, 1266, 430, 324, 2074, 921],\n",
       " [36, 4, 767, 1450, 2, 417, 451, 484, 1, 321, 86, 3, 2],\n",
       " [1915, 2, 1, 57, 674, 2, 2769, 6],\n",
       " [205, 4238, 23, 1, 712, 396, 3, 11, 3980, 4, 32, 2, 28, 2695],\n",
       " [12,\n",
       "  519,\n",
       "  2830,\n",
       "  1522,\n",
       "  6,\n",
       "  4459,\n",
       "  4460,\n",
       "  3,\n",
       "  16,\n",
       "  1523,\n",
       "  4461,\n",
       "  2831,\n",
       "  3,\n",
       "  598,\n",
       "  689,\n",
       "  4462,\n",
       "  1524,\n",
       "  4463,\n",
       "  5,\n",
       "  1026],\n",
       " [1444, 1279, 374, 20, 1755, 1, 307, 3033, 2, 333, 711],\n",
       " [882, 3, 71, 66, 877, 79, 3, 54],\n",
       " [202, 38, 724, 3, 3535, 1, 622, 463, 2246, 2, 5, 3535, 1, 37],\n",
       " [92, 3, 144, 470, 4, 1738, 2, 202],\n",
       " [1072, 2055, 2, 257, 28, 247, 1742, 3, 3345, 88, 159],\n",
       " [3, 3937, 3937, 2, 7, 480, 3719, 4, 2644, 3, 66, 699, 59, 510, 2, 880],\n",
       " [413, 250, 187, 895, 4479, 303, 4480, 1233],\n",
       " [40, 118, 2, 72, 63, 1],\n",
       " [3, 16, 1411, 2643, 47, 102, 4, 203, 2, 15, 1218, 2570, 1, 140, 3, 3229],\n",
       " [183, 694, 634, 14, 90, 1, 5, 961, 687],\n",
       " [52, 86, 73, 11, 2, 93, 20, 13, 2517, 146, 75, 59],\n",
       " [2, 2209, 259, 22, 395],\n",
       " [1647, 29, 2171, 3, 4945, 1, 384, 8, 824, 4946, 720, 549, 54],\n",
       " [6, 1, 845, 3, 23, 1, 324, 2074, 2024, 3],\n",
       " [25, 491, 58, 809, 237, 20, 30, 4, 7, 1042, 166, 140, 3, 575, 1, 659, 2, 100],\n",
       " [50, 2671, 3, 318, 2, 447, 4, 995, 11, 746, 4, 3774, 2019],\n",
       " [557, 2207, 244, 2208, 3],\n",
       " [5, 859, 3538, 264, 2, 898, 3, 3539, 2, 9, 1, 5, 1417],\n",
       " [170, 1416, 1, 2067, 939, 364, 421, 768],\n",
       " [3331, 29, 2089, 291, 1, 5, 647, 1700, 2, 500],\n",
       " [33, 6, 943, 1082, 2, 447, 43, 160, 116, 2, 2320, 95, 3],\n",
       " [2314, 549, 1, 292, 1200, 6, 4, 116, 2, 4249, 467],\n",
       " [2790, 2, 69, 2, 84, 8, 4297, 2312, 3787, 2, 34],\n",
       " [775, 776, 21, 2187, 1, 1, 3043, 107, 82, 957, 24, 2773, 174],\n",
       " [1499,\n",
       "  1947,\n",
       "  49,\n",
       "  158,\n",
       "  386,\n",
       "  2843,\n",
       "  4025,\n",
       "  1408,\n",
       "  22,\n",
       "  327,\n",
       "  91,\n",
       "  3731,\n",
       "  2507,\n",
       "  3732,\n",
       "  58,\n",
       "  2295,\n",
       "  319,\n",
       "  22,\n",
       "  4032,\n",
       "  29,\n",
       "  441,\n",
       "  58,\n",
       "  778,\n",
       "  200,\n",
       "  418,\n",
       "  157,\n",
       "  2113,\n",
       "  1499,\n",
       "  1947,\n",
       "  21,\n",
       "  2623,\n",
       "  174,\n",
       "  218,\n",
       "  1180,\n",
       "  1150,\n",
       "  1403,\n",
       "  2497,\n",
       "  255,\n",
       "  735,\n",
       "  492,\n",
       "  154],\n",
       " [1891, 220, 21, 595, 589, 3, 589, 2, 208, 95, 4, 39, 589, 347, 4, 5, 261],\n",
       " [380, 9, 1, 5, 1624, 284, 4826, 2, 698, 597, 548, 2126],\n",
       " [2219, 1170, 4, 68, 836, 764, 28, 40],\n",
       " [28, 1086, 381, 20, 4883, 244, 693, 934, 1, 218, 2151, 548, 74],\n",
       " [25, 125, 23, 1, 256, 2, 150, 532, 1452, 4, 2527, 6],\n",
       " [7, 53, 3734, 2, 4, 60, 3888, 1, 26, 2, 12, 107],\n",
       " [2697,\n",
       "  2060,\n",
       "  197,\n",
       "  3,\n",
       "  436,\n",
       "  3,\n",
       "  16,\n",
       "  1978,\n",
       "  552,\n",
       "  41,\n",
       "  419,\n",
       "  75,\n",
       "  3,\n",
       "  432,\n",
       "  2,\n",
       "  103,\n",
       "  2739,\n",
       "  436],\n",
       " [25, 2301, 50, 367, 1, 109, 601, 196, 4, 1631, 2003],\n",
       " [239, 2, 973, 6, 971, 3500, 738, 4, 239, 88, 159],\n",
       " [3362, 2, 62, 2289, 1, 3, 901, 2, 201],\n",
       " [1467, 2619, 3, 16, 2187, 3679, 19, 2658, 85, 3, 290, 105],\n",
       " [189,\n",
       "  3395,\n",
       "  4,\n",
       "  653,\n",
       "  2,\n",
       "  2880,\n",
       "  1155,\n",
       "  3343,\n",
       "  2019,\n",
       "  3,\n",
       "  2398,\n",
       "  4,\n",
       "  375,\n",
       "  3471,\n",
       "  1,\n",
       "  3060],\n",
       " [557, 2207, 244, 180, 163, 2208, 811, 744, 981, 330, 3453, 88, 356],\n",
       " [3, 4, 939, 542, 85, 3, 4186, 1105, 149, 2356, 2],\n",
       " [1304, 184, 4, 118, 8, 98, 2492],\n",
       " [431, 1410, 3, 26, 1837, 4, 326, 2, 450, 1076, 2244, 1, 254, 178],\n",
       " [597, 1636, 4, 5, 2290, 2, 1443, 1, 1610, 2444, 2, 960, 1782],\n",
       " [5, 4915, 3, 83, 2163, 813, 229, 4916, 3058, 260, 19, 27],\n",
       " [2409, 1788, 365, 2, 1784, 2, 2410, 38, 2411, 1658, 13, 1, 100],\n",
       " [676, 3, 1184, 2724, 25, 1394, 82, 2700, 347, 6, 767],\n",
       " [1197,\n",
       "  232,\n",
       "  21,\n",
       "  22,\n",
       "  1731,\n",
       "  2311,\n",
       "  1219,\n",
       "  70,\n",
       "  29,\n",
       "  1150,\n",
       "  1403,\n",
       "  425,\n",
       "  232,\n",
       "  4091,\n",
       "  148,\n",
       "  134,\n",
       "  22,\n",
       "  324,\n",
       "  3011,\n",
       "  3883,\n",
       "  70,\n",
       "  324,\n",
       "  2425,\n",
       "  22,\n",
       "  368,\n",
       "  1197,\n",
       "  148,\n",
       "  3955,\n",
       "  21,\n",
       "  1150,\n",
       "  1403,\n",
       "  883,\n",
       "  157,\n",
       "  307,\n",
       "  29,\n",
       "  22,\n",
       "  309,\n",
       "  307,\n",
       "  368,\n",
       "  1259,\n",
       "  1411,\n",
       "  1444,\n",
       "  2646,\n",
       "  879,\n",
       "  1170,\n",
       "  1451,\n",
       "  89,\n",
       "  87,\n",
       "  183,\n",
       "  1433,\n",
       "  574,\n",
       "  21,\n",
       "  1433,\n",
       "  1025],\n",
       " [16, 11, 4, 1375, 6, 259, 59, 1],\n",
       " [7, 3, 137, 160, 20, 2, 3359, 4, 5, 655, 3922, 242, 347, 1, 32],\n",
       " [4, 1461, 3, 165, 1, 748, 2, 3777, 58, 3125],\n",
       " [1934, 4, 585, 3, 1108, 2, 322, 12, 108, 14, 107, 3, 4, 208, 2, 5, 47],\n",
       " [2632, 583, 1, 2684, 2632, 6, 3, 97, 10, 513, 77, 17],\n",
       " [35, 1313, 2, 1035, 80, 3034, 1627, 2, 3469, 557],\n",
       " [3208, 2, 5, 454, 3, 934, 1691, 3209, 1, 171],\n",
       " [74,\n",
       "  24,\n",
       "  1525,\n",
       "  2,\n",
       "  72,\n",
       "  63,\n",
       "  2090,\n",
       "  1632,\n",
       "  1,\n",
       "  5,\n",
       "  133,\n",
       "  3,\n",
       "  328,\n",
       "  2,\n",
       "  26,\n",
       "  2,\n",
       "  1231,\n",
       "  1997],\n",
       " [25, 1758, 3, 2368, 1, 802, 1275],\n",
       " [4767, 2978, 1, 2979, 4768, 2, 362],\n",
       " [1962, 1971, 13, 1, 473, 103, 1496, 2524, 246],\n",
       " [155, 1279, 14, 270, 4673, 41, 123, 264, 2930, 2, 212, 4, 427, 427],\n",
       " [254, 178, 350, 98, 2466, 2, 28, 1721, 1813, 1, 212],\n",
       " [4128, 16, 977, 42, 1, 451, 484, 3, 977, 347, 4, 10, 1321, 13, 38, 1412],\n",
       " [357, 1261, 4, 911, 115, 1, 2892, 1560],\n",
       " [1902, 4, 14, 2950, 642, 2, 33, 29, 364, 923, 3879],\n",
       " [1607, 163, 1715, 1, 1307],\n",
       " [60, 93, 3, 38, 1412, 38, 3895, 6, 38, 2743, 338, 237, 3, 699, 17],\n",
       " [278, 4444, 9, 4445, 2, 407, 1, 279, 2, 798, 4446],\n",
       " [212,\n",
       "  1591,\n",
       "  3,\n",
       "  1,\n",
       "  1055,\n",
       "  2075,\n",
       "  4,\n",
       "  4702,\n",
       "  1592,\n",
       "  2,\n",
       "  4703,\n",
       "  198,\n",
       "  55,\n",
       "  3,\n",
       "  2944,\n",
       "  2,\n",
       "  1056,\n",
       "  1056],\n",
       " [1880, 756, 471, 3, 1187, 1, 2552, 782, 75, 2, 33, 125, 3, 16, 2553, 2554],\n",
       " [734, 191, 2, 3800, 8, 1874, 1875, 144, 75, 25, 3801, 3, 203, 2, 121, 871],\n",
       " [29,\n",
       "  58,\n",
       "  1157,\n",
       "  658,\n",
       "  58,\n",
       "  22,\n",
       "  39,\n",
       "  148,\n",
       "  134,\n",
       "  253,\n",
       "  567,\n",
       "  441,\n",
       "  283,\n",
       "  1007,\n",
       "  49,\n",
       "  6,\n",
       "  151,\n",
       "  58,\n",
       "  553,\n",
       "  80,\n",
       "  1219,\n",
       "  4197],\n",
       " [158, 3719, 8, 166, 680, 1, 1, 3026, 2, 39],\n",
       " [68, 54, 1, 4613, 2, 704, 6, 815, 1, 1263],\n",
       " [1121, 2, 202, 753, 9, 1, 388, 543],\n",
       " [26, 487, 2, 2137, 3, 2138, 3023, 2, 5, 1079],\n",
       " [915, 168, 83, 828, 2154, 1317, 260, 1, 359],\n",
       " [2, 113, 1, 7, 60, 1, 1096, 1474, 1, 191, 5, 34, 753],\n",
       " [42, 1964, 11, 2, 135, 61, 4, 655, 4061],\n",
       " [246, 1445, 3808, 2, 3809, 4, 196, 3, 2, 407],\n",
       " [4, 33, 1091, 217, 4, 15, 32, 6, 1899, 1450, 1, 5, 1434],\n",
       " [176, 1, 207, 1010, 4, 96, 369, 2, 207, 282, 95, 290],\n",
       " [12, 8, 3879, 6, 2473, 3, 16, 4, 1848, 3, 853],\n",
       " [438, 12, 2, 943, 6, 445, 961, 2, 3896, 1067, 1, 563],\n",
       " [1831, 320, 4, 5, 3241, 1, 577, 13, 1, 140, 3, 1274, 71, 782, 17, 47],\n",
       " [436, 3, 4014, 8, 970, 337, 2, 777, 4384, 6, 3973],\n",
       " [5, 1887, 3943, 3, 772, 3944, 146, 4, 7, 94, 2091, 117, 41, 147, 4, 57, 930],\n",
       " [775, 10, 1, 51, 1406, 596, 3948, 1767, 1],\n",
       " [1728, 709, 1, 5, 3312, 896, 393, 1736, 41, 38, 1, 1737, 1, 1738],\n",
       " [3146,\n",
       "  206,\n",
       "  4032,\n",
       "  29,\n",
       "  441,\n",
       "  897,\n",
       "  283,\n",
       "  778,\n",
       "  495,\n",
       "  4336,\n",
       "  58,\n",
       "  70,\n",
       "  429,\n",
       "  49,\n",
       "  778,\n",
       "  225,\n",
       "  4263,\n",
       "  779,\n",
       "  3258,\n",
       "  22,\n",
       "  2566,\n",
       "  1020,\n",
       "  1007,\n",
       "  4021,\n",
       "  511,\n",
       "  22,\n",
       "  293,\n",
       "  1356,\n",
       "  49,\n",
       "  273,\n",
       "  91,\n",
       "  225],\n",
       " [186, 711, 3391, 3],\n",
       " [51,\n",
       "  155,\n",
       "  2218,\n",
       "  3,\n",
       "  3640,\n",
       "  4,\n",
       "  5,\n",
       "  65,\n",
       "  229,\n",
       "  3640,\n",
       "  23,\n",
       "  1,\n",
       "  1314,\n",
       "  2144,\n",
       "  198,\n",
       "  2145,\n",
       "  2425],\n",
       " [1151, 117, 2, 48, 47, 48, 2, 3744, 1, 849, 2, 142, 2, 34],\n",
       " [38, 1065, 199, 1, 600, 89, 926, 302, 4766, 1603],\n",
       " [271, 272, 4, 570, 1, 172, 573, 3, 2434],\n",
       " [28, 227, 937, 17, 46, 1, 37, 185, 1373, 44, 243],\n",
       " [25,\n",
       "  240,\n",
       "  3515,\n",
       "  1,\n",
       "  530,\n",
       "  2,\n",
       "  1981,\n",
       "  1981,\n",
       "  638,\n",
       "  317,\n",
       "  6,\n",
       "  2756,\n",
       "  2,\n",
       "  135,\n",
       "  789,\n",
       "  6,\n",
       "  392],\n",
       " [130, 1, 188, 1, 990, 86, 1, 865, 792, 455, 935],\n",
       " [120, 76, 1213, 3, 127, 11, 2733, 2, 1826, 4, 15, 5, 1512],\n",
       " [556, 4226, 196, 3, 196, 117, 46, 64, 4, 191, 80, 4099, 500, 117, 82],\n",
       " [533, 184, 376, 3, 38, 1, 526, 1, 19, 300, 145, 6, 170, 141, 17],\n",
       " [93, 3, 6, 2020, 1, 1096, 11, 4122, 1857, 2, 4123, 113],\n",
       " [60, 1095, 3, 2, 652, 4, 377, 131, 237, 1, 114],\n",
       " [35, 2, 62, 3, 289, 1, 72, 63, 83, 104, 3],\n",
       " [1889, 11, 2, 759, 665, 290, 1221, 1805, 2, 5, 1890, 59],\n",
       " [33, 1459, 1125, 980, 1510, 43, 35, 7],\n",
       " [2807,\n",
       "  2808,\n",
       "  2779,\n",
       "  30,\n",
       "  2,\n",
       "  353,\n",
       "  838,\n",
       "  1,\n",
       "  7,\n",
       "  2485,\n",
       "  11,\n",
       "  4169,\n",
       "  449,\n",
       "  449,\n",
       "  4,\n",
       "  5,\n",
       "  856,\n",
       "  1890],\n",
       " [518, 28, 10, 353, 424, 1, 766, 420, 851, 1, 960, 1782, 1331, 3, 1, 766, 958],\n",
       " [1764, 2, 1, 251, 241, 250],\n",
       " [4, 1181, 463, 1933, 147, 4, 177, 82, 19, 4239, 1871],\n",
       " [92, 50, 2, 1, 3433, 496, 2, 37],\n",
       " [33, 662, 101, 41, 1516, 3, 261, 16, 1192, 1, 2, 656],\n",
       " [310, 1373, 2, 2932, 80, 695, 8, 714, 2, 188, 19, 300, 145],\n",
       " [48, 79, 2780, 2, 325, 2, 7, 36],\n",
       " [197, 3, 2582, 270, 4, 239, 2, 1, 2416, 1],\n",
       " [27, 304, 726, 2101, 24, 867, 1],\n",
       " [383, 920, 4, 1041, 3, 1268, 2, 225, 4696, 4, 5, 2072, 429, 2939, 4697],\n",
       " [1349, 4, 911, 2, 724, 1037, 6, 3319, 1, 715],\n",
       " [122, 54, 1, 1284, 28, 106, 1, 1751, 1673],\n",
       " [614, 1587, 3, 4, 1588, 3, 802, 4685, 4686],\n",
       " [52, 4229, 2, 5, 3873, 2641, 2, 104, 739, 248, 4205, 43, 1, 32],\n",
       " [122, 31, 3, 94, 54, 11, 2, 1323, 1, 1035],\n",
       " [281, 1902, 195, 49, 609, 786, 4302, 4135, 1911, 1921, 4303, 2792, 2792],\n",
       " [89, 1648, 11, 1, 2319, 2079, 60, 30, 106],\n",
       " [1533, 353, 2920, 353, 1577, 2921, 3, 1, 186, 711],\n",
       " [3534, 2034, 2, 390, 356, 99, 3031, 4, 1335, 1280, 1, 636, 223],\n",
       " [1942,\n",
       "  52,\n",
       "  24,\n",
       "  314,\n",
       "  2,\n",
       "  7,\n",
       "  821,\n",
       "  6,\n",
       "  407,\n",
       "  1,\n",
       "  7,\n",
       "  1209,\n",
       "  215,\n",
       "  2669,\n",
       "  1013,\n",
       "  1,\n",
       "  142,\n",
       "  2,\n",
       "  34],\n",
       " [5, 374, 141, 141, 4, 927, 1297, 1, 327, 282, 368],\n",
       " [65, 1, 256, 305, 2, 566, 415, 11, 575, 1, 7, 2557, 2669],\n",
       " [5, 155, 4733, 1, 7, 1532, 3, 4734, 4735, 1, 2092, 19, 300, 4736],\n",
       " [317, 1908, 2, 7, 84, 28, 4, 160, 1, 248, 3821, 131],\n",
       " [181, 1390, 1, 1787, 193, 4, 3, 2399, 764, 72, 63],\n",
       " [205,\n",
       "  1800,\n",
       "  2436,\n",
       "  3543,\n",
       "  13,\n",
       "  1,\n",
       "  15,\n",
       "  954,\n",
       "  855,\n",
       "  760,\n",
       "  627,\n",
       "  10,\n",
       "  285,\n",
       "  335,\n",
       "  70,\n",
       "  2379,\n",
       "  425,\n",
       "  91,\n",
       "  502,\n",
       "  49,\n",
       "  363,\n",
       "  2437,\n",
       "  148,\n",
       "  134,\n",
       "  70,\n",
       "  1582],\n",
       " [2624, 441, 386, 273, 58, 425, 285, 58, 375, 233, 4, 5, 1, 32],\n",
       " [5, 1477, 5, 196, 3, 2743, 4, 513],\n",
       " [4008, 444, 60, 1, 4009, 3, 4010, 3807, 1, 141],\n",
       " [52, 1057, 1, 2083, 1, 491, 58, 809, 1, 2948, 4717, 106, 1, 114],\n",
       " [36, 671, 20, 1906, 2, 334, 367, 2, 647, 109, 601],\n",
       " [644, 1652, 396, 2, 1029, 8, 332, 333, 3, 2253, 1],\n",
       " [3643, 31, 3, 137, 983, 90, 3950, 2, 2303, 78, 266, 182, 4, 12, 3, 83, 296],\n",
       " [25, 61, 106, 20, 1517, 2, 179, 331, 193, 211, 2, 302, 30, 1, 114],\n",
       " [3411, 85, 46, 64, 4233, 1536, 3, 2304, 82, 1, 5, 94, 2653, 2, 322, 2598],\n",
       " [2009, 2855, 2856, 4522, 1, 141, 236, 133, 1, 202],\n",
       " [336, 387, 1067, 823, 20, 2982, 2, 4784, 201, 21, 201, 902],\n",
       " [195,\n",
       "  1,\n",
       "  686,\n",
       "  81,\n",
       "  4203,\n",
       "  881,\n",
       "  1139,\n",
       "  81,\n",
       "  1967,\n",
       "  21,\n",
       "  4246,\n",
       "  605,\n",
       "  81,\n",
       "  881,\n",
       "  134,\n",
       "  1260,\n",
       "  285,\n",
       "  21,\n",
       "  4240,\n",
       "  199,\n",
       "  4247],\n",
       " [228, 138, 111, 3, 1032, 2164, 1],\n",
       " [1019, 3, 215, 109, 93, 606, 2, 1, 731, 13, 1, 888, 3, 873, 6],\n",
       " [26, 3775, 2, 7, 136, 6, 2, 15, 345, 967, 6, 1695, 2, 5, 34],\n",
       " [2235, 221, 1, 359, 3, 393, 1, 50, 2440, 1, 187],\n",
       " [123, 112, 2459, 3, 4201, 772, 10, 125, 149, 4202, 3, 5, 848, 2, 36],\n",
       " [183, 42, 4001, 383, 11, 4, 60, 782],\n",
       " [1038, 305, 2, 179, 1814, 193, 306, 1, 190, 55, 1374],\n",
       " [241,\n",
       "  2683,\n",
       "  1951,\n",
       "  6,\n",
       "  1212,\n",
       "  2766,\n",
       "  2474,\n",
       "  3,\n",
       "  1212,\n",
       "  14,\n",
       "  3,\n",
       "  270,\n",
       "  1490,\n",
       "  4,\n",
       "  788,\n",
       "  561],\n",
       " [1703, 8, 145, 352, 19, 241],\n",
       " [901, 2, 201, 1535, 14, 3, 4516, 1, 152, 4517, 1, 5, 682, 4518],\n",
       " [36, 4012, 6, 765, 2, 13, 1, 15, 66, 1002, 1, 120, 1, 132],\n",
       " [2946, 3, 2079, 2080, 1, 4707, 2947, 2, 4708, 44, 614],\n",
       " [249, 3135, 593, 841, 3523, 1, 5, 1033],\n",
       " [417, 1668, 4, 1043, 2, 608, 1543, 2861, 1, 228, 138, 1410, 13, 1, 453, 725],\n",
       " [194, 3, 10, 55, 3, 12, 24, 136, 13, 507, 6, 2652, 1, 141, 2, 234],\n",
       " [2710, 2711, 1747, 16, 3, 3, 18, 882, 3, 4, 5, 678, 1143, 59],\n",
       " [1793, 1334, 4, 1820, 1431, 1601, 3606, 1, 260, 1, 3607],\n",
       " [1277, 56, 133, 1, 2927, 289, 813, 3, 46],\n",
       " [12, 2, 2530, 372, 35, 271, 272, 1, 1488, 4033],\n",
       " [2681,\n",
       "  4107,\n",
       "  1842,\n",
       "  1471,\n",
       "  10,\n",
       "  804,\n",
       "  3,\n",
       "  268,\n",
       "  9,\n",
       "  1,\n",
       "  5,\n",
       "  4108,\n",
       "  344,\n",
       "  38,\n",
       "  399,\n",
       "  3,\n",
       "  1019],\n",
       " [742, 2, 2462, 6, 1, 1818, 112, 6, 4, 2, 131, 1, 988],\n",
       " [147, 4, 295, 3, 177, 6, 123, 1, 140, 3, 71, 4037, 3, 47],\n",
       " [3596, 2460, 1, 2117, 2461, 56, 498, 46],\n",
       " [214, 4, 5, 60, 66, 297, 2, 32, 8, 1770, 612, 23, 82, 156, 43],\n",
       " [731, 1621, 845, 30, 2, 2373, 1, 2188, 2189],\n",
       " [23, 1, 89, 37, 1, 1101, 1342, 20, 944, 1, 262],\n",
       " [165, 2, 7, 3, 2616, 3904, 317, 686, 11, 2733, 2, 339, 4, 1182, 216],\n",
       " [195, 479, 1137, 11, 1, 7, 2, 1903, 1980],\n",
       " [368, 213, 27, 312, 4, 3468, 2395],\n",
       " [36, 6, 874, 1, 211, 2, 1914, 3, 1177, 23, 1, 2684, 1193, 1193],\n",
       " [452, 2, 50, 367, 2, 522, 2, 5, 1835, 3118, 6, 835, 291, 1, 245],\n",
       " [186, 1989, 1224, 56, 2825, 349, 198, 40, 799, 1, 297, 2, 1225, 280],\n",
       " [3347, 2643, 471, 3, 1187, 1, 2552, 2952, 30, 16, 595, 220, 2, 141],\n",
       " [155,\n",
       "  221,\n",
       "  1,\n",
       "  707,\n",
       "  3,\n",
       "  1268,\n",
       "  2,\n",
       "  1565,\n",
       "  4631,\n",
       "  4,\n",
       "  5,\n",
       "  303,\n",
       "  1,\n",
       "  2052,\n",
       "  1,\n",
       "  57,\n",
       "  4632,\n",
       "  4633],\n",
       " [900, 1379, 496, 489, 8, 220, 772, 4, 427, 427],\n",
       " [807, 1, 7, 42, 1825, 4310, 589, 95, 233],\n",
       " [698, 553, 1690, 1373, 9, 1, 553, 960, 653, 1128],\n",
       " [179, 1571, 222, 3, 1103, 1, 840, 3158, 6, 3],\n",
       " [433, 452, 8, 3078, 1, 2392, 2960, 17, 46, 185, 288, 3, 3466],\n",
       " [3177, 682, 207, 9, 1, 5, 1079, 3255, 3, 357],\n",
       " [98, 323, 2, 69, 19, 92, 1, 1027, 4464, 2, 51, 374],\n",
       " [51, 1337, 1, 3447, 4, 493, 6, 4401, 99, 4406, 4, 2594, 2, 57, 5, 65],\n",
       " [312, 105, 2, 2663, 16, 3626, 3627, 79, 1, 199, 1250],\n",
       " [4095,\n",
       "  1380,\n",
       "  281,\n",
       "  2093,\n",
       "  11,\n",
       "  2581,\n",
       "  944,\n",
       "  4,\n",
       "  7,\n",
       "  480,\n",
       "  1,\n",
       "  859,\n",
       "  108,\n",
       "  14,\n",
       "  1917,\n",
       "  2591],\n",
       " [1188, 1882, 783, 4, 819, 1426, 1052, 17, 35, 1845, 199, 1, 2606, 39],\n",
       " [1129, 3293, 2, 2303, 122, 449, 9, 1724, 4, 104, 78, 303, 3, 14, 3294],\n",
       " [2332, 2, 72, 63, 1, 39, 83, 1257, 3, 260, 1, 181],\n",
       " [104, 129, 341, 3, 48, 38, 949, 6, 1004, 99, 1382, 1, 84],\n",
       " [20, 13, 1, 7, 94, 1376, 1, 2],\n",
       " [4, 175, 4, 1962, 1327, 1, 39],\n",
       " [7, 1883, 2666, 2, 135, 67, 1, 566, 966, 476, 276],\n",
       " [991, 1, 4012, 3, 294, 10, 2788, 1476, 6, 4121, 16, 1015, 75, 17, 4, 3751],\n",
       " [702, 481, 1, 37, 460, 3500, 738],\n",
       " [1201, 1982, 7, 679, 104, 10, 398, 3, 2, 135, 842],\n",
       " [50, 367, 2, 5, 740, 1851, 2, 7, 36, 6, 41, 419, 47, 930],\n",
       " [5,\n",
       "  825,\n",
       "  2140,\n",
       "  3,\n",
       "  14,\n",
       "  4922,\n",
       "  1,\n",
       "  5,\n",
       "  620,\n",
       "  1,\n",
       "  77,\n",
       "  2,\n",
       "  390,\n",
       "  45,\n",
       "  44,\n",
       "  3061,\n",
       "  2,\n",
       "  4923,\n",
       "  1,\n",
       "  4924],\n",
       " [277, 2746, 2, 627, 3, 10, 127, 4072, 554, 3, 3595],\n",
       " [12, 8, 210, 163, 184, 2177, 2, 2835, 2836, 1, 520, 200, 2837],\n",
       " [468, 672, 547, 1, 2, 231, 1, 659, 126, 8, 1198, 1952, 1109],\n",
       " [450, 3632, 1, 152, 54, 11, 1232, 2, 3455],\n",
       " ...]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2761, 100)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp310-cp310-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 2.5 MB/s eta 0:00:00\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.1 smart-open-6.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec_model = Word2Vec.load('word2vec/word2vec_300dim_20epochs.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.33542043, -1.6471791 ,  0.82437915,  0.34260795,  0.5697868 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv['lalaki'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(model, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, index in word_index.items():\n",
    "        if word in model.wv.key_to_index:\n",
    "            embedding_matrix[index] = model.wv[word][:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embedding_matrix = create_embedding_matrix(\n",
    "    word2vec_model,\n",
    "    tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10714, 300)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8388090349075975"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 100, 300)          3214200   \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 94, 128)           268928    \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,484,429\n",
      "Trainable params: 270,229\n",
      "Non-trainable params: 3,214,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen, \n",
    "                           trainable=False))\n",
    "model.add(layers.Conv1D(128, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "#opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 2s 13ms/step - loss: 0.3633 - accuracy: 0.8468 - val_loss: 0.2925 - val_accuracy: 0.8784\n",
      "Epoch 2/50\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.0861 - accuracy: 0.9721 - val_loss: 0.3081 - val_accuracy: 0.8857\n",
      "Epoch 3/50\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.0283 - accuracy: 0.9953 - val_loss: 0.2943 - val_accuracy: 0.8915\n",
      "Epoch 4/50\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.0096 - accuracy: 0.9982 - val_loss: 0.3352 - val_accuracy: 0.8987\n",
      "Epoch 5/50\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.3652 - val_accuracy: 0.9001\n",
      "Epoch 6/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.3527 - val_accuracy: 0.9001\n",
      "Epoch 7/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.0033 - accuracy: 0.9986 - val_loss: 0.3654 - val_accuracy: 0.9016\n",
      "Epoch 8/50\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.0034 - accuracy: 0.9982 - val_loss: 0.3612 - val_accuracy: 0.9001\n",
      "Epoch 9/50\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.0030 - accuracy: 0.9986 - val_loss: 0.3702 - val_accuracy: 0.9030\n",
      "Epoch 10/50\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.0023 - accuracy: 0.9989 - val_loss: 0.3792 - val_accuracy: 0.9001\n",
      "Epoch 11/50\n",
      "87/87 [==============================] - 1s 12ms/step - loss: 0.0021 - accuracy: 0.9989 - val_loss: 0.3911 - val_accuracy: 0.9001\n",
      "Epoch 12/50\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.0020 - accuracy: 0.9989 - val_loss: 0.3953 - val_accuracy: 0.8987\n",
      "Epoch 13/50\n",
      "87/87 [==============================] - 1s 12ms/step - loss: 0.0019 - accuracy: 0.9989 - val_loss: 0.3846 - val_accuracy: 0.9016\n",
      "Epoch 14/50\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.0014 - accuracy: 0.9989 - val_loss: 0.3955 - val_accuracy: 0.8987\n",
      "Epoch 15/50\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.0012 - accuracy: 0.9989 - val_loss: 0.4051 - val_accuracy: 0.9001\n",
      "Epoch 16/50\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 9.3940e-04 - accuracy: 0.9993 - val_loss: 0.4220 - val_accuracy: 0.8987\n",
      "Epoch 17/50\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 9.1131e-04 - accuracy: 1.0000 - val_loss: 0.4268 - val_accuracy: 0.8987\n",
      "Epoch 18/50\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 3.1549e-04 - accuracy: 1.0000 - val_loss: 0.4278 - val_accuracy: 0.8987\n",
      "Epoch 19/50\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 2.1261e-04 - accuracy: 1.0000 - val_loss: 0.4333 - val_accuracy: 0.9001\n",
      "Epoch 20/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.7926e-04 - accuracy: 1.0000 - val_loss: 0.4344 - val_accuracy: 0.8987\n",
      "Epoch 21/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.5693e-04 - accuracy: 1.0000 - val_loss: 0.4428 - val_accuracy: 0.9001\n",
      "Epoch 22/50\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 1.3774e-04 - accuracy: 1.0000 - val_loss: 0.4483 - val_accuracy: 0.9001\n",
      "Epoch 23/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.2230e-04 - accuracy: 1.0000 - val_loss: 0.4503 - val_accuracy: 0.9001\n",
      "Epoch 24/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0935e-04 - accuracy: 1.0000 - val_loss: 0.4544 - val_accuracy: 0.9001\n",
      "Epoch 25/50\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 9.7582e-05 - accuracy: 1.0000 - val_loss: 0.4601 - val_accuracy: 0.9001\n",
      "Epoch 26/50\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 8.7833e-05 - accuracy: 1.0000 - val_loss: 0.4649 - val_accuracy: 0.9001\n",
      "Epoch 27/50\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 7.9372e-05 - accuracy: 1.0000 - val_loss: 0.4686 - val_accuracy: 0.9001\n",
      "Epoch 28/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 7.1867e-05 - accuracy: 1.0000 - val_loss: 0.4715 - val_accuracy: 0.9001\n",
      "Epoch 29/50\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 6.5365e-05 - accuracy: 1.0000 - val_loss: 0.4751 - val_accuracy: 0.9001\n",
      "Epoch 30/50\n",
      "87/87 [==============================] - 1s 12ms/step - loss: 5.9556e-05 - accuracy: 1.0000 - val_loss: 0.4788 - val_accuracy: 0.9001\n",
      "Epoch 31/50\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 5.4280e-05 - accuracy: 1.0000 - val_loss: 0.4845 - val_accuracy: 0.9001\n",
      "Epoch 32/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 4.9643e-05 - accuracy: 1.0000 - val_loss: 0.4875 - val_accuracy: 0.9001\n",
      "Epoch 33/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 4.5379e-05 - accuracy: 1.0000 - val_loss: 0.4921 - val_accuracy: 0.9001\n",
      "Epoch 34/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 4.1709e-05 - accuracy: 1.0000 - val_loss: 0.4934 - val_accuracy: 0.9001\n",
      "Epoch 35/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 3.8280e-05 - accuracy: 1.0000 - val_loss: 0.4968 - val_accuracy: 0.9001\n",
      "Epoch 36/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 3.5204e-05 - accuracy: 1.0000 - val_loss: 0.4987 - val_accuracy: 0.9016\n",
      "Epoch 37/50\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 3.2470e-05 - accuracy: 1.0000 - val_loss: 0.5068 - val_accuracy: 0.9001\n",
      "Epoch 38/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 2.9884e-05 - accuracy: 1.0000 - val_loss: 0.5096 - val_accuracy: 0.9001\n",
      "Epoch 39/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 2.7618e-05 - accuracy: 1.0000 - val_loss: 0.5136 - val_accuracy: 0.9001\n",
      "Epoch 40/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 2.5435e-05 - accuracy: 1.0000 - val_loss: 0.5161 - val_accuracy: 0.9001\n",
      "Epoch 41/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 2.3534e-05 - accuracy: 1.0000 - val_loss: 0.5179 - val_accuracy: 0.9001\n",
      "Epoch 42/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 2.1841e-05 - accuracy: 1.0000 - val_loss: 0.5229 - val_accuracy: 0.9001\n",
      "Epoch 43/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 2.0239e-05 - accuracy: 1.0000 - val_loss: 0.5251 - val_accuracy: 0.9001\n",
      "Epoch 44/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.8823e-05 - accuracy: 1.0000 - val_loss: 0.5290 - val_accuracy: 0.9001\n",
      "Epoch 45/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.7473e-05 - accuracy: 1.0000 - val_loss: 0.5319 - val_accuracy: 0.9001\n",
      "Epoch 46/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.6254e-05 - accuracy: 1.0000 - val_loss: 0.5378 - val_accuracy: 0.9001\n",
      "Epoch 47/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.5144e-05 - accuracy: 1.0000 - val_loss: 0.5383 - val_accuracy: 0.9001\n",
      "Epoch 48/50\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 1.4077e-05 - accuracy: 1.0000 - val_loss: 0.5423 - val_accuracy: 0.9001\n",
      "Epoch 49/50\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.3131e-05 - accuracy: 1.0000 - val_loss: 0.5448 - val_accuracy: 0.9001\n",
      "Epoch 50/50\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 1.2237e-05 - accuracy: 1.0000 - val_loss: 0.5472 - val_accuracy: 0.9001\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy:  0.9001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# STAMP = f'CNN_Glove_Vectors'\n",
    "# early_stopping =EarlyStopping(monitor='val_loss', patience=5)\n",
    "# bst_model_path = STAMP + '.h5'\n",
    "# model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "#, callbacks=[early_stopping, model_checkpoint]\n",
    "\n",
    "#model.load_weights(bst_model_path)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 300)          3214200   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              439296    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,656,077\n",
      "Trainable params: 3,656,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RNN_model = Sequential()\n",
    "RNN_model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "RNN_model.add(layers.Bidirectional(layers.LSTM(128)))\n",
    "RNN_model.add(layers.Dense(10, activation='relu'))\n",
    "RNN_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "RNN_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "RNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "87/87 [==============================] - 6s 30ms/step - loss: 0.3845 - accuracy: 0.8156 - val_loss: 0.2659 - val_accuracy: 0.8871\n",
      "Epoch 2/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 0.1245 - accuracy: 0.9565 - val_loss: 0.2286 - val_accuracy: 0.9190\n",
      "Epoch 3/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 0.0689 - accuracy: 0.9772 - val_loss: 0.2258 - val_accuracy: 0.9190\n",
      "Epoch 4/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 0.0360 - accuracy: 0.9899 - val_loss: 0.4316 - val_accuracy: 0.8857\n",
      "Epoch 5/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 0.0229 - accuracy: 0.9931 - val_loss: 0.3311 - val_accuracy: 0.9161\n",
      "Epoch 6/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 0.0110 - accuracy: 0.9975 - val_loss: 0.3773 - val_accuracy: 0.91320096 - \n",
      "Epoch 7/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 0.0058 - accuracy: 0.9986 - val_loss: 0.4113 - val_accuracy: 0.9247\n",
      "Epoch 8/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.4951 - val_accuracy: 0.9161\n",
      "Epoch 9/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 8.5394e-04 - accuracy: 0.9996 - val_loss: 0.6092 - val_accuracy: 0.9117\n",
      "Epoch 10/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.1918e-05 - accuracy: 1.0000 - val_loss: 0.7085 - val_accuracy: 0.9146\n",
      "Epoch 11/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.7868e-04 - accuracy: 1.0000 - val_loss: 0.7664 - val_accuracy: 0.9161\n",
      "Epoch 12/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.5574e-07 - accuracy: 1.0000 - val_loss: 0.8123 - val_accuracy: 0.9132\n",
      "Epoch 13/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.8854 - val_accuracy: 0.9103\n",
      "Epoch 14/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 9.8411e-08 - accuracy: 1.0000 - val_loss: 0.8877 - val_accuracy: 0.9103\n",
      "Epoch 15/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 5.4797e-08 - accuracy: 1.0000 - val_loss: 0.9397 - val_accuracy: 0.9103\n",
      "Epoch 16/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.5955e-08 - accuracy: 1.0000 - val_loss: 0.9706 - val_accuracy: 0.9161\n",
      "Epoch 17/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.9447e-09 - accuracy: 1.0000 - val_loss: 1.0054 - val_accuracy: 0.9175\n",
      "Epoch 18/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.6734e-09 - accuracy: 1.0000 - val_loss: 1.0242 - val_accuracy: 0.9161\n",
      "Epoch 19/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.7245e-09 - accuracy: 1.0000 - val_loss: 1.0143 - val_accuracy: 0.9146\n",
      "Epoch 20/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.3055e-09 - accuracy: 1.0000 - val_loss: 1.0351 - val_accuracy: 0.9161\n",
      "Epoch 21/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.0363e-09 - accuracy: 1.0000 - val_loss: 1.0549 - val_accuracy: 0.9161\n",
      "Epoch 22/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 8.7398e-10 - accuracy: 1.0000 - val_loss: 1.0558 - val_accuracy: 0.9161\n",
      "Epoch 23/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 7.4296e-10 - accuracy: 1.0000 - val_loss: 1.0620 - val_accuracy: 0.9161\n",
      "Epoch 24/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 6.3552e-10 - accuracy: 1.0000 - val_loss: 1.0775 - val_accuracy: 0.9146\n",
      "Epoch 25/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 5.9361e-10 - accuracy: 1.0000 - val_loss: 1.0710 - val_accuracy: 0.9146\n",
      "Epoch 26/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 5.2610e-10 - accuracy: 1.0000 - val_loss: 1.0786 - val_accuracy: 0.9146\n",
      "Epoch 27/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 4.7863e-10 - accuracy: 1.0000 - val_loss: 1.0866 - val_accuracy: 0.9146\n",
      "Epoch 28/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.4560e-10 - accuracy: 1.0000 - val_loss: 1.0901 - val_accuracy: 0.9132\n",
      "Epoch 29/50\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 4.1310e-10 - accuracy: 1.0000 - val_loss: 1.0944 - val_accuracy: 0.9146\n",
      "Epoch 30/50\n",
      "87/87 [==============================] - 2s 24ms/step - loss: 3.8877e-10 - accuracy: 1.0000 - val_loss: 1.0941 - val_accuracy: 0.9132\n",
      "Epoch 31/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.6516e-10 - accuracy: 1.0000 - val_loss: 1.0961 - val_accuracy: 0.9132\n",
      "Epoch 32/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 3.4691e-10 - accuracy: 1.0000 - val_loss: 1.0966 - val_accuracy: 0.9117\n",
      "Epoch 33/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 3.2686e-10 - accuracy: 1.0000 - val_loss: 1.0965 - val_accuracy: 0.9117\n",
      "Epoch 34/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 3.1054e-10 - accuracy: 1.0000 - val_loss: 1.0971 - val_accuracy: 0.9117\n",
      "Epoch 35/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.9970e-10 - accuracy: 1.0000 - val_loss: 1.0975 - val_accuracy: 0.9117\n",
      "Epoch 36/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.8241e-10 - accuracy: 1.0000 - val_loss: 1.0969 - val_accuracy: 0.9132\n",
      "Epoch 37/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.6828e-10 - accuracy: 1.0000 - val_loss: 1.1026 - val_accuracy: 0.9117\n",
      "Epoch 38/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.5586e-10 - accuracy: 1.0000 - val_loss: 1.1078 - val_accuracy: 0.9117\n",
      "Epoch 39/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.4897e-10 - accuracy: 1.0000 - val_loss: 1.1107 - val_accuracy: 0.9117\n",
      "Epoch 40/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.4160e-10 - accuracy: 1.0000 - val_loss: 1.1122 - val_accuracy: 0.9117\n",
      "Epoch 41/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.3337e-10 - accuracy: 1.0000 - val_loss: 1.1152 - val_accuracy: 0.9117\n",
      "Epoch 42/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.2511e-10 - accuracy: 1.0000 - val_loss: 1.1105 - val_accuracy: 0.9117\n",
      "Epoch 43/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.1873e-10 - accuracy: 1.0000 - val_loss: 1.1202 - val_accuracy: 0.9132\n",
      "Epoch 44/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.1646e-10 - accuracy: 1.0000 - val_loss: 1.1231 - val_accuracy: 0.9146\n",
      "Epoch 45/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.1546e-10 - accuracy: 1.0000 - val_loss: 1.1225 - val_accuracy: 0.9146\n",
      "Epoch 46/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.0910e-10 - accuracy: 1.0000 - val_loss: 1.1223 - val_accuracy: 0.9132\n",
      "Epoch 47/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.0340e-10 - accuracy: 1.0000 - val_loss: 1.1326 - val_accuracy: 0.9146\n",
      "Epoch 48/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.0559e-10 - accuracy: 1.0000 - val_loss: 1.1227 - val_accuracy: 0.9132\n",
      "Epoch 49/50\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.9551e-10 - accuracy: 1.0000 - val_loss: 1.1240 - val_accuracy: 0.9132\n",
      "Epoch 50/50\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.9184e-10 - accuracy: 1.0000 - val_loss: 1.1272 - val_accuracy: 0.9132\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy:  0.9132\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = RNN_model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=32)\n",
    "\n",
    "\n",
    "loss, accuracy = RNN_model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = RNN_model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_CNNModel(hl, ol, act, num_filter, kernel_size):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "    model.add(layers.Conv1D(num_filter, kernel_size, activation=hl))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(24, activation=hl))\n",
    "    model.add(layers.Dense(1, activation=ol))\n",
    "    model.compile(optimizer=act,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1/1125 done, hidden = sigmoid, output = sigmoid, optim = SGD || accuracy: 0.502170741558075\n",
      "model 2/1125 done, hidden = sigmoid, output = sigmoid, optim = SGD || accuracy: 0.502170741558075\n",
      "model 3/1125 done, hidden = sigmoid, output = sigmoid, optim = SGD || accuracy: 0.5166425704956055\n",
      "model 4/1125 done, hidden = sigmoid, output = sigmoid, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 5/1125 done, hidden = sigmoid, output = sigmoid, optim = SGD || accuracy: 0.502170741558075\n",
      "model 6/1125 done, hidden = sigmoid, output = sigmoid, optim = SGD || accuracy: 0.6685962080955505\n",
      "model 7/1125 done, hidden = sigmoid, output = sigmoid, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 8/1125 done, hidden = sigmoid, output = sigmoid, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 9/1125 done, hidden = sigmoid, output = sigmoid, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 10/1125 done, hidden = sigmoid, output = sigmoid, optim = RMSprop || accuracy: 0.9059334397315979\n",
      "model 11/1125 done, hidden = sigmoid, output = sigmoid, optim = RMSprop || accuracy: 0.9044862389564514\n",
      "model 12/1125 done, hidden = sigmoid, output = sigmoid, optim = RMSprop || accuracy: 0.9001446962356567\n",
      "model 13/1125 done, hidden = sigmoid, output = sigmoid, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 14/1125 done, hidden = sigmoid, output = sigmoid, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 15/1125 done, hidden = sigmoid, output = sigmoid, optim = RMSprop || accuracy: 0.9001446962356567\n",
      "model 16/1125 done, hidden = sigmoid, output = sigmoid, optim = RMSprop || accuracy: 0.8929088115692139\n",
      "model 17/1125 done, hidden = sigmoid, output = sigmoid, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 18/1125 done, hidden = sigmoid, output = sigmoid, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 19/1125 done, hidden = sigmoid, output = sigmoid, optim = Adam || accuracy: 0.9044862389564514\n",
      "model 20/1125 done, hidden = sigmoid, output = sigmoid, optim = Adam || accuracy: 0.9015918970108032\n",
      "model 21/1125 done, hidden = sigmoid, output = sigmoid, optim = Adam || accuracy: 0.916063666343689\n",
      "model 22/1125 done, hidden = sigmoid, output = sigmoid, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 23/1125 done, hidden = sigmoid, output = sigmoid, optim = Adam || accuracy: 0.8900144696235657\n",
      "model 24/1125 done, hidden = sigmoid, output = sigmoid, optim = Adam || accuracy: 0.9044862389564514\n",
      "model 25/1125 done, hidden = sigmoid, output = sigmoid, optim = Adam || accuracy: 0.898697555065155\n",
      "model 26/1125 done, hidden = sigmoid, output = sigmoid, optim = Adam || accuracy: 0.9059334397315979\n",
      "model 27/1125 done, hidden = sigmoid, output = sigmoid, optim = Adam || accuracy: 0.9102749824523926\n",
      "model 28/1125 done, hidden = sigmoid, output = sigmoid, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 29/1125 done, hidden = sigmoid, output = sigmoid, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 30/1125 done, hidden = sigmoid, output = sigmoid, optim = Adagrad || accuracy: 0.49927639961242676\n",
      "model 31/1125 done, hidden = sigmoid, output = sigmoid, optim = Adagrad || accuracy: 0.502170741558075\n",
      "model 32/1125 done, hidden = sigmoid, output = sigmoid, optim = Adagrad || accuracy: 0.502170741558075\n",
      "model 33/1125 done, hidden = sigmoid, output = sigmoid, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 34/1125 done, hidden = sigmoid, output = sigmoid, optim = Adagrad || accuracy: 0.502170741558075\n",
      "model 35/1125 done, hidden = sigmoid, output = sigmoid, optim = Adagrad || accuracy: 0.4920405149459839\n",
      "model 36/1125 done, hidden = sigmoid, output = sigmoid, optim = Adagrad || accuracy: 0.502170741558075\n",
      "model 37/1125 done, hidden = sigmoid, output = sigmoid, optim = Adamax || accuracy: 0.8784370422363281\n",
      "model 38/1125 done, hidden = sigmoid, output = sigmoid, optim = Adamax || accuracy: 0.9088277816772461\n",
      "model 39/1125 done, hidden = sigmoid, output = sigmoid, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 40/1125 done, hidden = sigmoid, output = sigmoid, optim = Adamax || accuracy: 0.8885672688484192\n",
      "model 41/1125 done, hidden = sigmoid, output = sigmoid, optim = Adamax || accuracy: 0.9146165251731873\n",
      "model 42/1125 done, hidden = sigmoid, output = sigmoid, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 43/1125 done, hidden = sigmoid, output = sigmoid, optim = Adamax || accuracy: 0.898697555065155\n",
      "model 44/1125 done, hidden = sigmoid, output = sigmoid, optim = Adamax || accuracy: 0.9088277816772461\n",
      "model 45/1125 done, hidden = sigmoid, output = sigmoid, optim = Adamax || accuracy: 0.9088277816772461\n",
      "model 46/1125 done, hidden = sigmoid, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 47/1125 done, hidden = sigmoid, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 48/1125 done, hidden = sigmoid, output = tanh, optim = SGD || accuracy: 0.502170741558075\n",
      "model 49/1125 done, hidden = sigmoid, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 50/1125 done, hidden = sigmoid, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 51/1125 done, hidden = sigmoid, output = tanh, optim = SGD || accuracy: 0.502170741558075\n",
      "model 52/1125 done, hidden = sigmoid, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 53/1125 done, hidden = sigmoid, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 54/1125 done, hidden = sigmoid, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 55/1125 done, hidden = sigmoid, output = tanh, optim = RMSprop || accuracy: 0.8755427002906799\n",
      "model 56/1125 done, hidden = sigmoid, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 57/1125 done, hidden = sigmoid, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 58/1125 done, hidden = sigmoid, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 59/1125 done, hidden = sigmoid, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 60/1125 done, hidden = sigmoid, output = tanh, optim = RMSprop || accuracy: 0.8523878455162048\n",
      "model 61/1125 done, hidden = sigmoid, output = tanh, optim = RMSprop || accuracy: 0.8494935035705566\n",
      "model 62/1125 done, hidden = sigmoid, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 63/1125 done, hidden = sigmoid, output = tanh, optim = RMSprop || accuracy: 0.8480463027954102\n",
      "model 64/1125 done, hidden = sigmoid, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 65/1125 done, hidden = sigmoid, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 66/1125 done, hidden = sigmoid, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 67/1125 done, hidden = sigmoid, output = tanh, optim = Adam || accuracy: 0.8740954995155334\n",
      "model 68/1125 done, hidden = sigmoid, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 69/1125 done, hidden = sigmoid, output = tanh, optim = Adam || accuracy: 0.8726483583450317\n",
      "model 70/1125 done, hidden = sigmoid, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 71/1125 done, hidden = sigmoid, output = tanh, optim = Adam || accuracy: 0.8871201276779175\n",
      "model 72/1125 done, hidden = sigmoid, output = tanh, optim = Adam || accuracy: 0.8784370422363281\n",
      "model 73/1125 done, hidden = sigmoid, output = tanh, optim = Adagrad || accuracy: 0.5123010277748108\n",
      "model 74/1125 done, hidden = sigmoid, output = tanh, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 75/1125 done, hidden = sigmoid, output = tanh, optim = Adagrad || accuracy: 0.5007236003875732\n",
      "model 76/1125 done, hidden = sigmoid, output = tanh, optim = Adagrad || accuracy: 0.5325614809989929\n",
      "model 77/1125 done, hidden = sigmoid, output = tanh, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 78/1125 done, hidden = sigmoid, output = tanh, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 79/1125 done, hidden = sigmoid, output = tanh, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 80/1125 done, hidden = sigmoid, output = tanh, optim = Adagrad || accuracy: 0.5007236003875732\n",
      "model 81/1125 done, hidden = sigmoid, output = tanh, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 82/1125 done, hidden = sigmoid, output = tanh, optim = Adamax || accuracy: 0.8813313841819763\n",
      "model 83/1125 done, hidden = sigmoid, output = tanh, optim = Adamax || accuracy: 0.8871201276779175\n",
      "model 84/1125 done, hidden = sigmoid, output = tanh, optim = Adamax || accuracy: 0.8943560123443604\n",
      "model 85/1125 done, hidden = sigmoid, output = tanh, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 86/1125 done, hidden = sigmoid, output = tanh, optim = Adamax || accuracy: 0.8871201276779175\n",
      "model 87/1125 done, hidden = sigmoid, output = tanh, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 88/1125 done, hidden = sigmoid, output = tanh, optim = Adamax || accuracy: 0.8827785849571228\n",
      "model 89/1125 done, hidden = sigmoid, output = tanh, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 90/1125 done, hidden = sigmoid, output = tanh, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 91/1125 done, hidden = sigmoid, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 92/1125 done, hidden = sigmoid, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 93/1125 done, hidden = sigmoid, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 94/1125 done, hidden = sigmoid, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 95/1125 done, hidden = sigmoid, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 96/1125 done, hidden = sigmoid, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 97/1125 done, hidden = sigmoid, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 98/1125 done, hidden = sigmoid, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 99/1125 done, hidden = sigmoid, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 100/1125 done, hidden = sigmoid, output = relu, optim = RMSprop || accuracy: 0.9030390977859497\n",
      "model 101/1125 done, hidden = sigmoid, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 102/1125 done, hidden = sigmoid, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 103/1125 done, hidden = sigmoid, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 104/1125 done, hidden = sigmoid, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 105/1125 done, hidden = sigmoid, output = relu, optim = RMSprop || accuracy: 0.9044862389564514\n",
      "model 106/1125 done, hidden = sigmoid, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 107/1125 done, hidden = sigmoid, output = relu, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 108/1125 done, hidden = sigmoid, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 109/1125 done, hidden = sigmoid, output = relu, optim = Adam || accuracy: 0.9044862389564514\n",
      "model 110/1125 done, hidden = sigmoid, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 111/1125 done, hidden = sigmoid, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 112/1125 done, hidden = sigmoid, output = relu, optim = Adam || accuracy: 0.502170741558075\n",
      "model 113/1125 done, hidden = sigmoid, output = relu, optim = Adam || accuracy: 0.9073805809020996\n",
      "model 114/1125 done, hidden = sigmoid, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 115/1125 done, hidden = sigmoid, output = relu, optim = Adam || accuracy: 0.8885672688484192\n",
      "model 116/1125 done, hidden = sigmoid, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 117/1125 done, hidden = sigmoid, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 118/1125 done, hidden = sigmoid, output = relu, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 119/1125 done, hidden = sigmoid, output = relu, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 120/1125 done, hidden = sigmoid, output = relu, optim = Adagrad || accuracy: 0.555716335773468\n",
      "model 121/1125 done, hidden = sigmoid, output = relu, optim = Adagrad || accuracy: 0.5036179423332214\n",
      "model 122/1125 done, hidden = sigmoid, output = relu, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 123/1125 done, hidden = sigmoid, output = relu, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 124/1125 done, hidden = sigmoid, output = relu, optim = Adagrad || accuracy: 0.558610737323761\n",
      "model 125/1125 done, hidden = sigmoid, output = relu, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 126/1125 done, hidden = sigmoid, output = relu, optim = Adagrad || accuracy: 0.5571635365486145\n",
      "model 127/1125 done, hidden = sigmoid, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 128/1125 done, hidden = sigmoid, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 129/1125 done, hidden = sigmoid, output = relu, optim = Adamax || accuracy: 0.898697555065155\n",
      "model 130/1125 done, hidden = sigmoid, output = relu, optim = Adamax || accuracy: 0.8827785849571228\n",
      "model 131/1125 done, hidden = sigmoid, output = relu, optim = Adamax || accuracy: 0.9001446962356567\n",
      "model 132/1125 done, hidden = sigmoid, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 133/1125 done, hidden = sigmoid, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 134/1125 done, hidden = sigmoid, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 135/1125 done, hidden = sigmoid, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 136/1125 done, hidden = sigmoid, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 137/1125 done, hidden = sigmoid, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 138/1125 done, hidden = sigmoid, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 139/1125 done, hidden = sigmoid, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 140/1125 done, hidden = sigmoid, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 141/1125 done, hidden = sigmoid, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 142/1125 done, hidden = sigmoid, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 143/1125 done, hidden = sigmoid, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 144/1125 done, hidden = sigmoid, output = LeakyReLU, optim = SGD || accuracy: 0.5383502244949341\n",
      "model 145/1125 done, hidden = sigmoid, output = LeakyReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 146/1125 done, hidden = sigmoid, output = LeakyReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 147/1125 done, hidden = sigmoid, output = LeakyReLU, optim = RMSprop || accuracy: 0.9088277816772461\n",
      "model 148/1125 done, hidden = sigmoid, output = LeakyReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 149/1125 done, hidden = sigmoid, output = LeakyReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 150/1125 done, hidden = sigmoid, output = LeakyReLU, optim = RMSprop || accuracy: 0.502170741558075\n",
      "model 151/1125 done, hidden = sigmoid, output = LeakyReLU, optim = RMSprop || accuracy: 0.8914616703987122\n",
      "model 152/1125 done, hidden = sigmoid, output = LeakyReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 153/1125 done, hidden = sigmoid, output = LeakyReLU, optim = RMSprop || accuracy: 0.9088277816772461\n",
      "model 154/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adam || accuracy: 0.502170741558075\n",
      "model 155/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adam || accuracy: 0.9044862389564514\n",
      "model 156/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 157/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adam || accuracy: 0.8914616703987122\n",
      "model 158/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adam || accuracy: 0.9059334397315979\n",
      "model 159/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adam || accuracy: 0.502170741558075\n",
      "model 160/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adam || accuracy: 0.8958032131195068\n",
      "model 161/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adam || accuracy: 0.502170741558075\n",
      "model 162/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 163/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adagrad || accuracy: 0.5354558825492859\n",
      "model 164/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 165/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adagrad || accuracy: 0.6468885540962219\n",
      "model 166/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 167/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 168/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adagrad || accuracy: 0.589001476764679\n",
      "model 169/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 170/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 171/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adagrad || accuracy: 0.6005788445472717\n",
      "model 172/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adamax || accuracy: 0.8900144696235657\n",
      "model 173/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 174/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adamax || accuracy: 0.502170741558075\n",
      "model 175/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 176/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 177/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adamax || accuracy: 0.9146165251731873\n",
      "model 178/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 179/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adamax || accuracy: 0.8914616703987122\n",
      "model 180/1125 done, hidden = sigmoid, output = LeakyReLU, optim = Adamax || accuracy: 0.502170741558075\n",
      "model 181/1125 done, hidden = sigmoid, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 182/1125 done, hidden = sigmoid, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 183/1125 done, hidden = sigmoid, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 184/1125 done, hidden = sigmoid, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 185/1125 done, hidden = sigmoid, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 186/1125 done, hidden = sigmoid, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 187/1125 done, hidden = sigmoid, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 188/1125 done, hidden = sigmoid, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 189/1125 done, hidden = sigmoid, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 190/1125 done, hidden = sigmoid, output = PReLU, optim = RMSprop || accuracy: 0.8900144696235657\n",
      "model 191/1125 done, hidden = sigmoid, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 192/1125 done, hidden = sigmoid, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 193/1125 done, hidden = sigmoid, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 194/1125 done, hidden = sigmoid, output = PReLU, optim = RMSprop || accuracy: 0.9073805809020996\n",
      "model 195/1125 done, hidden = sigmoid, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 196/1125 done, hidden = sigmoid, output = PReLU, optim = RMSprop || accuracy: 0.8958032131195068\n",
      "model 197/1125 done, hidden = sigmoid, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 198/1125 done, hidden = sigmoid, output = PReLU, optim = RMSprop || accuracy: 0.9030390977859497\n",
      "model 199/1125 done, hidden = sigmoid, output = PReLU, optim = Adam || accuracy: 0.502170741558075\n",
      "model 200/1125 done, hidden = sigmoid, output = PReLU, optim = Adam || accuracy: 0.9044862389564514\n",
      "model 201/1125 done, hidden = sigmoid, output = PReLU, optim = Adam || accuracy: 0.9131693243980408\n",
      "model 202/1125 done, hidden = sigmoid, output = PReLU, optim = Adam || accuracy: 0.8929088115692139\n",
      "model 203/1125 done, hidden = sigmoid, output = PReLU, optim = Adam || accuracy: 0.9059334397315979\n",
      "model 204/1125 done, hidden = sigmoid, output = PReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 205/1125 done, hidden = sigmoid, output = PReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 206/1125 done, hidden = sigmoid, output = PReLU, optim = Adam || accuracy: 0.9044862389564514\n",
      "model 207/1125 done, hidden = sigmoid, output = PReLU, optim = Adam || accuracy: 0.502170741558075\n",
      "model 208/1125 done, hidden = sigmoid, output = PReLU, optim = Adagrad || accuracy: 0.502170741558075\n",
      "model 209/1125 done, hidden = sigmoid, output = PReLU, optim = Adagrad || accuracy: 0.5759768486022949\n",
      "model 210/1125 done, hidden = sigmoid, output = PReLU, optim = Adagrad || accuracy: 0.5036179423332214\n",
      "model 211/1125 done, hidden = sigmoid, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 212/1125 done, hidden = sigmoid, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 213/1125 done, hidden = sigmoid, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 214/1125 done, hidden = sigmoid, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 215/1125 done, hidden = sigmoid, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 216/1125 done, hidden = sigmoid, output = PReLU, optim = Adagrad || accuracy: 0.502170741558075\n",
      "model 217/1125 done, hidden = sigmoid, output = PReLU, optim = Adamax || accuracy: 0.8914616703987122\n",
      "model 218/1125 done, hidden = sigmoid, output = PReLU, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 219/1125 done, hidden = sigmoid, output = PReLU, optim = Adamax || accuracy: 0.8885672688484192\n",
      "model 220/1125 done, hidden = sigmoid, output = PReLU, optim = Adamax || accuracy: 0.8871201276779175\n",
      "model 221/1125 done, hidden = sigmoid, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 222/1125 done, hidden = sigmoid, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 223/1125 done, hidden = sigmoid, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 224/1125 done, hidden = sigmoid, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 225/1125 done, hidden = sigmoid, output = PReLU, optim = Adamax || accuracy: 0.9001446962356567\n",
      "model 226/1125 done, hidden = tanh, output = sigmoid, optim = SGD || accuracy: 0.7018813490867615\n",
      "model 227/1125 done, hidden = tanh, output = sigmoid, optim = SGD || accuracy: 0.740955114364624\n",
      "model 228/1125 done, hidden = tanh, output = sigmoid, optim = SGD || accuracy: 0.7424023151397705\n",
      "model 229/1125 done, hidden = tanh, output = sigmoid, optim = SGD || accuracy: 0.7525325417518616\n",
      "model 230/1125 done, hidden = tanh, output = sigmoid, optim = SGD || accuracy: 0.7626628279685974\n",
      "model 231/1125 done, hidden = tanh, output = sigmoid, optim = SGD || accuracy: 0.7626628279685974\n",
      "model 232/1125 done, hidden = tanh, output = sigmoid, optim = SGD || accuracy: 0.7452966570854187\n",
      "model 233/1125 done, hidden = tanh, output = sigmoid, optim = SGD || accuracy: 0.7698987126350403\n",
      "model 234/1125 done, hidden = tanh, output = sigmoid, optim = SGD || accuracy: 0.7655571699142456\n",
      "model 235/1125 done, hidden = tanh, output = sigmoid, optim = RMSprop || accuracy: 0.9131693243980408\n",
      "model 236/1125 done, hidden = tanh, output = sigmoid, optim = RMSprop || accuracy: 0.9059334397315979\n",
      "model 237/1125 done, hidden = tanh, output = sigmoid, optim = RMSprop || accuracy: 0.9146165251731873\n",
      "model 238/1125 done, hidden = tanh, output = sigmoid, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 239/1125 done, hidden = tanh, output = sigmoid, optim = RMSprop || accuracy: 0.9059334397315979\n",
      "model 240/1125 done, hidden = tanh, output = sigmoid, optim = RMSprop || accuracy: 0.9175108671188354\n",
      "model 241/1125 done, hidden = tanh, output = sigmoid, optim = RMSprop || accuracy: 0.9073805809020996\n",
      "model 242/1125 done, hidden = tanh, output = sigmoid, optim = RMSprop || accuracy: 0.9146165251731873\n",
      "model 243/1125 done, hidden = tanh, output = sigmoid, optim = RMSprop || accuracy: 0.9030390977859497\n",
      "model 244/1125 done, hidden = tanh, output = sigmoid, optim = Adam || accuracy: 0.898697555065155\n",
      "model 245/1125 done, hidden = tanh, output = sigmoid, optim = Adam || accuracy: 0.9059334397315979\n",
      "model 246/1125 done, hidden = tanh, output = sigmoid, optim = Adam || accuracy: 0.8958032131195068\n",
      "model 247/1125 done, hidden = tanh, output = sigmoid, optim = Adam || accuracy: 0.9059334397315979\n",
      "model 248/1125 done, hidden = tanh, output = sigmoid, optim = Adam || accuracy: 0.9015918970108032\n",
      "model 249/1125 done, hidden = tanh, output = sigmoid, optim = Adam || accuracy: 0.9131693243980408\n",
      "model 250/1125 done, hidden = tanh, output = sigmoid, optim = Adam || accuracy: 0.9015918970108032\n",
      "model 251/1125 done, hidden = tanh, output = sigmoid, optim = Adam || accuracy: 0.9146165251731873\n",
      "model 252/1125 done, hidden = tanh, output = sigmoid, optim = Adam || accuracy: 0.9102749824523926\n",
      "model 253/1125 done, hidden = tanh, output = sigmoid, optim = Adagrad || accuracy: 0.7221418023109436\n",
      "model 254/1125 done, hidden = tanh, output = sigmoid, optim = Adagrad || accuracy: 0.7149059176445007\n",
      "model 255/1125 done, hidden = tanh, output = sigmoid, optim = Adagrad || accuracy: 0.7163531184196472\n",
      "model 256/1125 done, hidden = tanh, output = sigmoid, optim = Adagrad || accuracy: 0.7192474603652954\n",
      "model 257/1125 done, hidden = tanh, output = sigmoid, optim = Adagrad || accuracy: 0.7206946611404419\n",
      "model 258/1125 done, hidden = tanh, output = sigmoid, optim = Adagrad || accuracy: 0.7293776869773865\n",
      "model 259/1125 done, hidden = tanh, output = sigmoid, optim = Adagrad || accuracy: 0.710564374923706\n",
      "model 260/1125 done, hidden = tanh, output = sigmoid, optim = Adagrad || accuracy: 0.7395079731941223\n",
      "model 261/1125 done, hidden = tanh, output = sigmoid, optim = Adagrad || accuracy: 0.730824887752533\n",
      "model 262/1125 done, hidden = tanh, output = sigmoid, optim = Adamax || accuracy: 0.9102749824523926\n",
      "model 263/1125 done, hidden = tanh, output = sigmoid, optim = Adamax || accuracy: 0.9015918970108032\n",
      "model 264/1125 done, hidden = tanh, output = sigmoid, optim = Adamax || accuracy: 0.9088277816772461\n",
      "model 265/1125 done, hidden = tanh, output = sigmoid, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 266/1125 done, hidden = tanh, output = sigmoid, optim = Adamax || accuracy: 0.9030390977859497\n",
      "model 267/1125 done, hidden = tanh, output = sigmoid, optim = Adamax || accuracy: 0.9117221236228943\n",
      "model 268/1125 done, hidden = tanh, output = sigmoid, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 269/1125 done, hidden = tanh, output = sigmoid, optim = Adamax || accuracy: 0.9001446962356567\n",
      "model 270/1125 done, hidden = tanh, output = sigmoid, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 271/1125 done, hidden = tanh, output = tanh, optim = SGD || accuracy: 0.6772792935371399\n",
      "model 272/1125 done, hidden = tanh, output = tanh, optim = SGD || accuracy: 0.7973950505256653\n",
      "model 273/1125 done, hidden = tanh, output = tanh, optim = SGD || accuracy: 0.8335745334625244\n",
      "model 274/1125 done, hidden = tanh, output = tanh, optim = SGD || accuracy: 0.8523878455162048\n",
      "model 275/1125 done, hidden = tanh, output = tanh, optim = SGD || accuracy: 0.8364688754081726\n",
      "model 276/1125 done, hidden = tanh, output = tanh, optim = SGD || accuracy: 0.8567293882369995\n",
      "model 277/1125 done, hidden = tanh, output = tanh, optim = SGD || accuracy: 0.8364688754081726\n",
      "model 278/1125 done, hidden = tanh, output = tanh, optim = SGD || accuracy: 0.8422576189041138\n",
      "model 279/1125 done, hidden = tanh, output = tanh, optim = SGD || accuracy: 0.855282187461853\n",
      "model 280/1125 done, hidden = tanh, output = tanh, optim = RMSprop || accuracy: 0.7872648239135742\n",
      "model 281/1125 done, hidden = tanh, output = tanh, optim = RMSprop || accuracy: 0.7771345973014832\n",
      "model 282/1125 done, hidden = tanh, output = tanh, optim = RMSprop || accuracy: 0.8248914480209351\n",
      "model 283/1125 done, hidden = tanh, output = tanh, optim = RMSprop || accuracy: 0.8031837940216064\n",
      "model 284/1125 done, hidden = tanh, output = tanh, optim = RMSprop || accuracy: 0.7756873965263367\n",
      "model 285/1125 done, hidden = tanh, output = tanh, optim = RMSprop || accuracy: 0.8089724779129028\n",
      "model 286/1125 done, hidden = tanh, output = tanh, optim = RMSprop || accuracy: 0.784370481967926\n",
      "model 287/1125 done, hidden = tanh, output = tanh, optim = RMSprop || accuracy: 0.7800289392471313\n",
      "model 288/1125 done, hidden = tanh, output = tanh, optim = RMSprop || accuracy: 0.7973950505256653\n",
      "model 289/1125 done, hidden = tanh, output = tanh, optim = Adam || accuracy: 0.8654124736785889\n",
      "model 290/1125 done, hidden = tanh, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 291/1125 done, hidden = tanh, output = tanh, optim = Adam || accuracy: 0.8712011575698853\n",
      "model 292/1125 done, hidden = tanh, output = tanh, optim = Adam || accuracy: 0.8581765294075012\n",
      "model 293/1125 done, hidden = tanh, output = tanh, optim = Adam || accuracy: 0.8900144696235657\n",
      "model 294/1125 done, hidden = tanh, output = tanh, optim = Adam || accuracy: 0.9030390977859497\n",
      "model 295/1125 done, hidden = tanh, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 296/1125 done, hidden = tanh, output = tanh, optim = Adam || accuracy: 0.8842257857322693\n",
      "model 297/1125 done, hidden = tanh, output = tanh, optim = Adam || accuracy: 0.9059334397315979\n",
      "model 298/1125 done, hidden = tanh, output = tanh, optim = Adagrad || accuracy: 0.7047756910324097\n",
      "model 299/1125 done, hidden = tanh, output = tanh, optim = Adagrad || accuracy: 0.7192474603652954\n",
      "model 300/1125 done, hidden = tanh, output = tanh, optim = Adagrad || accuracy: 0.7279305458068848\n",
      "model 301/1125 done, hidden = tanh, output = tanh, optim = Adagrad || accuracy: 0.7481909990310669\n",
      "model 302/1125 done, hidden = tanh, output = tanh, optim = Adagrad || accuracy: 0.7366136312484741\n",
      "model 303/1125 done, hidden = tanh, output = tanh, optim = Adagrad || accuracy: 0.7510854005813599\n",
      "model 304/1125 done, hidden = tanh, output = tanh, optim = Adagrad || accuracy: 0.7612156271934509\n",
      "model 305/1125 done, hidden = tanh, output = tanh, optim = Adagrad || accuracy: 0.7337192296981812\n",
      "model 306/1125 done, hidden = tanh, output = tanh, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 307/1125 done, hidden = tanh, output = tanh, optim = Adamax || accuracy: 0.8610709309577942\n",
      "model 308/1125 done, hidden = tanh, output = tanh, optim = Adamax || accuracy: 0.8654124736785889\n",
      "model 309/1125 done, hidden = tanh, output = tanh, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 310/1125 done, hidden = tanh, output = tanh, optim = Adamax || accuracy: 0.8769898414611816\n",
      "model 311/1125 done, hidden = tanh, output = tanh, optim = Adamax || accuracy: 0.855282187461853\n",
      "model 312/1125 done, hidden = tanh, output = tanh, optim = Adamax || accuracy: 0.8639652729034424\n",
      "model 313/1125 done, hidden = tanh, output = tanh, optim = Adamax || accuracy: 0.8668596148490906\n",
      "model 314/1125 done, hidden = tanh, output = tanh, optim = Adamax || accuracy: 0.8639652729034424\n",
      "model 315/1125 done, hidden = tanh, output = tanh, optim = Adamax || accuracy: 0.8871201276779175\n",
      "model 316/1125 done, hidden = tanh, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 317/1125 done, hidden = tanh, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 318/1125 done, hidden = tanh, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 319/1125 done, hidden = tanh, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 320/1125 done, hidden = tanh, output = relu, optim = SGD || accuracy: 0.8494935035705566\n",
      "model 321/1125 done, hidden = tanh, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 322/1125 done, hidden = tanh, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 323/1125 done, hidden = tanh, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 324/1125 done, hidden = tanh, output = relu, optim = SGD || accuracy: 0.8596237301826477\n",
      "model 325/1125 done, hidden = tanh, output = relu, optim = RMSprop || accuracy: 0.8885672688484192\n",
      "model 326/1125 done, hidden = tanh, output = relu, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 327/1125 done, hidden = tanh, output = relu, optim = RMSprop || accuracy: 0.9117221236228943\n",
      "model 328/1125 done, hidden = tanh, output = relu, optim = RMSprop || accuracy: 0.8900144696235657\n",
      "model 329/1125 done, hidden = tanh, output = relu, optim = RMSprop || accuracy: 0.9073805809020996\n",
      "model 330/1125 done, hidden = tanh, output = relu, optim = RMSprop || accuracy: 0.8929088115692139\n",
      "model 331/1125 done, hidden = tanh, output = relu, optim = RMSprop || accuracy: 0.8914616703987122\n",
      "model 332/1125 done, hidden = tanh, output = relu, optim = RMSprop || accuracy: 0.8958032131195068\n",
      "model 333/1125 done, hidden = tanh, output = relu, optim = RMSprop || accuracy: 0.8914616703987122\n",
      "model 334/1125 done, hidden = tanh, output = relu, optim = Adam || accuracy: 0.8929088115692139\n",
      "model 335/1125 done, hidden = tanh, output = relu, optim = Adam || accuracy: 0.898697555065155\n",
      "model 336/1125 done, hidden = tanh, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 337/1125 done, hidden = tanh, output = relu, optim = Adam || accuracy: 0.8885672688484192\n",
      "model 338/1125 done, hidden = tanh, output = relu, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 339/1125 done, hidden = tanh, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 340/1125 done, hidden = tanh, output = relu, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 341/1125 done, hidden = tanh, output = relu, optim = Adam || accuracy: 0.9015918970108032\n",
      "model 342/1125 done, hidden = tanh, output = relu, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 343/1125 done, hidden = tanh, output = relu, optim = Adagrad || accuracy: 0.6801736354827881\n",
      "model 344/1125 done, hidden = tanh, output = relu, optim = Adagrad || accuracy: 0.7641099691390991\n",
      "model 345/1125 done, hidden = tanh, output = relu, optim = Adagrad || accuracy: 0.6758321523666382\n",
      "model 346/1125 done, hidden = tanh, output = relu, optim = Adagrad || accuracy: 0.7568740844726562\n",
      "model 347/1125 done, hidden = tanh, output = relu, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 348/1125 done, hidden = tanh, output = relu, optim = Adagrad || accuracy: 0.7525325417518616\n",
      "model 349/1125 done, hidden = tanh, output = relu, optim = Adagrad || accuracy: 0.7785817384719849\n",
      "model 350/1125 done, hidden = tanh, output = relu, optim = Adagrad || accuracy: 0.7554269433021545\n",
      "model 351/1125 done, hidden = tanh, output = relu, optim = Adagrad || accuracy: 0.7756873965263367\n",
      "model 352/1125 done, hidden = tanh, output = relu, optim = Adamax || accuracy: 0.8885672688484192\n",
      "model 353/1125 done, hidden = tanh, output = relu, optim = Adamax || accuracy: 0.898697555065155\n",
      "model 354/1125 done, hidden = tanh, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 355/1125 done, hidden = tanh, output = relu, optim = Adamax || accuracy: 0.885672926902771\n",
      "model 356/1125 done, hidden = tanh, output = relu, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 357/1125 done, hidden = tanh, output = relu, optim = Adamax || accuracy: 0.8958032131195068\n",
      "model 358/1125 done, hidden = tanh, output = relu, optim = Adamax || accuracy: 0.8900144696235657\n",
      "model 359/1125 done, hidden = tanh, output = relu, optim = Adamax || accuracy: 0.8900144696235657\n",
      "model 360/1125 done, hidden = tanh, output = relu, optim = Adamax || accuracy: 0.9088277816772461\n",
      "model 361/1125 done, hidden = tanh, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 362/1125 done, hidden = tanh, output = LeakyReLU, optim = SGD || accuracy: 0.8422576189041138\n",
      "model 363/1125 done, hidden = tanh, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 364/1125 done, hidden = tanh, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 365/1125 done, hidden = tanh, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 366/1125 done, hidden = tanh, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 367/1125 done, hidden = tanh, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 368/1125 done, hidden = tanh, output = LeakyReLU, optim = SGD || accuracy: 0.8567293882369995\n",
      "model 369/1125 done, hidden = tanh, output = LeakyReLU, optim = SGD || accuracy: 0.8480463027954102\n",
      "model 370/1125 done, hidden = tanh, output = LeakyReLU, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 371/1125 done, hidden = tanh, output = LeakyReLU, optim = RMSprop || accuracy: 0.8943560123443604\n",
      "model 372/1125 done, hidden = tanh, output = LeakyReLU, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 373/1125 done, hidden = tanh, output = LeakyReLU, optim = RMSprop || accuracy: 0.9088277816772461\n",
      "model 374/1125 done, hidden = tanh, output = LeakyReLU, optim = RMSprop || accuracy: 0.8929088115692139\n",
      "model 375/1125 done, hidden = tanh, output = LeakyReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 376/1125 done, hidden = tanh, output = LeakyReLU, optim = RMSprop || accuracy: 0.8900144696235657\n",
      "model 377/1125 done, hidden = tanh, output = LeakyReLU, optim = RMSprop || accuracy: 0.9001446962356567\n",
      "model 378/1125 done, hidden = tanh, output = LeakyReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 379/1125 done, hidden = tanh, output = LeakyReLU, optim = Adam || accuracy: 0.8929088115692139\n",
      "model 380/1125 done, hidden = tanh, output = LeakyReLU, optim = Adam || accuracy: 0.9102749824523926\n",
      "model 381/1125 done, hidden = tanh, output = LeakyReLU, optim = Adam || accuracy: 0.8929088115692139\n",
      "model 382/1125 done, hidden = tanh, output = LeakyReLU, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 383/1125 done, hidden = tanh, output = LeakyReLU, optim = Adam || accuracy: 0.9044862389564514\n",
      "model 384/1125 done, hidden = tanh, output = LeakyReLU, optim = Adam || accuracy: 0.8885672688484192\n",
      "model 385/1125 done, hidden = tanh, output = LeakyReLU, optim = Adam || accuracy: 0.9044862389564514\n",
      "model 386/1125 done, hidden = tanh, output = LeakyReLU, optim = Adam || accuracy: 0.8958032131195068\n",
      "model 387/1125 done, hidden = tanh, output = LeakyReLU, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 388/1125 done, hidden = tanh, output = LeakyReLU, optim = Adagrad || accuracy: 0.7293776869773865\n",
      "model 389/1125 done, hidden = tanh, output = LeakyReLU, optim = Adagrad || accuracy: 0.774240255355835\n",
      "model 390/1125 done, hidden = tanh, output = LeakyReLU, optim = Adagrad || accuracy: 0.7554269433021545\n",
      "model 391/1125 done, hidden = tanh, output = LeakyReLU, optim = Adagrad || accuracy: 0.7583212852478027\n",
      "model 392/1125 done, hidden = tanh, output = LeakyReLU, optim = Adagrad || accuracy: 0.7916063666343689\n",
      "model 393/1125 done, hidden = tanh, output = LeakyReLU, optim = Adagrad || accuracy: 0.7684515118598938\n",
      "model 394/1125 done, hidden = tanh, output = LeakyReLU, optim = Adagrad || accuracy: 0.8046309947967529\n",
      "model 395/1125 done, hidden = tanh, output = LeakyReLU, optim = Adagrad || accuracy: 0.7626628279685974\n",
      "model 396/1125 done, hidden = tanh, output = LeakyReLU, optim = Adagrad || accuracy: 0.7641099691390991\n",
      "model 397/1125 done, hidden = tanh, output = LeakyReLU, optim = Adamax || accuracy: 0.8914616703987122\n",
      "model 398/1125 done, hidden = tanh, output = LeakyReLU, optim = Adamax || accuracy: 0.885672926902771\n",
      "model 399/1125 done, hidden = tanh, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 400/1125 done, hidden = tanh, output = LeakyReLU, optim = Adamax || accuracy: 0.8842257857322693\n",
      "model 401/1125 done, hidden = tanh, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 402/1125 done, hidden = tanh, output = LeakyReLU, optim = Adamax || accuracy: 0.9030390977859497\n",
      "model 403/1125 done, hidden = tanh, output = LeakyReLU, optim = Adamax || accuracy: 0.8943560123443604\n",
      "model 404/1125 done, hidden = tanh, output = LeakyReLU, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 405/1125 done, hidden = tanh, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 406/1125 done, hidden = tanh, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 407/1125 done, hidden = tanh, output = PReLU, optim = SGD || accuracy: 0.8408104181289673\n",
      "model 408/1125 done, hidden = tanh, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 409/1125 done, hidden = tanh, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 410/1125 done, hidden = tanh, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 411/1125 done, hidden = tanh, output = PReLU, optim = SGD || accuracy: 0.8379160761833191\n",
      "model 412/1125 done, hidden = tanh, output = PReLU, optim = SGD || accuracy: 0.8509406447410583\n",
      "model 413/1125 done, hidden = tanh, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 414/1125 done, hidden = tanh, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 415/1125 done, hidden = tanh, output = PReLU, optim = RMSprop || accuracy: 0.8943560123443604\n",
      "model 416/1125 done, hidden = tanh, output = PReLU, optim = RMSprop || accuracy: 0.9088277816772461\n",
      "model 417/1125 done, hidden = tanh, output = PReLU, optim = RMSprop || accuracy: 0.9073805809020996\n",
      "model 418/1125 done, hidden = tanh, output = PReLU, optim = RMSprop || accuracy: 0.8914616703987122\n",
      "model 419/1125 done, hidden = tanh, output = PReLU, optim = RMSprop || accuracy: 0.8943560123443604\n",
      "model 420/1125 done, hidden = tanh, output = PReLU, optim = RMSprop || accuracy: 0.9030390977859497\n",
      "model 421/1125 done, hidden = tanh, output = PReLU, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 422/1125 done, hidden = tanh, output = PReLU, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 423/1125 done, hidden = tanh, output = PReLU, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 424/1125 done, hidden = tanh, output = PReLU, optim = Adam || accuracy: 0.898697555065155\n",
      "model 425/1125 done, hidden = tanh, output = PReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 426/1125 done, hidden = tanh, output = PReLU, optim = Adam || accuracy: 0.8929088115692139\n",
      "model 427/1125 done, hidden = tanh, output = PReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 428/1125 done, hidden = tanh, output = PReLU, optim = Adam || accuracy: 0.9088277816772461\n",
      "model 429/1125 done, hidden = tanh, output = PReLU, optim = Adam || accuracy: 0.9030390977859497\n",
      "model 430/1125 done, hidden = tanh, output = PReLU, optim = Adam || accuracy: 0.8943560123443604\n",
      "model 431/1125 done, hidden = tanh, output = PReLU, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 432/1125 done, hidden = tanh, output = PReLU, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 433/1125 done, hidden = tanh, output = PReLU, optim = Adagrad || accuracy: 0.7510854005813599\n",
      "model 434/1125 done, hidden = tanh, output = PReLU, optim = Adagrad || accuracy: 0.7597684264183044\n",
      "model 435/1125 done, hidden = tanh, output = PReLU, optim = Adagrad || accuracy: 0.7626628279685974\n",
      "model 436/1125 done, hidden = tanh, output = PReLU, optim = Adagrad || accuracy: 0.7887120246887207\n",
      "model 437/1125 done, hidden = tanh, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 438/1125 done, hidden = tanh, output = PReLU, optim = Adagrad || accuracy: 0.7597684264183044\n",
      "model 439/1125 done, hidden = tanh, output = PReLU, optim = Adagrad || accuracy: 0.7901591658592224\n",
      "model 440/1125 done, hidden = tanh, output = PReLU, optim = Adagrad || accuracy: 0.7684515118598938\n",
      "model 441/1125 done, hidden = tanh, output = PReLU, optim = Adagrad || accuracy: 0.7698987126350403\n",
      "model 442/1125 done, hidden = tanh, output = PReLU, optim = Adamax || accuracy: 0.8885672688484192\n",
      "model 443/1125 done, hidden = tanh, output = PReLU, optim = Adamax || accuracy: 0.8914616703987122\n",
      "model 444/1125 done, hidden = tanh, output = PReLU, optim = Adamax || accuracy: 0.8842257857322693\n",
      "model 445/1125 done, hidden = tanh, output = PReLU, optim = Adamax || accuracy: 0.8827785849571228\n",
      "model 446/1125 done, hidden = tanh, output = PReLU, optim = Adamax || accuracy: 0.9015918970108032\n",
      "model 447/1125 done, hidden = tanh, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 448/1125 done, hidden = tanh, output = PReLU, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 449/1125 done, hidden = tanh, output = PReLU, optim = Adamax || accuracy: 0.9030390977859497\n",
      "model 450/1125 done, hidden = tanh, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 451/1125 done, hidden = relu, output = sigmoid, optim = SGD || accuracy: 0.743849515914917\n",
      "model 452/1125 done, hidden = relu, output = sigmoid, optim = SGD || accuracy: 0.7264833450317383\n",
      "model 453/1125 done, hidden = relu, output = sigmoid, optim = SGD || accuracy: 0.7395079731941223\n",
      "model 454/1125 done, hidden = relu, output = sigmoid, optim = SGD || accuracy: 0.7337192296981812\n",
      "model 455/1125 done, hidden = relu, output = sigmoid, optim = SGD || accuracy: 0.740955114364624\n",
      "model 456/1125 done, hidden = relu, output = sigmoid, optim = SGD || accuracy: 0.7467438578605652\n",
      "model 457/1125 done, hidden = relu, output = sigmoid, optim = SGD || accuracy: 0.7525325417518616\n",
      "model 458/1125 done, hidden = relu, output = sigmoid, optim = SGD || accuracy: 0.7510854005813599\n",
      "model 459/1125 done, hidden = relu, output = sigmoid, optim = SGD || accuracy: 0.743849515914917\n",
      "model 460/1125 done, hidden = relu, output = sigmoid, optim = RMSprop || accuracy: 0.8900144696235657\n",
      "model 461/1125 done, hidden = relu, output = sigmoid, optim = RMSprop || accuracy: 0.9073805809020996\n",
      "model 462/1125 done, hidden = relu, output = sigmoid, optim = RMSprop || accuracy: 0.9044862389564514\n",
      "model 463/1125 done, hidden = relu, output = sigmoid, optim = RMSprop || accuracy: 0.9030390977859497\n",
      "model 464/1125 done, hidden = relu, output = sigmoid, optim = RMSprop || accuracy: 0.9102749824523926\n",
      "model 465/1125 done, hidden = relu, output = sigmoid, optim = RMSprop || accuracy: 0.9059334397315979\n",
      "model 466/1125 done, hidden = relu, output = sigmoid, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 467/1125 done, hidden = relu, output = sigmoid, optim = RMSprop || accuracy: 0.9117221236228943\n",
      "model 468/1125 done, hidden = relu, output = sigmoid, optim = RMSprop || accuracy: 0.9088277816772461\n",
      "model 469/1125 done, hidden = relu, output = sigmoid, optim = Adam || accuracy: 0.898697555065155\n",
      "model 470/1125 done, hidden = relu, output = sigmoid, optim = Adam || accuracy: 0.9204052090644836\n",
      "model 471/1125 done, hidden = relu, output = sigmoid, optim = Adam || accuracy: 0.916063666343689\n",
      "model 472/1125 done, hidden = relu, output = sigmoid, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 473/1125 done, hidden = relu, output = sigmoid, optim = Adam || accuracy: 0.9175108671188354\n",
      "model 474/1125 done, hidden = relu, output = sigmoid, optim = Adam || accuracy: 0.9102749824523926\n",
      "model 475/1125 done, hidden = relu, output = sigmoid, optim = Adam || accuracy: 0.898697555065155\n",
      "model 476/1125 done, hidden = relu, output = sigmoid, optim = Adam || accuracy: 0.9117221236228943\n",
      "model 477/1125 done, hidden = relu, output = sigmoid, optim = Adam || accuracy: 0.9102749824523926\n",
      "model 478/1125 done, hidden = relu, output = sigmoid, optim = Adagrad || accuracy: 0.6266281008720398\n",
      "model 479/1125 done, hidden = relu, output = sigmoid, optim = Adagrad || accuracy: 0.670043408870697\n",
      "model 480/1125 done, hidden = relu, output = sigmoid, optim = Adagrad || accuracy: 0.6671490669250488\n",
      "model 481/1125 done, hidden = relu, output = sigmoid, optim = Adagrad || accuracy: 0.7076700329780579\n",
      "model 482/1125 done, hidden = relu, output = sigmoid, optim = Adagrad || accuracy: 0.6888567209243774\n",
      "model 483/1125 done, hidden = relu, output = sigmoid, optim = Adagrad || accuracy: 0.6251809000968933\n",
      "model 484/1125 done, hidden = relu, output = sigmoid, optim = Adagrad || accuracy: 0.7293776869773865\n",
      "model 485/1125 done, hidden = relu, output = sigmoid, optim = Adagrad || accuracy: 0.700434148311615\n",
      "model 486/1125 done, hidden = relu, output = sigmoid, optim = Adagrad || accuracy: 0.6439942121505737\n",
      "model 487/1125 done, hidden = relu, output = sigmoid, optim = Adamax || accuracy: 0.9001446962356567\n",
      "model 488/1125 done, hidden = relu, output = sigmoid, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 489/1125 done, hidden = relu, output = sigmoid, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 490/1125 done, hidden = relu, output = sigmoid, optim = Adamax || accuracy: 0.9001446962356567\n",
      "model 491/1125 done, hidden = relu, output = sigmoid, optim = Adamax || accuracy: 0.898697555065155\n",
      "model 492/1125 done, hidden = relu, output = sigmoid, optim = Adamax || accuracy: 0.9088277816772461\n",
      "model 493/1125 done, hidden = relu, output = sigmoid, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 494/1125 done, hidden = relu, output = sigmoid, optim = Adamax || accuracy: 0.9015918970108032\n",
      "model 495/1125 done, hidden = relu, output = sigmoid, optim = Adamax || accuracy: 0.9088277816772461\n",
      "model 496/1125 done, hidden = relu, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 497/1125 done, hidden = relu, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 498/1125 done, hidden = relu, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 499/1125 done, hidden = relu, output = tanh, optim = SGD || accuracy: 0.8321273326873779\n",
      "model 500/1125 done, hidden = relu, output = tanh, optim = SGD || accuracy: 0.8104196786880493\n",
      "model 501/1125 done, hidden = relu, output = tanh, optim = SGD || accuracy: 0.7916063666343689\n",
      "model 502/1125 done, hidden = relu, output = tanh, optim = SGD || accuracy: 0.8364688754081726\n",
      "model 503/1125 done, hidden = relu, output = tanh, optim = SGD || accuracy: 0.784370481967926\n",
      "model 504/1125 done, hidden = relu, output = tanh, optim = SGD || accuracy: 0.8263386487960815\n",
      "model 505/1125 done, hidden = relu, output = tanh, optim = RMSprop || accuracy: 0.7785817384719849\n",
      "model 506/1125 done, hidden = relu, output = tanh, optim = RMSprop || accuracy: 0.7988422513008118\n",
      "model 507/1125 done, hidden = relu, output = tanh, optim = RMSprop || accuracy: 0.8277857899665833\n",
      "model 508/1125 done, hidden = relu, output = tanh, optim = RMSprop || accuracy: 0.8089724779129028\n",
      "model 509/1125 done, hidden = relu, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 510/1125 done, hidden = relu, output = tanh, optim = RMSprop || accuracy: 0.6859623789787292\n",
      "model 511/1125 done, hidden = relu, output = tanh, optim = RMSprop || accuracy: 0.7988422513008118\n",
      "model 512/1125 done, hidden = relu, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 513/1125 done, hidden = relu, output = tanh, optim = RMSprop || accuracy: 0.8277857899665833\n",
      "model 514/1125 done, hidden = relu, output = tanh, optim = Adam || accuracy: 0.845151960849762\n",
      "model 515/1125 done, hidden = relu, output = tanh, optim = Adam || accuracy: 0.8697539567947388\n",
      "model 516/1125 done, hidden = relu, output = tanh, optim = Adam || accuracy: 0.8784370422363281\n",
      "model 517/1125 done, hidden = relu, output = tanh, optim = Adam || accuracy: 0.8871201276779175\n",
      "model 518/1125 done, hidden = relu, output = tanh, optim = Adam || accuracy: 0.885672926902771\n",
      "model 519/1125 done, hidden = relu, output = tanh, optim = Adam || accuracy: 0.8914616703987122\n",
      "model 520/1125 done, hidden = relu, output = tanh, optim = Adam || accuracy: 0.8929088115692139\n",
      "model 521/1125 done, hidden = relu, output = tanh, optim = Adam || accuracy: 0.8943560123443604\n",
      "model 522/1125 done, hidden = relu, output = tanh, optim = Adam || accuracy: 0.8798842430114746\n",
      "model 523/1125 done, hidden = relu, output = tanh, optim = Adagrad || accuracy: 0.6280752420425415\n",
      "model 524/1125 done, hidden = relu, output = tanh, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 525/1125 done, hidden = relu, output = tanh, optim = Adagrad || accuracy: 0.7178003191947937\n",
      "model 526/1125 done, hidden = relu, output = tanh, optim = Adagrad || accuracy: 0.700434148311615\n",
      "model 527/1125 done, hidden = relu, output = tanh, optim = Adagrad || accuracy: 0.7337192296981812\n",
      "model 528/1125 done, hidden = relu, output = tanh, optim = Adagrad || accuracy: 0.7192474603652954\n",
      "model 529/1125 done, hidden = relu, output = tanh, optim = Adagrad || accuracy: 0.740955114364624\n",
      "model 530/1125 done, hidden = relu, output = tanh, optim = Adagrad || accuracy: 0.7120115756988525\n",
      "model 531/1125 done, hidden = relu, output = tanh, optim = Adagrad || accuracy: 0.7366136312484741\n",
      "model 532/1125 done, hidden = relu, output = tanh, optim = Adamax || accuracy: 0.8610709309577942\n",
      "model 533/1125 done, hidden = relu, output = tanh, optim = Adamax || accuracy: 0.8697539567947388\n",
      "model 534/1125 done, hidden = relu, output = tanh, optim = Adamax || accuracy: 0.8784370422363281\n",
      "model 535/1125 done, hidden = relu, output = tanh, optim = Adamax || accuracy: 0.8567293882369995\n",
      "model 536/1125 done, hidden = relu, output = tanh, optim = Adamax || accuracy: 0.8581765294075012\n",
      "model 537/1125 done, hidden = relu, output = tanh, optim = Adamax || accuracy: 0.8827785849571228\n",
      "model 538/1125 done, hidden = relu, output = tanh, optim = Adamax || accuracy: 0.8726483583450317\n",
      "model 539/1125 done, hidden = relu, output = tanh, optim = Adamax || accuracy: 0.8538350462913513\n",
      "model 540/1125 done, hidden = relu, output = tanh, optim = Adamax || accuracy: 0.8740954995155334\n",
      "model 541/1125 done, hidden = relu, output = relu, optim = SGD || accuracy: 0.8596237301826477\n",
      "model 542/1125 done, hidden = relu, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 543/1125 done, hidden = relu, output = relu, optim = SGD || accuracy: 0.8567293882369995\n",
      "model 544/1125 done, hidden = relu, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 545/1125 done, hidden = relu, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 546/1125 done, hidden = relu, output = relu, optim = SGD || accuracy: 0.8581765294075012\n",
      "model 547/1125 done, hidden = relu, output = relu, optim = SGD || accuracy: 0.8465991020202637\n",
      "model 548/1125 done, hidden = relu, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 549/1125 done, hidden = relu, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 550/1125 done, hidden = relu, output = relu, optim = RMSprop || accuracy: 0.8958032131195068\n",
      "model 551/1125 done, hidden = relu, output = relu, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 552/1125 done, hidden = relu, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 553/1125 done, hidden = relu, output = relu, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 554/1125 done, hidden = relu, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 555/1125 done, hidden = relu, output = relu, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 556/1125 done, hidden = relu, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 557/1125 done, hidden = relu, output = relu, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 558/1125 done, hidden = relu, output = relu, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 559/1125 done, hidden = relu, output = relu, optim = Adam || accuracy: 0.898697555065155\n",
      "model 560/1125 done, hidden = relu, output = relu, optim = Adam || accuracy: 0.9073805809020996\n",
      "model 561/1125 done, hidden = relu, output = relu, optim = Adam || accuracy: 0.9059334397315979\n",
      "model 562/1125 done, hidden = relu, output = relu, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 563/1125 done, hidden = relu, output = relu, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 564/1125 done, hidden = relu, output = relu, optim = Adam || accuracy: 0.8813313841819763\n",
      "model 565/1125 done, hidden = relu, output = relu, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 566/1125 done, hidden = relu, output = relu, optim = Adam || accuracy: 0.9015918970108032\n",
      "model 567/1125 done, hidden = relu, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 568/1125 done, hidden = relu, output = relu, optim = Adagrad || accuracy: 0.7626628279685974\n",
      "model 569/1125 done, hidden = relu, output = relu, optim = Adagrad || accuracy: 0.7424023151397705\n",
      "model 570/1125 done, hidden = relu, output = relu, optim = Adagrad || accuracy: 0.7525325417518616\n",
      "model 571/1125 done, hidden = relu, output = relu, optim = Adagrad || accuracy: 0.7235890030860901\n",
      "model 572/1125 done, hidden = relu, output = relu, optim = Adagrad || accuracy: 0.7583212852478027\n",
      "model 573/1125 done, hidden = relu, output = relu, optim = Adagrad || accuracy: 0.7670043706893921\n",
      "model 574/1125 done, hidden = relu, output = relu, optim = Adagrad || accuracy: 0.784370481967926\n",
      "model 575/1125 done, hidden = relu, output = relu, optim = Adagrad || accuracy: 0.7698987126350403\n",
      "model 576/1125 done, hidden = relu, output = relu, optim = Adagrad || accuracy: 0.7337192296981812\n",
      "model 577/1125 done, hidden = relu, output = relu, optim = Adamax || accuracy: 0.8885672688484192\n",
      "model 578/1125 done, hidden = relu, output = relu, optim = Adamax || accuracy: 0.8885672688484192\n",
      "model 579/1125 done, hidden = relu, output = relu, optim = Adamax || accuracy: 0.9030390977859497\n",
      "model 580/1125 done, hidden = relu, output = relu, optim = Adamax || accuracy: 0.8929088115692139\n",
      "model 581/1125 done, hidden = relu, output = relu, optim = Adamax || accuracy: 0.8929088115692139\n",
      "model 582/1125 done, hidden = relu, output = relu, optim = Adamax || accuracy: 0.8943560123443604\n",
      "model 583/1125 done, hidden = relu, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 584/1125 done, hidden = relu, output = relu, optim = Adamax || accuracy: 0.9015918970108032\n",
      "model 585/1125 done, hidden = relu, output = relu, optim = Adamax || accuracy: 0.9102749824523926\n",
      "model 586/1125 done, hidden = relu, output = LeakyReLU, optim = SGD || accuracy: 0.8625180721282959\n",
      "model 587/1125 done, hidden = relu, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 588/1125 done, hidden = relu, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 589/1125 done, hidden = relu, output = LeakyReLU, optim = SGD || accuracy: 0.8509406447410583\n",
      "model 590/1125 done, hidden = relu, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 591/1125 done, hidden = relu, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 592/1125 done, hidden = relu, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 593/1125 done, hidden = relu, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 594/1125 done, hidden = relu, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 595/1125 done, hidden = relu, output = LeakyReLU, optim = RMSprop || accuracy: 0.885672926902771\n",
      "model 596/1125 done, hidden = relu, output = LeakyReLU, optim = RMSprop || accuracy: 0.8755427002906799\n",
      "model 597/1125 done, hidden = relu, output = LeakyReLU, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 598/1125 done, hidden = relu, output = LeakyReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 599/1125 done, hidden = relu, output = LeakyReLU, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 600/1125 done, hidden = relu, output = LeakyReLU, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 601/1125 done, hidden = relu, output = LeakyReLU, optim = RMSprop || accuracy: 0.9001446962356567\n",
      "model 602/1125 done, hidden = relu, output = LeakyReLU, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 603/1125 done, hidden = relu, output = LeakyReLU, optim = RMSprop || accuracy: 0.8914616703987122\n",
      "model 604/1125 done, hidden = relu, output = LeakyReLU, optim = Adam || accuracy: 0.8958032131195068\n",
      "model 605/1125 done, hidden = relu, output = LeakyReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 606/1125 done, hidden = relu, output = LeakyReLU, optim = Adam || accuracy: 0.8929088115692139\n",
      "model 607/1125 done, hidden = relu, output = LeakyReLU, optim = Adam || accuracy: 0.9015918970108032\n",
      "model 608/1125 done, hidden = relu, output = LeakyReLU, optim = Adam || accuracy: 0.9073805809020996\n",
      "model 609/1125 done, hidden = relu, output = LeakyReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 610/1125 done, hidden = relu, output = LeakyReLU, optim = Adam || accuracy: 0.9131693243980408\n",
      "model 611/1125 done, hidden = relu, output = LeakyReLU, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 612/1125 done, hidden = relu, output = LeakyReLU, optim = Adam || accuracy: 0.9088277816772461\n",
      "model 613/1125 done, hidden = relu, output = LeakyReLU, optim = Adagrad || accuracy: 0.629522442817688\n",
      "model 614/1125 done, hidden = relu, output = LeakyReLU, optim = Adagrad || accuracy: 0.7235890030860901\n",
      "model 615/1125 done, hidden = relu, output = LeakyReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 616/1125 done, hidden = relu, output = LeakyReLU, optim = Adagrad || accuracy: 0.7554269433021545\n",
      "model 617/1125 done, hidden = relu, output = LeakyReLU, optim = Adagrad || accuracy: 0.784370481967926\n",
      "model 618/1125 done, hidden = relu, output = LeakyReLU, optim = Adagrad || accuracy: 0.7496381998062134\n",
      "model 619/1125 done, hidden = relu, output = LeakyReLU, optim = Adagrad || accuracy: 0.7076700329780579\n",
      "model 620/1125 done, hidden = relu, output = LeakyReLU, optim = Adagrad || accuracy: 0.7612156271934509\n",
      "model 621/1125 done, hidden = relu, output = LeakyReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 622/1125 done, hidden = relu, output = LeakyReLU, optim = Adamax || accuracy: 0.8871201276779175\n",
      "model 623/1125 done, hidden = relu, output = LeakyReLU, optim = Adamax || accuracy: 0.8943560123443604\n",
      "model 624/1125 done, hidden = relu, output = LeakyReLU, optim = Adamax || accuracy: 0.8929088115692139\n",
      "model 625/1125 done, hidden = relu, output = LeakyReLU, optim = Adamax || accuracy: 0.9073805809020996\n",
      "model 626/1125 done, hidden = relu, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 627/1125 done, hidden = relu, output = LeakyReLU, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 628/1125 done, hidden = relu, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 629/1125 done, hidden = relu, output = LeakyReLU, optim = Adamax || accuracy: 0.9001446962356567\n",
      "model 630/1125 done, hidden = relu, output = LeakyReLU, optim = Adamax || accuracy: 0.9015918970108032\n",
      "model 631/1125 done, hidden = relu, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 632/1125 done, hidden = relu, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 633/1125 done, hidden = relu, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 634/1125 done, hidden = relu, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 635/1125 done, hidden = relu, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 636/1125 done, hidden = relu, output = PReLU, optim = SGD || accuracy: 0.8625180721282959\n",
      "model 637/1125 done, hidden = relu, output = PReLU, optim = SGD || accuracy: 0.8596237301826477\n",
      "model 638/1125 done, hidden = relu, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 639/1125 done, hidden = relu, output = PReLU, optim = SGD || accuracy: 0.7771345973014832\n",
      "model 640/1125 done, hidden = relu, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 641/1125 done, hidden = relu, output = PReLU, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 642/1125 done, hidden = relu, output = PReLU, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 643/1125 done, hidden = relu, output = PReLU, optim = RMSprop || accuracy: 0.8943560123443604\n",
      "model 644/1125 done, hidden = relu, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 645/1125 done, hidden = relu, output = PReLU, optim = RMSprop || accuracy: 0.8929088115692139\n",
      "model 646/1125 done, hidden = relu, output = PReLU, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 647/1125 done, hidden = relu, output = PReLU, optim = RMSprop || accuracy: 0.8958032131195068\n",
      "model 648/1125 done, hidden = relu, output = PReLU, optim = RMSprop || accuracy: 0.9073805809020996\n",
      "model 649/1125 done, hidden = relu, output = PReLU, optim = Adam || accuracy: 0.8784370422363281\n",
      "model 650/1125 done, hidden = relu, output = PReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 651/1125 done, hidden = relu, output = PReLU, optim = Adam || accuracy: 0.9131693243980408\n",
      "model 652/1125 done, hidden = relu, output = PReLU, optim = Adam || accuracy: 0.8900144696235657\n",
      "model 653/1125 done, hidden = relu, output = PReLU, optim = Adam || accuracy: 0.898697555065155\n",
      "model 654/1125 done, hidden = relu, output = PReLU, optim = Adam || accuracy: 0.9044862389564514\n",
      "model 655/1125 done, hidden = relu, output = PReLU, optim = Adam || accuracy: 0.9015918970108032\n",
      "model 656/1125 done, hidden = relu, output = PReLU, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 657/1125 done, hidden = relu, output = PReLU, optim = Adam || accuracy: 0.8885672688484192\n",
      "model 658/1125 done, hidden = relu, output = PReLU, optim = Adagrad || accuracy: 0.7018813490867615\n",
      "model 659/1125 done, hidden = relu, output = PReLU, optim = Adagrad || accuracy: 0.7279305458068848\n",
      "model 660/1125 done, hidden = relu, output = PReLU, optim = Adagrad || accuracy: 0.7481909990310669\n",
      "model 661/1125 done, hidden = relu, output = PReLU, optim = Adagrad || accuracy: 0.7424023151397705\n",
      "model 662/1125 done, hidden = relu, output = PReLU, optim = Adagrad || accuracy: 0.7091172337532043\n",
      "model 663/1125 done, hidden = relu, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 664/1125 done, hidden = relu, output = PReLU, optim = Adagrad || accuracy: 0.7829232811927795\n",
      "model 665/1125 done, hidden = relu, output = PReLU, optim = Adagrad || accuracy: 0.7785817384719849\n",
      "model 666/1125 done, hidden = relu, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 667/1125 done, hidden = relu, output = PReLU, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 668/1125 done, hidden = relu, output = PReLU, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 669/1125 done, hidden = relu, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 670/1125 done, hidden = relu, output = PReLU, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 671/1125 done, hidden = relu, output = PReLU, optim = Adamax || accuracy: 0.8740954995155334\n",
      "model 672/1125 done, hidden = relu, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 673/1125 done, hidden = relu, output = PReLU, optim = Adamax || accuracy: 0.885672926902771\n",
      "model 674/1125 done, hidden = relu, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 675/1125 done, hidden = relu, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 676/1125 done, hidden = LeakyReLU, output = sigmoid, optim = SGD || accuracy: 0.7525325417518616\n",
      "model 677/1125 done, hidden = LeakyReLU, output = sigmoid, optim = SGD || accuracy: 0.730824887752533\n",
      "model 678/1125 done, hidden = LeakyReLU, output = sigmoid, optim = SGD || accuracy: 0.7684515118598938\n",
      "model 679/1125 done, hidden = LeakyReLU, output = sigmoid, optim = SGD || accuracy: 0.7366136312484741\n",
      "model 680/1125 done, hidden = LeakyReLU, output = sigmoid, optim = SGD || accuracy: 0.743849515914917\n",
      "model 681/1125 done, hidden = LeakyReLU, output = sigmoid, optim = SGD || accuracy: 0.730824887752533\n",
      "model 682/1125 done, hidden = LeakyReLU, output = sigmoid, optim = SGD || accuracy: 0.7583212852478027\n",
      "model 683/1125 done, hidden = LeakyReLU, output = sigmoid, optim = SGD || accuracy: 0.7554269433021545\n",
      "model 684/1125 done, hidden = LeakyReLU, output = sigmoid, optim = SGD || accuracy: 0.7366136312484741\n",
      "model 685/1125 done, hidden = LeakyReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9001446962356567\n",
      "model 686/1125 done, hidden = LeakyReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9102749824523926\n",
      "model 687/1125 done, hidden = LeakyReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9044862389564514\n",
      "model 688/1125 done, hidden = LeakyReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9102749824523926\n",
      "model 689/1125 done, hidden = LeakyReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9117221236228943\n",
      "model 690/1125 done, hidden = LeakyReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9059334397315979\n",
      "model 691/1125 done, hidden = LeakyReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9001446962356567\n",
      "model 692/1125 done, hidden = LeakyReLU, output = sigmoid, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 693/1125 done, hidden = LeakyReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9102749824523926\n",
      "model 694/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 695/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adam || accuracy: 0.9117221236228943\n",
      "model 696/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 697/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adam || accuracy: 0.8900144696235657\n",
      "model 698/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adam || accuracy: 0.9073805809020996\n",
      "model 699/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adam || accuracy: 0.9088277816772461\n",
      "model 700/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adam || accuracy: 0.8943560123443604\n",
      "model 701/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adam || accuracy: 0.9015918970108032\n",
      "model 702/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adam || accuracy: 0.9117221236228943\n",
      "model 703/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adagrad || accuracy: 0.7351664304733276\n",
      "model 704/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adagrad || accuracy: 0.6584659814834595\n",
      "model 705/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adagrad || accuracy: 0.7018813490867615\n",
      "model 706/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adagrad || accuracy: 0.7076700329780579\n",
      "model 707/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adagrad || accuracy: 0.6541244387626648\n",
      "model 708/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adagrad || accuracy: 0.6685962080955505\n",
      "model 709/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adagrad || accuracy: 0.740955114364624\n",
      "model 710/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adagrad || accuracy: 0.7351664304733276\n",
      "model 711/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adagrad || accuracy: 0.7076700329780579\n",
      "model 712/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adamax || accuracy: 0.8900144696235657\n",
      "model 713/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adamax || accuracy: 0.9088277816772461\n",
      "model 714/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 715/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adamax || accuracy: 0.8943560123443604\n",
      "model 716/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adamax || accuracy: 0.9073805809020996\n",
      "model 717/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 718/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 719/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adamax || accuracy: 0.9015918970108032\n",
      "model 720/1125 done, hidden = LeakyReLU, output = sigmoid, optim = Adamax || accuracy: 0.9015918970108032\n",
      "model 721/1125 done, hidden = LeakyReLU, output = tanh, optim = SGD || accuracy: 0.8089724779129028\n",
      "model 722/1125 done, hidden = LeakyReLU, output = tanh, optim = SGD || accuracy: 0.8162084221839905\n",
      "model 723/1125 done, hidden = LeakyReLU, output = tanh, optim = SGD || accuracy: 0.8263386487960815\n",
      "model 724/1125 done, hidden = LeakyReLU, output = tanh, optim = SGD || accuracy: 0.8465991020202637\n",
      "model 725/1125 done, hidden = LeakyReLU, output = tanh, optim = SGD || accuracy: 0.8437047600746155\n",
      "model 726/1125 done, hidden = LeakyReLU, output = tanh, optim = SGD || accuracy: 0.8162084221839905\n",
      "model 727/1125 done, hidden = LeakyReLU, output = tanh, optim = SGD || accuracy: 0.7756873965263367\n",
      "model 728/1125 done, hidden = LeakyReLU, output = tanh, optim = SGD || accuracy: 0.8219971060752869\n",
      "model 729/1125 done, hidden = LeakyReLU, output = tanh, optim = SGD || accuracy: 0.8263386487960815\n",
      "model 730/1125 done, hidden = LeakyReLU, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 731/1125 done, hidden = LeakyReLU, output = tanh, optim = RMSprop || accuracy: 0.7510854005813599\n",
      "model 732/1125 done, hidden = LeakyReLU, output = tanh, optim = RMSprop || accuracy: 0.8133140206336975\n",
      "model 733/1125 done, hidden = LeakyReLU, output = tanh, optim = RMSprop || accuracy: 0.7785817384719849\n",
      "model 734/1125 done, hidden = LeakyReLU, output = tanh, optim = RMSprop || accuracy: 0.7670043706893921\n",
      "model 735/1125 done, hidden = LeakyReLU, output = tanh, optim = RMSprop || accuracy: 0.7612156271934509\n",
      "model 736/1125 done, hidden = LeakyReLU, output = tanh, optim = RMSprop || accuracy: 0.7814761400222778\n",
      "model 737/1125 done, hidden = LeakyReLU, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 738/1125 done, hidden = LeakyReLU, output = tanh, optim = RMSprop || accuracy: 0.784370481967926\n",
      "model 739/1125 done, hidden = LeakyReLU, output = tanh, optim = Adam || accuracy: 0.8625180721282959\n",
      "model 740/1125 done, hidden = LeakyReLU, output = tanh, optim = Adam || accuracy: 0.8842257857322693\n",
      "model 741/1125 done, hidden = LeakyReLU, output = tanh, optim = Adam || accuracy: 0.8871201276779175\n",
      "model 742/1125 done, hidden = LeakyReLU, output = tanh, optim = Adam || accuracy: 0.8740954995155334\n",
      "model 743/1125 done, hidden = LeakyReLU, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 744/1125 done, hidden = LeakyReLU, output = tanh, optim = Adam || accuracy: 0.8697539567947388\n",
      "model 745/1125 done, hidden = LeakyReLU, output = tanh, optim = Adam || accuracy: 0.8871201276779175\n",
      "model 746/1125 done, hidden = LeakyReLU, output = tanh, optim = Adam || accuracy: 0.8871201276779175\n",
      "model 747/1125 done, hidden = LeakyReLU, output = tanh, optim = Adam || accuracy: 0.9030390977859497\n",
      "model 748/1125 done, hidden = LeakyReLU, output = tanh, optim = Adagrad || accuracy: 0.7279305458068848\n",
      "model 749/1125 done, hidden = LeakyReLU, output = tanh, optim = Adagrad || accuracy: 0.7091172337532043\n",
      "model 750/1125 done, hidden = LeakyReLU, output = tanh, optim = Adagrad || accuracy: 0.7337192296981812\n",
      "model 751/1125 done, hidden = LeakyReLU, output = tanh, optim = Adagrad || accuracy: 0.7221418023109436\n",
      "model 752/1125 done, hidden = LeakyReLU, output = tanh, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 753/1125 done, hidden = LeakyReLU, output = tanh, optim = Adagrad || accuracy: 0.730824887752533\n",
      "model 754/1125 done, hidden = LeakyReLU, output = tanh, optim = Adagrad || accuracy: 0.7670043706893921\n",
      "model 755/1125 done, hidden = LeakyReLU, output = tanh, optim = Adagrad || accuracy: 0.7293776869773865\n",
      "model 756/1125 done, hidden = LeakyReLU, output = tanh, optim = Adagrad || accuracy: 0.7322720885276794\n",
      "model 757/1125 done, hidden = LeakyReLU, output = tanh, optim = Adamax || accuracy: 0.8755427002906799\n",
      "model 758/1125 done, hidden = LeakyReLU, output = tanh, optim = Adamax || accuracy: 0.8480463027954102\n",
      "model 759/1125 done, hidden = LeakyReLU, output = tanh, optim = Adamax || accuracy: 0.8654124736785889\n",
      "model 760/1125 done, hidden = LeakyReLU, output = tanh, optim = Adamax || accuracy: 0.8712011575698853\n",
      "model 761/1125 done, hidden = LeakyReLU, output = tanh, optim = Adamax || accuracy: 0.8654124736785889\n",
      "model 762/1125 done, hidden = LeakyReLU, output = tanh, optim = Adamax || accuracy: 0.8697539567947388\n",
      "model 763/1125 done, hidden = LeakyReLU, output = tanh, optim = Adamax || accuracy: 0.8567293882369995\n",
      "model 764/1125 done, hidden = LeakyReLU, output = tanh, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 765/1125 done, hidden = LeakyReLU, output = tanh, optim = Adamax || accuracy: 0.8683068156242371\n",
      "model 766/1125 done, hidden = LeakyReLU, output = relu, optim = SGD || accuracy: 0.8668596148490906\n",
      "model 767/1125 done, hidden = LeakyReLU, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 768/1125 done, hidden = LeakyReLU, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 769/1125 done, hidden = LeakyReLU, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 770/1125 done, hidden = LeakyReLU, output = relu, optim = SGD || accuracy: 0.855282187461853\n",
      "model 771/1125 done, hidden = LeakyReLU, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 772/1125 done, hidden = LeakyReLU, output = relu, optim = SGD || accuracy: 0.8668596148490906\n",
      "model 773/1125 done, hidden = LeakyReLU, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 774/1125 done, hidden = LeakyReLU, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 775/1125 done, hidden = LeakyReLU, output = relu, optim = RMSprop || accuracy: 0.8914616703987122\n",
      "model 776/1125 done, hidden = LeakyReLU, output = relu, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 777/1125 done, hidden = LeakyReLU, output = relu, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 778/1125 done, hidden = LeakyReLU, output = relu, optim = RMSprop || accuracy: 0.8798842430114746\n",
      "model 779/1125 done, hidden = LeakyReLU, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 780/1125 done, hidden = LeakyReLU, output = relu, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 781/1125 done, hidden = LeakyReLU, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 782/1125 done, hidden = LeakyReLU, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 783/1125 done, hidden = LeakyReLU, output = relu, optim = RMSprop || accuracy: 0.9001446962356567\n",
      "model 784/1125 done, hidden = LeakyReLU, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 785/1125 done, hidden = LeakyReLU, output = relu, optim = Adam || accuracy: 0.8914616703987122\n",
      "model 786/1125 done, hidden = LeakyReLU, output = relu, optim = Adam || accuracy: 0.9073805809020996\n",
      "model 787/1125 done, hidden = LeakyReLU, output = relu, optim = Adam || accuracy: 0.8784370422363281\n",
      "model 788/1125 done, hidden = LeakyReLU, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 789/1125 done, hidden = LeakyReLU, output = relu, optim = Adam || accuracy: 0.898697555065155\n",
      "model 790/1125 done, hidden = LeakyReLU, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 791/1125 done, hidden = LeakyReLU, output = relu, optim = Adam || accuracy: 0.9030390977859497\n",
      "model 792/1125 done, hidden = LeakyReLU, output = relu, optim = Adam || accuracy: 0.9030390977859497\n",
      "model 793/1125 done, hidden = LeakyReLU, output = relu, optim = Adagrad || accuracy: 0.7235890030860901\n",
      "model 794/1125 done, hidden = LeakyReLU, output = relu, optim = Adagrad || accuracy: 0.7539797425270081\n",
      "model 795/1125 done, hidden = LeakyReLU, output = relu, optim = Adagrad || accuracy: 0.7120115756988525\n",
      "model 796/1125 done, hidden = LeakyReLU, output = relu, optim = Adagrad || accuracy: 0.7612156271934509\n",
      "model 797/1125 done, hidden = LeakyReLU, output = relu, optim = Adagrad || accuracy: 0.7496381998062134\n",
      "model 798/1125 done, hidden = LeakyReLU, output = relu, optim = Adagrad || accuracy: 0.7626628279685974\n",
      "model 799/1125 done, hidden = LeakyReLU, output = relu, optim = Adagrad || accuracy: 0.7510854005813599\n",
      "model 800/1125 done, hidden = LeakyReLU, output = relu, optim = Adagrad || accuracy: 0.7539797425270081\n",
      "model 801/1125 done, hidden = LeakyReLU, output = relu, optim = Adagrad || accuracy: 0.7554269433021545\n",
      "model 802/1125 done, hidden = LeakyReLU, output = relu, optim = Adamax || accuracy: 0.8871201276779175\n",
      "model 803/1125 done, hidden = LeakyReLU, output = relu, optim = Adamax || accuracy: 0.8842257857322693\n",
      "model 804/1125 done, hidden = LeakyReLU, output = relu, optim = Adamax || accuracy: 0.9015918970108032\n",
      "model 805/1125 done, hidden = LeakyReLU, output = relu, optim = Adamax || accuracy: 0.7945007085800171\n",
      "model 806/1125 done, hidden = LeakyReLU, output = relu, optim = Adamax || accuracy: 0.9073805809020996\n",
      "model 807/1125 done, hidden = LeakyReLU, output = relu, optim = Adamax || accuracy: 0.9073805809020996\n",
      "model 808/1125 done, hidden = LeakyReLU, output = relu, optim = Adamax || accuracy: 0.9015918970108032\n",
      "model 809/1125 done, hidden = LeakyReLU, output = relu, optim = Adamax || accuracy: 0.8943560123443604\n",
      "model 810/1125 done, hidden = LeakyReLU, output = relu, optim = Adamax || accuracy: 0.8914616703987122\n",
      "model 811/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = SGD || accuracy: 0.8364688754081726\n",
      "model 812/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 813/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 814/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 815/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 816/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 817/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = SGD || accuracy: 0.8668596148490906\n",
      "model 818/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 819/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 820/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.8943560123443604\n",
      "model 821/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.9044862389564514\n",
      "model 822/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.9088277816772461\n",
      "model 823/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 824/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.9131693243980408\n",
      "model 825/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.8871201276779175\n",
      "model 826/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.8958032131195068\n",
      "model 827/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.9030390977859497\n",
      "model 828/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.8900144696235657\n",
      "model 829/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 830/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 831/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adam || accuracy: 0.9102749824523926\n",
      "model 832/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adam || accuracy: 0.8914616703987122\n",
      "model 833/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adam || accuracy: 0.9088277816772461\n",
      "model 834/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adam || accuracy: 0.898697555065155\n",
      "model 835/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adam || accuracy: 0.898697555065155\n",
      "model 836/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 837/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 838/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7163531184196472\n",
      "model 839/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7452966570854187\n",
      "model 840/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7554269433021545\n",
      "model 841/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.774240255355835\n",
      "model 842/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7380607724189758\n",
      "model 843/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7351664304733276\n",
      "model 844/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7872648239135742\n",
      "model 845/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7727930545806885\n",
      "model 846/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7452966570854187\n",
      "model 847/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 848/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.8914616703987122\n",
      "model 849/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 850/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.8958032131195068\n",
      "model 851/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 852/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 853/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 854/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.8943560123443604\n",
      "model 855/1125 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 856/1125 done, hidden = LeakyReLU, output = PReLU, optim = SGD || accuracy: 0.8581765294075012\n",
      "model 857/1125 done, hidden = LeakyReLU, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 858/1125 done, hidden = LeakyReLU, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 859/1125 done, hidden = LeakyReLU, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 860/1125 done, hidden = LeakyReLU, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 861/1125 done, hidden = LeakyReLU, output = PReLU, optim = SGD || accuracy: 0.8437047600746155\n",
      "model 862/1125 done, hidden = LeakyReLU, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 863/1125 done, hidden = LeakyReLU, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 864/1125 done, hidden = LeakyReLU, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 865/1125 done, hidden = LeakyReLU, output = PReLU, optim = RMSprop || accuracy: 0.8914616703987122\n",
      "model 866/1125 done, hidden = LeakyReLU, output = PReLU, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 867/1125 done, hidden = LeakyReLU, output = PReLU, optim = RMSprop || accuracy: 0.8958032131195068\n",
      "model 868/1125 done, hidden = LeakyReLU, output = PReLU, optim = RMSprop || accuracy: 0.8958032131195068\n",
      "model 869/1125 done, hidden = LeakyReLU, output = PReLU, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 870/1125 done, hidden = LeakyReLU, output = PReLU, optim = RMSprop || accuracy: 0.9073805809020996\n",
      "model 871/1125 done, hidden = LeakyReLU, output = PReLU, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 872/1125 done, hidden = LeakyReLU, output = PReLU, optim = RMSprop || accuracy: 0.9044862389564514\n",
      "model 873/1125 done, hidden = LeakyReLU, output = PReLU, optim = RMSprop || accuracy: 0.8958032131195068\n",
      "model 874/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adam || accuracy: 0.8943560123443604\n",
      "model 875/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adam || accuracy: 0.9073805809020996\n",
      "model 876/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adam || accuracy: 0.8958032131195068\n",
      "model 877/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adam || accuracy: 0.8885672688484192\n",
      "model 878/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 879/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 880/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adam || accuracy: 0.8914616703987122\n",
      "model 881/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 882/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adam || accuracy: 0.898697555065155\n",
      "model 883/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adagrad || accuracy: 0.616497814655304\n",
      "model 884/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adagrad || accuracy: 0.7279305458068848\n",
      "model 885/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adagrad || accuracy: 0.7163531184196472\n",
      "model 886/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adagrad || accuracy: 0.7539797425270081\n",
      "model 887/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adagrad || accuracy: 0.7496381998062134\n",
      "model 888/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adagrad || accuracy: 0.743849515914917\n",
      "model 889/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 890/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adagrad || accuracy: 0.7771345973014832\n",
      "model 891/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adagrad || accuracy: 0.7597684264183044\n",
      "model 892/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adamax || accuracy: 0.8943560123443604\n",
      "model 893/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adamax || accuracy: 0.8900144696235657\n",
      "model 894/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adamax || accuracy: 0.898697555065155\n",
      "model 895/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adamax || accuracy: 0.8827785849571228\n",
      "model 896/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adamax || accuracy: 0.9015918970108032\n",
      "model 897/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adamax || accuracy: 0.8929088115692139\n",
      "model 898/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 899/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adamax || accuracy: 0.9030390977859497\n",
      "model 900/1125 done, hidden = LeakyReLU, output = PReLU, optim = Adamax || accuracy: 0.898697555065155\n",
      "model 901/1125 done, hidden = PReLU, output = sigmoid, optim = SGD || accuracy: 0.7554269433021545\n",
      "model 902/1125 done, hidden = PReLU, output = sigmoid, optim = SGD || accuracy: 0.7337192296981812\n",
      "model 903/1125 done, hidden = PReLU, output = sigmoid, optim = SGD || accuracy: 0.7351664304733276\n",
      "model 904/1125 done, hidden = PReLU, output = sigmoid, optim = SGD || accuracy: 0.7424023151397705\n",
      "model 905/1125 done, hidden = PReLU, output = sigmoid, optim = SGD || accuracy: 0.7554269433021545\n",
      "model 906/1125 done, hidden = PReLU, output = sigmoid, optim = SGD || accuracy: 0.7510854005813599\n",
      "model 907/1125 done, hidden = PReLU, output = sigmoid, optim = SGD || accuracy: 0.7525325417518616\n",
      "model 908/1125 done, hidden = PReLU, output = sigmoid, optim = SGD || accuracy: 0.7525325417518616\n",
      "model 909/1125 done, hidden = PReLU, output = sigmoid, optim = SGD || accuracy: 0.7351664304733276\n",
      "model 910/1125 done, hidden = PReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9088277816772461\n",
      "model 911/1125 done, hidden = PReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9102749824523926\n",
      "model 912/1125 done, hidden = PReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9088277816772461\n",
      "model 913/1125 done, hidden = PReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 914/1125 done, hidden = PReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9146165251731873\n",
      "model 915/1125 done, hidden = PReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9102749824523926\n",
      "model 916/1125 done, hidden = PReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 917/1125 done, hidden = PReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9117221236228943\n",
      "model 918/1125 done, hidden = PReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 919/1125 done, hidden = PReLU, output = sigmoid, optim = Adam || accuracy: 0.898697555065155\n",
      "model 920/1125 done, hidden = PReLU, output = sigmoid, optim = Adam || accuracy: 0.916063666343689\n",
      "model 921/1125 done, hidden = PReLU, output = sigmoid, optim = Adam || accuracy: 0.9073805809020996\n",
      "model 922/1125 done, hidden = PReLU, output = sigmoid, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 923/1125 done, hidden = PReLU, output = sigmoid, optim = Adam || accuracy: 0.9102749824523926\n",
      "model 924/1125 done, hidden = PReLU, output = sigmoid, optim = Adam || accuracy: 0.9102749824523926\n",
      "model 925/1125 done, hidden = PReLU, output = sigmoid, optim = Adam || accuracy: 0.8943560123443604\n",
      "model 926/1125 done, hidden = PReLU, output = sigmoid, optim = Adam || accuracy: 0.9088277816772461\n",
      "model 927/1125 done, hidden = PReLU, output = sigmoid, optim = Adam || accuracy: 0.9146165251731873\n",
      "model 928/1125 done, hidden = PReLU, output = sigmoid, optim = Adagrad || accuracy: 0.639652669429779\n",
      "model 929/1125 done, hidden = PReLU, output = sigmoid, optim = Adagrad || accuracy: 0.683068037033081\n",
      "model 930/1125 done, hidden = PReLU, output = sigmoid, optim = Adagrad || accuracy: 0.5166425704956055\n",
      "model 931/1125 done, hidden = PReLU, output = sigmoid, optim = Adagrad || accuracy: 0.6917510628700256\n",
      "model 932/1125 done, hidden = PReLU, output = sigmoid, optim = Adagrad || accuracy: 0.589001476764679\n",
      "model 933/1125 done, hidden = PReLU, output = sigmoid, optim = Adagrad || accuracy: 0.7120115756988525\n",
      "model 934/1125 done, hidden = PReLU, output = sigmoid, optim = Adagrad || accuracy: 0.6570188403129578\n",
      "model 935/1125 done, hidden = PReLU, output = sigmoid, optim = Adagrad || accuracy: 0.7091172337532043\n",
      "model 936/1125 done, hidden = PReLU, output = sigmoid, optim = Adagrad || accuracy: 0.6917510628700256\n",
      "model 937/1125 done, hidden = PReLU, output = sigmoid, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 938/1125 done, hidden = PReLU, output = sigmoid, optim = Adamax || accuracy: 0.898697555065155\n",
      "model 939/1125 done, hidden = PReLU, output = sigmoid, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 940/1125 done, hidden = PReLU, output = sigmoid, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 941/1125 done, hidden = PReLU, output = sigmoid, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 942/1125 done, hidden = PReLU, output = sigmoid, optim = Adamax || accuracy: 0.8929088115692139\n",
      "model 943/1125 done, hidden = PReLU, output = sigmoid, optim = Adamax || accuracy: 0.898697555065155\n",
      "model 944/1125 done, hidden = PReLU, output = sigmoid, optim = Adamax || accuracy: 0.9131693243980408\n",
      "model 945/1125 done, hidden = PReLU, output = sigmoid, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 946/1125 done, hidden = PReLU, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 947/1125 done, hidden = PReLU, output = tanh, optim = SGD || accuracy: 0.8393632173538208\n",
      "model 948/1125 done, hidden = PReLU, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 949/1125 done, hidden = PReLU, output = tanh, optim = SGD || accuracy: 0.8379160761833191\n",
      "model 950/1125 done, hidden = PReLU, output = tanh, optim = SGD || accuracy: 0.8306801915168762\n",
      "model 951/1125 done, hidden = PReLU, output = tanh, optim = SGD || accuracy: 0.8089724779129028\n",
      "model 952/1125 done, hidden = PReLU, output = tanh, optim = SGD || accuracy: 0.8408104181289673\n",
      "model 953/1125 done, hidden = PReLU, output = tanh, optim = SGD || accuracy: 0.8437047600746155\n",
      "model 954/1125 done, hidden = PReLU, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 955/1125 done, hidden = PReLU, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 956/1125 done, hidden = PReLU, output = tanh, optim = RMSprop || accuracy: 0.8031837940216064\n",
      "model 957/1125 done, hidden = PReLU, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 958/1125 done, hidden = PReLU, output = tanh, optim = RMSprop || accuracy: 0.7539797425270081\n",
      "model 959/1125 done, hidden = PReLU, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 960/1125 done, hidden = PReLU, output = tanh, optim = RMSprop || accuracy: 0.8162084221839905\n",
      "model 961/1125 done, hidden = PReLU, output = tanh, optim = RMSprop || accuracy: 0.771345853805542\n",
      "model 962/1125 done, hidden = PReLU, output = tanh, optim = RMSprop || accuracy: 0.8162084221839905\n",
      "model 963/1125 done, hidden = PReLU, output = tanh, optim = RMSprop || accuracy: 0.8306801915168762\n",
      "model 964/1125 done, hidden = PReLU, output = tanh, optim = Adam || accuracy: 0.8639652729034424\n",
      "model 965/1125 done, hidden = PReLU, output = tanh, optim = Adam || accuracy: 0.8523878455162048\n",
      "model 966/1125 done, hidden = PReLU, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 967/1125 done, hidden = PReLU, output = tanh, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 968/1125 done, hidden = PReLU, output = tanh, optim = Adam || accuracy: 0.8958032131195068\n",
      "model 969/1125 done, hidden = PReLU, output = tanh, optim = Adam || accuracy: 0.8813313841819763\n",
      "model 970/1125 done, hidden = PReLU, output = tanh, optim = Adam || accuracy: 0.8871201276779175\n",
      "model 971/1125 done, hidden = PReLU, output = tanh, optim = Adam || accuracy: 0.8798842430114746\n",
      "model 972/1125 done, hidden = PReLU, output = tanh, optim = Adam || accuracy: 0.885672926902771\n",
      "model 973/1125 done, hidden = PReLU, output = tanh, optim = Adagrad || accuracy: 0.7235890030860901\n",
      "model 974/1125 done, hidden = PReLU, output = tanh, optim = Adagrad || accuracy: 0.7149059176445007\n",
      "model 975/1125 done, hidden = PReLU, output = tanh, optim = Adagrad || accuracy: 0.7279305458068848\n",
      "model 976/1125 done, hidden = PReLU, output = tanh, optim = Adagrad || accuracy: 0.7351664304733276\n",
      "model 977/1125 done, hidden = PReLU, output = tanh, optim = Adagrad || accuracy: 0.7424023151397705\n",
      "model 978/1125 done, hidden = PReLU, output = tanh, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 979/1125 done, hidden = PReLU, output = tanh, optim = Adagrad || accuracy: 0.7163531184196472\n",
      "model 980/1125 done, hidden = PReLU, output = tanh, optim = Adagrad || accuracy: 0.7395079731941223\n",
      "model 981/1125 done, hidden = PReLU, output = tanh, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 982/1125 done, hidden = PReLU, output = tanh, optim = Adamax || accuracy: 0.8639652729034424\n",
      "model 983/1125 done, hidden = PReLU, output = tanh, optim = Adamax || accuracy: 0.8668596148490906\n",
      "model 984/1125 done, hidden = PReLU, output = tanh, optim = Adamax || accuracy: 0.855282187461853\n",
      "model 985/1125 done, hidden = PReLU, output = tanh, optim = Adamax || accuracy: 0.8523878455162048\n",
      "model 986/1125 done, hidden = PReLU, output = tanh, optim = Adamax || accuracy: 0.8697539567947388\n",
      "model 987/1125 done, hidden = PReLU, output = tanh, optim = Adamax || accuracy: 0.8784370422363281\n",
      "model 988/1125 done, hidden = PReLU, output = tanh, optim = Adamax || accuracy: 0.885672926902771\n",
      "model 989/1125 done, hidden = PReLU, output = tanh, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 990/1125 done, hidden = PReLU, output = tanh, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 991/1125 done, hidden = PReLU, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 992/1125 done, hidden = PReLU, output = relu, optim = SGD || accuracy: 0.8596237301826477\n",
      "model 993/1125 done, hidden = PReLU, output = relu, optim = SGD || accuracy: 0.8306801915168762\n",
      "model 994/1125 done, hidden = PReLU, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 995/1125 done, hidden = PReLU, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 996/1125 done, hidden = PReLU, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 997/1125 done, hidden = PReLU, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 998/1125 done, hidden = PReLU, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 999/1125 done, hidden = PReLU, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 1000/1125 done, hidden = PReLU, output = relu, optim = RMSprop || accuracy: 0.8885672688484192\n",
      "model 1001/1125 done, hidden = PReLU, output = relu, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 1002/1125 done, hidden = PReLU, output = relu, optim = RMSprop || accuracy: 0.9001446962356567\n",
      "model 1003/1125 done, hidden = PReLU, output = relu, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 1004/1125 done, hidden = PReLU, output = relu, optim = RMSprop || accuracy: 0.9059334397315979\n",
      "model 1005/1125 done, hidden = PReLU, output = relu, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 1006/1125 done, hidden = PReLU, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 1007/1125 done, hidden = PReLU, output = relu, optim = RMSprop || accuracy: 0.8871201276779175\n",
      "model 1008/1125 done, hidden = PReLU, output = relu, optim = RMSprop || accuracy: 0.9044862389564514\n",
      "model 1009/1125 done, hidden = PReLU, output = relu, optim = Adam || accuracy: 0.885672926902771\n",
      "model 1010/1125 done, hidden = PReLU, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 1011/1125 done, hidden = PReLU, output = relu, optim = Adam || accuracy: 0.8914616703987122\n",
      "model 1012/1125 done, hidden = PReLU, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 1013/1125 done, hidden = PReLU, output = relu, optim = Adam || accuracy: 0.8900144696235657\n",
      "model 1014/1125 done, hidden = PReLU, output = relu, optim = Adam || accuracy: 0.9030390977859497\n",
      "model 1015/1125 done, hidden = PReLU, output = relu, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 1016/1125 done, hidden = PReLU, output = relu, optim = Adam || accuracy: 0.8958032131195068\n",
      "model 1017/1125 done, hidden = PReLU, output = relu, optim = Adam || accuracy: 0.8914616703987122\n",
      "model 1018/1125 done, hidden = PReLU, output = relu, optim = Adagrad || accuracy: 0.7395079731941223\n",
      "model 1019/1125 done, hidden = PReLU, output = relu, optim = Adagrad || accuracy: 0.7539797425270081\n",
      "model 1020/1125 done, hidden = PReLU, output = relu, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 1021/1125 done, hidden = PReLU, output = relu, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 1022/1125 done, hidden = PReLU, output = relu, optim = Adagrad || accuracy: 0.7539797425270081\n",
      "model 1023/1125 done, hidden = PReLU, output = relu, optim = Adagrad || accuracy: 0.7612156271934509\n",
      "model 1024/1125 done, hidden = PReLU, output = relu, optim = Adagrad || accuracy: 0.7583212852478027\n",
      "model 1025/1125 done, hidden = PReLU, output = relu, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 1026/1125 done, hidden = PReLU, output = relu, optim = Adagrad || accuracy: 0.7698987126350403\n",
      "model 1027/1125 done, hidden = PReLU, output = relu, optim = Adamax || accuracy: 0.8798842430114746\n",
      "model 1028/1125 done, hidden = PReLU, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 1029/1125 done, hidden = PReLU, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 1030/1125 done, hidden = PReLU, output = relu, optim = Adamax || accuracy: 0.8958032131195068\n",
      "model 1031/1125 done, hidden = PReLU, output = relu, optim = Adamax || accuracy: 0.8885672688484192\n",
      "model 1032/1125 done, hidden = PReLU, output = relu, optim = Adamax || accuracy: 0.8900144696235657\n",
      "model 1033/1125 done, hidden = PReLU, output = relu, optim = Adamax || accuracy: 0.9030390977859497\n",
      "model 1034/1125 done, hidden = PReLU, output = relu, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 1035/1125 done, hidden = PReLU, output = relu, optim = Adamax || accuracy: 0.9088277816772461\n",
      "model 1036/1125 done, hidden = PReLU, output = LeakyReLU, optim = SGD || accuracy: 0.8292329907417297\n",
      "model 1037/1125 done, hidden = PReLU, output = LeakyReLU, optim = SGD || accuracy: 0.8408104181289673\n",
      "model 1038/1125 done, hidden = PReLU, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 1039/1125 done, hidden = PReLU, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 1040/1125 done, hidden = PReLU, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 1041/1125 done, hidden = PReLU, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 1042/1125 done, hidden = PReLU, output = LeakyReLU, optim = SGD || accuracy: 0.8567293882369995\n",
      "model 1043/1125 done, hidden = PReLU, output = LeakyReLU, optim = SGD || accuracy: 0.8191027641296387\n",
      "model 1044/1125 done, hidden = PReLU, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 1045/1125 done, hidden = PReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.9001446962356567\n",
      "model 1046/1125 done, hidden = PReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.8914616703987122\n",
      "model 1047/1125 done, hidden = PReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.8943560123443604\n",
      "model 1048/1125 done, hidden = PReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.8958032131195068\n",
      "model 1049/1125 done, hidden = PReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 1050/1125 done, hidden = PReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 1051/1125 done, hidden = PReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.8914616703987122\n",
      "model 1052/1125 done, hidden = PReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.9088277816772461\n",
      "model 1053/1125 done, hidden = PReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.9030390977859497\n",
      "model 1054/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adam || accuracy: 0.885672926902771\n",
      "model 1055/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 1056/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adam || accuracy: 0.9030390977859497\n",
      "model 1057/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adam || accuracy: 0.9015918970108032\n",
      "model 1058/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adam || accuracy: 0.8914616703987122\n",
      "model 1059/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adam || accuracy: 0.9015918970108032\n",
      "model 1060/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adam || accuracy: 0.8943560123443604\n",
      "model 1061/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 1062/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 1063/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7670043706893921\n",
      "model 1064/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7033284902572632\n",
      "model 1065/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7366136312484741\n",
      "model 1066/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.774240255355835\n",
      "model 1067/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.774240255355835\n",
      "model 1068/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7076700329780579\n",
      "model 1069/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7756873965263367\n",
      "model 1070/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7380607724189758\n",
      "model 1071/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7655571699142456\n",
      "model 1072/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 1073/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 1074/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.8813313841819763\n",
      "model 1075/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.8885672688484192\n",
      "model 1076/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.898697555065155\n",
      "model 1077/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.898697555065155\n",
      "model 1078/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.8929088115692139\n",
      "model 1079/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.9030390977859497\n",
      "model 1080/1125 done, hidden = PReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.8900144696235657\n",
      "model 1081/1125 done, hidden = PReLU, output = PReLU, optim = SGD || accuracy: 0.8523878455162048\n",
      "model 1082/1125 done, hidden = PReLU, output = PReLU, optim = SGD || accuracy: 0.8509406447410583\n",
      "model 1083/1125 done, hidden = PReLU, output = PReLU, optim = SGD || accuracy: 0.8437047600746155\n",
      "model 1084/1125 done, hidden = PReLU, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 1085/1125 done, hidden = PReLU, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 1086/1125 done, hidden = PReLU, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 1087/1125 done, hidden = PReLU, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 1088/1125 done, hidden = PReLU, output = PReLU, optim = SGD || accuracy: 0.8191027641296387\n",
      "model 1089/1125 done, hidden = PReLU, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 1090/1125 done, hidden = PReLU, output = PReLU, optim = RMSprop || accuracy: 0.8871201276779175\n",
      "model 1091/1125 done, hidden = PReLU, output = PReLU, optim = RMSprop || accuracy: 0.9059334397315979\n",
      "model 1092/1125 done, hidden = PReLU, output = PReLU, optim = RMSprop || accuracy: 0.9030390977859497\n",
      "model 1093/1125 done, hidden = PReLU, output = PReLU, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 1094/1125 done, hidden = PReLU, output = PReLU, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 1095/1125 done, hidden = PReLU, output = PReLU, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 1096/1125 done, hidden = PReLU, output = PReLU, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 1097/1125 done, hidden = PReLU, output = PReLU, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 1098/1125 done, hidden = PReLU, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 1099/1125 done, hidden = PReLU, output = PReLU, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 1100/1125 done, hidden = PReLU, output = PReLU, optim = Adam || accuracy: 0.8871201276779175\n",
      "model 1101/1125 done, hidden = PReLU, output = PReLU, optim = Adam || accuracy: 0.898697555065155\n",
      "model 1102/1125 done, hidden = PReLU, output = PReLU, optim = Adam || accuracy: 0.898697555065155\n",
      "model 1103/1125 done, hidden = PReLU, output = PReLU, optim = Adam || accuracy: 0.9015918970108032\n",
      "model 1104/1125 done, hidden = PReLU, output = PReLU, optim = Adam || accuracy: 0.8943560123443604\n",
      "model 1105/1125 done, hidden = PReLU, output = PReLU, optim = Adam || accuracy: 0.8827785849571228\n",
      "model 1106/1125 done, hidden = PReLU, output = PReLU, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 1107/1125 done, hidden = PReLU, output = PReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 1108/1125 done, hidden = PReLU, output = PReLU, optim = Adagrad || accuracy: 0.683068037033081\n",
      "model 1109/1125 done, hidden = PReLU, output = PReLU, optim = Adagrad || accuracy: 0.7467438578605652\n",
      "model 1110/1125 done, hidden = PReLU, output = PReLU, optim = Adagrad || accuracy: 0.7583212852478027\n",
      "model 1111/1125 done, hidden = PReLU, output = PReLU, optim = Adagrad || accuracy: 0.7597684264183044\n",
      "model 1112/1125 done, hidden = PReLU, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 1113/1125 done, hidden = PReLU, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 1114/1125 done, hidden = PReLU, output = PReLU, optim = Adagrad || accuracy: 0.7626628279685974\n",
      "model 1115/1125 done, hidden = PReLU, output = PReLU, optim = Adagrad || accuracy: 0.7554269433021545\n",
      "model 1116/1125 done, hidden = PReLU, output = PReLU, optim = Adagrad || accuracy: 0.7481909990310669\n",
      "model 1117/1125 done, hidden = PReLU, output = PReLU, optim = Adamax || accuracy: 0.885672926902771\n",
      "model 1118/1125 done, hidden = PReLU, output = PReLU, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 1119/1125 done, hidden = PReLU, output = PReLU, optim = Adamax || accuracy: 0.8958032131195068\n",
      "model 1120/1125 done, hidden = PReLU, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 1121/1125 done, hidden = PReLU, output = PReLU, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 1122/1125 done, hidden = PReLU, output = PReLU, optim = Adamax || accuracy: 0.8885672688484192\n",
      "model 1123/1125 done, hidden = PReLU, output = PReLU, optim = Adamax || accuracy: 0.8958032131195068\n",
      "model 1124/1125 done, hidden = PReLU, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 1125/1125 done, hidden = PReLU, output = PReLU, optim = Adamax || accuracy: 0.9001446962356567\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "hl_f = ['sigmoid','tanh','relu','LeakyReLU','PReLU']\n",
    "ol_f = ['sigmoid','tanh','relu','LeakyReLU','PReLU']\n",
    "act = ['SGD', 'RMSprop','Adam','Adagrad','Adamax']\n",
    "num_filters = [32, 64, 128]\n",
    "kernel_size = [3, 5, 7]\n",
    "table = defaultdict(list)\n",
    "count = 0\n",
    "for i in hl_f:\n",
    "    for j in ol_f:\n",
    "        for k in act:\n",
    "            for l in num_filters:\n",
    "                for m in kernel_size:\n",
    "\n",
    "                    table['hl_f'].append(i)\n",
    "                    table['ol_f'].append(j)\n",
    "                    table['k_f'].append(k)\n",
    "                    table['n_filters'].append(l)\n",
    "                    table['kernel_size'].append(m)\n",
    "\n",
    "                    \n",
    "\n",
    "                    model = create_CNNModel(i, j, k, l, m)\n",
    "\n",
    "                    history = model.fit(X_train, y_train,\n",
    "                        epochs=20,\n",
    "                        verbose=False,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        batch_size=32)\n",
    "                    loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "\n",
    "                    table['accuracy'].append(accuracy)\n",
    "                    table['loss'].append(loss)\n",
    "                    count = count + 1\n",
    "                    print(f'model {count}/1125 done, hidden = {i}, output = {j}, optim = {k} || accuracy: {accuracy}')\n",
    "                    tf.keras.backend.clear_session()\n",
    "                    with open('accuracies.txt', 'a') as f:\n",
    "                        f.write('\\n'+str(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Tagalog_Headlines_Deep_TEXTRES.txt') as f:\n",
    "    lines = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid\n",
      "sigmoid\n",
      "SGD\n",
      "0.502170741558075\n"
     ]
    }
   ],
   "source": [
    "print(re.search('hidden = (.*), output', test).group(1))\n",
    "print(re.search('output = (.*), optim', test).group(1))\n",
    "print(re.search('optim = (.*) accuracy', test).group(1)[:-3])\n",
    "print(re.search('accuracy: (.*)', test).group(1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model 1/1125 done, hidden = sigmoid, output = sigmoid, optim = SGD || accuracy: 0.502170741558075'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'sigmoid' in lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_res_dic = defaultdict(list)\n",
    "for i in lines:\n",
    "    cnn_res_dic['hidden layer activation function'].append(re.search('hidden = (.*), output', i).group(1))\n",
    "    cnn_res_dic['outputlayer activation function'].append(re.search('output = (.*), optim', i).group(1))\n",
    "    cnn_res_dic['optimizer'].append(re.search('optim = (.*) accuracy', i).group(1)[:-3])\n",
    "    cnn_res_dic['accuracy'].append(float(re.search('accuracy: (.*)', i).group(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden layer activation function</th>\n",
       "      <th>outputlayer activation function</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.502171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.502171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.516643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.497829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.502171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>0.905933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>0.888567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>0.895803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>0.497829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>0.900145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1125 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     hidden layer activation function outputlayer activation function  \\\n",
       "0                             sigmoid                         sigmoid   \n",
       "1                             sigmoid                         sigmoid   \n",
       "2                             sigmoid                         sigmoid   \n",
       "3                             sigmoid                         sigmoid   \n",
       "4                             sigmoid                         sigmoid   \n",
       "...                               ...                             ...   \n",
       "1120                            PReLU                           PReLU   \n",
       "1121                            PReLU                           PReLU   \n",
       "1122                            PReLU                           PReLU   \n",
       "1123                            PReLU                           PReLU   \n",
       "1124                            PReLU                           PReLU   \n",
       "\n",
       "     optimizer  accuracy  \n",
       "0          SGD  0.502171  \n",
       "1          SGD  0.502171  \n",
       "2          SGD  0.516643  \n",
       "3          SGD  0.497829  \n",
       "4          SGD  0.502171  \n",
       "...        ...       ...  \n",
       "1120    Adamax  0.905933  \n",
       "1121    Adamax  0.888567  \n",
       "1122    Adamax  0.895803  \n",
       "1123    Adamax  0.497829  \n",
       "1124    Adamax  0.900145  \n",
       "\n",
       "[1125 rows x 4 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_CNN_res = pd.DataFrame(cnn_res_dic)\n",
    "df_CNN_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CNN_res.to_csv('CNN_Experiment_Results_TAGALOG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden layer activation function</th>\n",
       "      <th>outputlayer activation function</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>relu</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.920405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.917511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>relu</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.917511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.916064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>relu</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.916064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.916064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.914617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.914617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.914617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.914617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>0.914617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>0.914617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.914617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>relu</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.913169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>0.913169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden layer activation function outputlayer activation function  \\\n",
       "469                             relu                         sigmoid   \n",
       "239                             tanh                         sigmoid   \n",
       "472                             relu                         sigmoid   \n",
       "20                           sigmoid                         sigmoid   \n",
       "470                             relu                         sigmoid   \n",
       "919                            PReLU                         sigmoid   \n",
       "241                             tanh                         sigmoid   \n",
       "926                            PReLU                         sigmoid   \n",
       "236                             tanh                         sigmoid   \n",
       "250                             tanh                         sigmoid   \n",
       "40                           sigmoid                         sigmoid   \n",
       "176                          sigmoid                       LeakyReLU   \n",
       "913                            PReLU                         sigmoid   \n",
       "609                             relu                       LeakyReLU   \n",
       "943                            PReLU                         sigmoid   \n",
       "\n",
       "    optimizer  accuracy  \n",
       "469      Adam  0.920405  \n",
       "239   RMSprop  0.917511  \n",
       "472      Adam  0.917511  \n",
       "20       Adam  0.916064  \n",
       "470      Adam  0.916064  \n",
       "919      Adam  0.916064  \n",
       "241   RMSprop  0.914617  \n",
       "926      Adam  0.914617  \n",
       "236   RMSprop  0.914617  \n",
       "250      Adam  0.914617  \n",
       "40     Adamax  0.914617  \n",
       "176    Adamax  0.914617  \n",
       "913   RMSprop  0.914617  \n",
       "609      Adam  0.913169  \n",
       "943    Adamax  0.913169  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_CNN_res.sort_values('accuracy', ascending=False).iloc[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNNModel(hl, ol, act, lstm_size):\n",
    "    RNN_model = Sequential()\n",
    "    RNN_model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "    RNN_model.add(layers.Bidirectional(layers.LSTM(64)))\n",
    "    RNN_model.add(layers.Dense(lstm_size, activation=hl))\n",
    "    RNN_model.add(layers.Dense(1, activation=ol))\n",
    "    RNN_model.compile(optimizer=act,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return RNN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n"
     ]
    }
   ],
   "source": [
    "hl_f = ['sigmoid','tanh','relu','LeakyReLU','PReLU']\n",
    "ol_f = ['sigmoid','tanh','relu','LeakyReLU','PReLU']\n",
    "act = ['SGD', 'RMSprop','Adam','Adagrad','Adamax']\n",
    "lstm_size = [32, 64, 128]\n",
    "count = 0\n",
    "for i in hl_f:\n",
    "    for j in ol_f:\n",
    "        for k in act:\n",
    "            for l in lstm_size:\n",
    "                count = count+1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1/375 done, hidden = sigmoid, output = sigmoid, optim = SGD || accuracy: 0.5195369124412537\n",
      "model 2/375 done, hidden = sigmoid, output = sigmoid, optim = SGD || accuracy: 0.502170741558075\n",
      "model 3/375 done, hidden = sigmoid, output = sigmoid, optim = SGD || accuracy: 0.502170741558075\n",
      "model 4/375 done, hidden = sigmoid, output = sigmoid, optim = RMSprop || accuracy: 0.9073805809020996\n",
      "model 5/375 done, hidden = sigmoid, output = sigmoid, optim = RMSprop || accuracy: 0.916063666343689\n",
      "model 6/375 done, hidden = sigmoid, output = sigmoid, optim = RMSprop || accuracy: 0.9073805809020996\n",
      "model 7/375 done, hidden = sigmoid, output = sigmoid, optim = Adam || accuracy: 0.9059334397315979\n",
      "model 8/375 done, hidden = sigmoid, output = sigmoid, optim = Adam || accuracy: 0.9088277816772461\n",
      "model 9/375 done, hidden = sigmoid, output = sigmoid, optim = Adam || accuracy: 0.898697555065155\n",
      "model 10/375 done, hidden = sigmoid, output = sigmoid, optim = Adagrad || accuracy: 0.5094066858291626\n",
      "model 11/375 done, hidden = sigmoid, output = sigmoid, optim = Adagrad || accuracy: 0.502170741558075\n",
      "model 12/375 done, hidden = sigmoid, output = sigmoid, optim = Adagrad || accuracy: 0.49927639961242676\n",
      "model 13/375 done, hidden = sigmoid, output = sigmoid, optim = Adamax || accuracy: 0.8958032131195068\n",
      "model 14/375 done, hidden = sigmoid, output = sigmoid, optim = Adamax || accuracy: 0.9030390977859497\n",
      "model 15/375 done, hidden = sigmoid, output = sigmoid, optim = Adamax || accuracy: 0.9015918970108032\n",
      "model 16/375 done, hidden = sigmoid, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 17/375 done, hidden = sigmoid, output = tanh, optim = SGD || accuracy: 0.5108538269996643\n",
      "model 18/375 done, hidden = sigmoid, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 19/375 done, hidden = sigmoid, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 20/375 done, hidden = sigmoid, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 21/375 done, hidden = sigmoid, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 22/375 done, hidden = sigmoid, output = tanh, optim = Adam || accuracy: 0.8900144696235657\n",
      "model 23/375 done, hidden = sigmoid, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 24/375 done, hidden = sigmoid, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 25/375 done, hidden = sigmoid, output = tanh, optim = Adagrad || accuracy: 0.6816208362579346\n",
      "model 26/375 done, hidden = sigmoid, output = tanh, optim = Adagrad || accuracy: 0.6975398063659668\n",
      "model 27/375 done, hidden = sigmoid, output = tanh, optim = Adagrad || accuracy: 0.6714906096458435\n",
      "model 28/375 done, hidden = sigmoid, output = tanh, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 29/375 done, hidden = sigmoid, output = tanh, optim = Adamax || accuracy: 0.9088277816772461\n",
      "model 30/375 done, hidden = sigmoid, output = tanh, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 31/375 done, hidden = sigmoid, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 32/375 done, hidden = sigmoid, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 33/375 done, hidden = sigmoid, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 34/375 done, hidden = sigmoid, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 35/375 done, hidden = sigmoid, output = relu, optim = RMSprop || accuracy: 0.8943560123443604\n",
      "model 36/375 done, hidden = sigmoid, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 37/375 done, hidden = sigmoid, output = relu, optim = Adam || accuracy: 0.9131693243980408\n",
      "model 38/375 done, hidden = sigmoid, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 39/375 done, hidden = sigmoid, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 40/375 done, hidden = sigmoid, output = relu, optim = Adagrad || accuracy: 0.6685962080955505\n",
      "model 41/375 done, hidden = sigmoid, output = relu, optim = Adagrad || accuracy: 0.670043408870697\n",
      "model 42/375 done, hidden = sigmoid, output = relu, optim = Adagrad || accuracy: 0.6338639855384827\n",
      "model 43/375 done, hidden = sigmoid, output = relu, optim = Adamax || accuracy: 0.9073805809020996\n",
      "model 44/375 done, hidden = sigmoid, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 45/375 done, hidden = sigmoid, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 46/375 done, hidden = sigmoid, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 47/375 done, hidden = sigmoid, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 48/375 done, hidden = sigmoid, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 49/375 done, hidden = sigmoid, output = LeakyReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 50/375 done, hidden = sigmoid, output = LeakyReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 51/375 done, hidden = sigmoid, output = LeakyReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 52/375 done, hidden = sigmoid, output = LeakyReLU, optim = Adam || accuracy: 0.9059334397315979\n",
      "model 53/375 done, hidden = sigmoid, output = LeakyReLU, optim = Adam || accuracy: 0.9131693243980408\n",
      "model 54/375 done, hidden = sigmoid, output = LeakyReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 55/375 done, hidden = sigmoid, output = LeakyReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 56/375 done, hidden = sigmoid, output = LeakyReLU, optim = Adagrad || accuracy: 0.659913182258606\n",
      "model 57/375 done, hidden = sigmoid, output = LeakyReLU, optim = Adagrad || accuracy: 0.730824887752533\n",
      "model 58/375 done, hidden = sigmoid, output = LeakyReLU, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 59/375 done, hidden = sigmoid, output = LeakyReLU, optim = Adamax || accuracy: 0.9030390977859497\n",
      "model 60/375 done, hidden = sigmoid, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 61/375 done, hidden = sigmoid, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 62/375 done, hidden = sigmoid, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 63/375 done, hidden = sigmoid, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 64/375 done, hidden = sigmoid, output = PReLU, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 65/375 done, hidden = sigmoid, output = PReLU, optim = RMSprop || accuracy: 0.502170741558075\n",
      "model 66/375 done, hidden = sigmoid, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 67/375 done, hidden = sigmoid, output = PReLU, optim = Adam || accuracy: 0.9102749824523926\n",
      "model 68/375 done, hidden = sigmoid, output = PReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 69/375 done, hidden = sigmoid, output = PReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 70/375 done, hidden = sigmoid, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 71/375 done, hidden = sigmoid, output = PReLU, optim = Adagrad || accuracy: 0.6989869475364685\n",
      "model 72/375 done, hidden = sigmoid, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 73/375 done, hidden = sigmoid, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 74/375 done, hidden = sigmoid, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 75/375 done, hidden = sigmoid, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 76/375 done, hidden = tanh, output = sigmoid, optim = SGD || accuracy: 0.6989869475364685\n",
      "model 77/375 done, hidden = tanh, output = sigmoid, optim = SGD || accuracy: 0.7047756910324097\n",
      "model 78/375 done, hidden = tanh, output = sigmoid, optim = SGD || accuracy: 0.6903039216995239\n",
      "model 79/375 done, hidden = tanh, output = sigmoid, optim = RMSprop || accuracy: 0.9088277816772461\n",
      "model 80/375 done, hidden = tanh, output = sigmoid, optim = RMSprop || accuracy: 0.916063666343689\n",
      "model 81/375 done, hidden = tanh, output = sigmoid, optim = RMSprop || accuracy: 0.9131693243980408\n",
      "model 82/375 done, hidden = tanh, output = sigmoid, optim = Adam || accuracy: 0.9131693243980408\n",
      "model 83/375 done, hidden = tanh, output = sigmoid, optim = Adam || accuracy: 0.9088277816772461\n",
      "model 84/375 done, hidden = tanh, output = sigmoid, optim = Adam || accuracy: 0.9117221236228943\n",
      "model 85/375 done, hidden = tanh, output = sigmoid, optim = Adagrad || accuracy: 0.6917510628700256\n",
      "model 86/375 done, hidden = tanh, output = sigmoid, optim = Adagrad || accuracy: 0.6107091307640076\n",
      "model 87/375 done, hidden = tanh, output = sigmoid, optim = Adagrad || accuracy: 0.5976845026016235\n",
      "model 88/375 done, hidden = tanh, output = sigmoid, optim = Adamax || accuracy: 0.8943560123443604\n",
      "model 89/375 done, hidden = tanh, output = sigmoid, optim = Adamax || accuracy: 0.9117221236228943\n",
      "model 90/375 done, hidden = tanh, output = sigmoid, optim = Adamax || accuracy: 0.9001446962356567\n",
      "model 91/375 done, hidden = tanh, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 92/375 done, hidden = tanh, output = tanh, optim = SGD || accuracy: 0.6353111267089844\n",
      "model 93/375 done, hidden = tanh, output = tanh, optim = SGD || accuracy: 0.6613603234291077\n",
      "model 94/375 done, hidden = tanh, output = tanh, optim = RMSprop || accuracy: 0.8089724779129028\n",
      "model 95/375 done, hidden = tanh, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 96/375 done, hidden = tanh, output = tanh, optim = RMSprop || accuracy: 0.8523878455162048\n",
      "model 97/375 done, hidden = tanh, output = tanh, optim = Adam || accuracy: 0.9102749824523926\n",
      "model 98/375 done, hidden = tanh, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 99/375 done, hidden = tanh, output = tanh, optim = Adam || accuracy: 0.9030390977859497\n",
      "model 100/375 done, hidden = tanh, output = tanh, optim = Adagrad || accuracy: 0.740955114364624\n",
      "model 101/375 done, hidden = tanh, output = tanh, optim = Adagrad || accuracy: 0.7467438578605652\n",
      "model 102/375 done, hidden = tanh, output = tanh, optim = Adagrad || accuracy: 0.7395079731941223\n",
      "model 103/375 done, hidden = tanh, output = tanh, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 104/375 done, hidden = tanh, output = tanh, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 105/375 done, hidden = tanh, output = tanh, optim = Adamax || accuracy: 0.8929088115692139\n",
      "model 106/375 done, hidden = tanh, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 107/375 done, hidden = tanh, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 108/375 done, hidden = tanh, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 109/375 done, hidden = tanh, output = relu, optim = RMSprop || accuracy: 0.9030390977859497\n",
      "model 110/375 done, hidden = tanh, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 111/375 done, hidden = tanh, output = relu, optim = RMSprop || accuracy: 0.8943560123443604\n",
      "model 112/375 done, hidden = tanh, output = relu, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 113/375 done, hidden = tanh, output = relu, optim = Adam || accuracy: 0.9044862389564514\n",
      "model 114/375 done, hidden = tanh, output = relu, optim = Adam || accuracy: 0.9131693243980408\n",
      "model 115/375 done, hidden = tanh, output = relu, optim = Adagrad || accuracy: 0.740955114364624\n",
      "model 116/375 done, hidden = tanh, output = relu, optim = Adagrad || accuracy: 0.7322720885276794\n",
      "model 117/375 done, hidden = tanh, output = relu, optim = Adagrad || accuracy: 0.7293776869773865\n",
      "model 118/375 done, hidden = tanh, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 119/375 done, hidden = tanh, output = relu, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 120/375 done, hidden = tanh, output = relu, optim = Adamax || accuracy: 0.9117221236228943\n",
      "model 121/375 done, hidden = tanh, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 122/375 done, hidden = tanh, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 123/375 done, hidden = tanh, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 124/375 done, hidden = tanh, output = LeakyReLU, optim = RMSprop || accuracy: 0.9131693243980408\n",
      "model 125/375 done, hidden = tanh, output = LeakyReLU, optim = RMSprop || accuracy: 0.9044862389564514\n",
      "model 126/375 done, hidden = tanh, output = LeakyReLU, optim = RMSprop || accuracy: 0.9059334397315979\n",
      "model 127/375 done, hidden = tanh, output = LeakyReLU, optim = Adam || accuracy: 0.9059334397315979\n",
      "model 128/375 done, hidden = tanh, output = LeakyReLU, optim = Adam || accuracy: 0.9030390977859497\n",
      "model 129/375 done, hidden = tanh, output = LeakyReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 130/375 done, hidden = tanh, output = LeakyReLU, optim = Adagrad || accuracy: 0.7467438578605652\n",
      "model 131/375 done, hidden = tanh, output = LeakyReLU, optim = Adagrad || accuracy: 0.7612156271934509\n",
      "model 132/375 done, hidden = tanh, output = LeakyReLU, optim = Adagrad || accuracy: 0.5007236003875732\n",
      "model 133/375 done, hidden = tanh, output = LeakyReLU, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 134/375 done, hidden = tanh, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 135/375 done, hidden = tanh, output = LeakyReLU, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 136/375 done, hidden = tanh, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 137/375 done, hidden = tanh, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 138/375 done, hidden = tanh, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 139/375 done, hidden = tanh, output = PReLU, optim = RMSprop || accuracy: 0.9030390977859497\n",
      "model 140/375 done, hidden = tanh, output = PReLU, optim = RMSprop || accuracy: 0.9044862389564514\n",
      "model 141/375 done, hidden = tanh, output = PReLU, optim = RMSprop || accuracy: 0.9117221236228943\n",
      "model 142/375 done, hidden = tanh, output = PReLU, optim = Adam || accuracy: 0.9117221236228943\n",
      "model 143/375 done, hidden = tanh, output = PReLU, optim = Adam || accuracy: 0.9117221236228943\n",
      "model 144/375 done, hidden = tanh, output = PReLU, optim = Adam || accuracy: 0.898697555065155\n",
      "model 145/375 done, hidden = tanh, output = PReLU, optim = Adagrad || accuracy: 0.7264833450317383\n",
      "model 146/375 done, hidden = tanh, output = PReLU, optim = Adagrad || accuracy: 0.7395079731941223\n",
      "model 147/375 done, hidden = tanh, output = PReLU, optim = Adagrad || accuracy: 0.7424023151397705\n",
      "model 148/375 done, hidden = tanh, output = PReLU, optim = Adamax || accuracy: 0.9001446962356567\n",
      "model 149/375 done, hidden = tanh, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 150/375 done, hidden = tanh, output = PReLU, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 151/375 done, hidden = relu, output = sigmoid, optim = SGD || accuracy: 0.6874095797538757\n",
      "model 152/375 done, hidden = relu, output = sigmoid, optim = SGD || accuracy: 0.6960926055908203\n",
      "model 153/375 done, hidden = relu, output = sigmoid, optim = SGD || accuracy: 0.6960926055908203\n",
      "model 154/375 done, hidden = relu, output = sigmoid, optim = RMSprop || accuracy: 0.9146165251731873\n",
      "model 155/375 done, hidden = relu, output = sigmoid, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 156/375 done, hidden = relu, output = sigmoid, optim = RMSprop || accuracy: 0.9088277816772461\n",
      "model 157/375 done, hidden = relu, output = sigmoid, optim = Adam || accuracy: 0.9175108671188354\n",
      "model 158/375 done, hidden = relu, output = sigmoid, optim = Adam || accuracy: 0.9059334397315979\n",
      "model 159/375 done, hidden = relu, output = sigmoid, optim = Adam || accuracy: 0.9059334397315979\n",
      "model 160/375 done, hidden = relu, output = sigmoid, optim = Adagrad || accuracy: 0.616497814655304\n",
      "model 161/375 done, hidden = relu, output = sigmoid, optim = Adagrad || accuracy: 0.6570188403129578\n",
      "model 162/375 done, hidden = relu, output = sigmoid, optim = Adagrad || accuracy: 0.5933429598808289\n",
      "model 163/375 done, hidden = relu, output = sigmoid, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 164/375 done, hidden = relu, output = sigmoid, optim = Adamax || accuracy: 0.9001446962356567\n",
      "model 165/375 done, hidden = relu, output = sigmoid, optim = Adamax || accuracy: 0.9131693243980408\n",
      "model 166/375 done, hidden = relu, output = tanh, optim = SGD || accuracy: 0.8089724779129028\n",
      "model 167/375 done, hidden = relu, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 168/375 done, hidden = relu, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 169/375 done, hidden = relu, output = tanh, optim = RMSprop || accuracy: 0.8465991020202637\n",
      "model 170/375 done, hidden = relu, output = tanh, optim = RMSprop || accuracy: 0.8581765294075012\n",
      "model 171/375 done, hidden = relu, output = tanh, optim = RMSprop || accuracy: 0.8437047600746155\n",
      "model 172/375 done, hidden = relu, output = tanh, optim = Adam || accuracy: 0.9030390977859497\n",
      "model 173/375 done, hidden = relu, output = tanh, optim = Adam || accuracy: 0.9030390977859497\n",
      "model 174/375 done, hidden = relu, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 175/375 done, hidden = relu, output = tanh, optim = Adagrad || accuracy: 0.639652669429779\n",
      "model 176/375 done, hidden = relu, output = tanh, optim = Adagrad || accuracy: 0.7047756910324097\n",
      "model 177/375 done, hidden = relu, output = tanh, optim = Adagrad || accuracy: 0.7192474603652954\n",
      "model 178/375 done, hidden = relu, output = tanh, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 179/375 done, hidden = relu, output = tanh, optim = Adamax || accuracy: 0.8914616703987122\n",
      "model 180/375 done, hidden = relu, output = tanh, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 181/375 done, hidden = relu, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 182/375 done, hidden = relu, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 183/375 done, hidden = relu, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 184/375 done, hidden = relu, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 185/375 done, hidden = relu, output = relu, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 186/375 done, hidden = relu, output = relu, optim = RMSprop || accuracy: 0.9102749824523926\n",
      "model 187/375 done, hidden = relu, output = relu, optim = Adam || accuracy: 0.9015918970108032\n",
      "model 188/375 done, hidden = relu, output = relu, optim = Adam || accuracy: 0.885672926902771\n",
      "model 189/375 done, hidden = relu, output = relu, optim = Adam || accuracy: 0.9117221236228943\n",
      "model 190/375 done, hidden = relu, output = relu, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 191/375 done, hidden = relu, output = relu, optim = Adagrad || accuracy: 0.7279305458068848\n",
      "model 192/375 done, hidden = relu, output = relu, optim = Adagrad || accuracy: 0.7235890030860901\n",
      "model 193/375 done, hidden = relu, output = relu, optim = Adamax || accuracy: 0.9088277816772461\n",
      "model 194/375 done, hidden = relu, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 195/375 done, hidden = relu, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 196/375 done, hidden = relu, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 197/375 done, hidden = relu, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 198/375 done, hidden = relu, output = LeakyReLU, optim = SGD || accuracy: 0.7814761400222778\n",
      "model 199/375 done, hidden = relu, output = LeakyReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 200/375 done, hidden = relu, output = LeakyReLU, optim = RMSprop || accuracy: 0.9073805809020996\n",
      "model 201/375 done, hidden = relu, output = LeakyReLU, optim = RMSprop || accuracy: 0.9059334397315979\n",
      "model 202/375 done, hidden = relu, output = LeakyReLU, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 203/375 done, hidden = relu, output = LeakyReLU, optim = Adam || accuracy: 0.898697555065155\n",
      "model 204/375 done, hidden = relu, output = LeakyReLU, optim = Adam || accuracy: 0.9117221236228943\n",
      "model 205/375 done, hidden = relu, output = LeakyReLU, optim = Adagrad || accuracy: 0.7279305458068848\n",
      "model 206/375 done, hidden = relu, output = LeakyReLU, optim = Adagrad || accuracy: 0.7206946611404419\n",
      "model 207/375 done, hidden = relu, output = LeakyReLU, optim = Adagrad || accuracy: 0.7539797425270081\n",
      "model 208/375 done, hidden = relu, output = LeakyReLU, optim = Adamax || accuracy: 0.9001446962356567\n",
      "model 209/375 done, hidden = relu, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 210/375 done, hidden = relu, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 211/375 done, hidden = relu, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 212/375 done, hidden = relu, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 213/375 done, hidden = relu, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 214/375 done, hidden = relu, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 215/375 done, hidden = relu, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 216/375 done, hidden = relu, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 217/375 done, hidden = relu, output = PReLU, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 218/375 done, hidden = relu, output = PReLU, optim = Adam || accuracy: 0.8943560123443604\n",
      "model 219/375 done, hidden = relu, output = PReLU, optim = Adam || accuracy: 0.898697555065155\n",
      "model 220/375 done, hidden = relu, output = PReLU, optim = Adagrad || accuracy: 0.7221418023109436\n",
      "model 221/375 done, hidden = relu, output = PReLU, optim = Adagrad || accuracy: 0.7279305458068848\n",
      "model 222/375 done, hidden = relu, output = PReLU, optim = Adagrad || accuracy: 0.7279305458068848\n",
      "model 223/375 done, hidden = relu, output = PReLU, optim = Adamax || accuracy: 0.898697555065155\n",
      "model 224/375 done, hidden = relu, output = PReLU, optim = Adamax || accuracy: 0.9073805809020996\n",
      "model 225/375 done, hidden = relu, output = PReLU, optim = Adamax || accuracy: 0.9073805809020996\n",
      "model 226/375 done, hidden = LeakyReLU, output = sigmoid, optim = SGD || accuracy: 0.6888567209243774\n",
      "model 227/375 done, hidden = LeakyReLU, output = sigmoid, optim = SGD || accuracy: 0.6903039216995239\n",
      "model 228/375 done, hidden = LeakyReLU, output = sigmoid, optim = SGD || accuracy: 0.6975398063659668\n",
      "model 229/375 done, hidden = LeakyReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9131693243980408\n",
      "model 230/375 done, hidden = LeakyReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9059334397315979\n",
      "model 231/375 done, hidden = LeakyReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9015918970108032\n",
      "model 232/375 done, hidden = LeakyReLU, output = sigmoid, optim = Adam || accuracy: 0.898697555065155\n",
      "model 233/375 done, hidden = LeakyReLU, output = sigmoid, optim = Adam || accuracy: 0.9131693243980408\n",
      "model 234/375 done, hidden = LeakyReLU, output = sigmoid, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 235/375 done, hidden = LeakyReLU, output = sigmoid, optim = Adagrad || accuracy: 0.6960926055908203\n",
      "model 236/375 done, hidden = LeakyReLU, output = sigmoid, optim = Adagrad || accuracy: 0.5672937631607056\n",
      "model 237/375 done, hidden = LeakyReLU, output = sigmoid, optim = Adagrad || accuracy: 0.6845151782035828\n",
      "model 238/375 done, hidden = LeakyReLU, output = sigmoid, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 239/375 done, hidden = LeakyReLU, output = sigmoid, optim = Adamax || accuracy: 0.9073805809020996\n",
      "model 240/375 done, hidden = LeakyReLU, output = sigmoid, optim = Adamax || accuracy: 0.9001446962356567\n",
      "model 241/375 done, hidden = LeakyReLU, output = tanh, optim = SGD || accuracy: 0.7583212852478027\n",
      "model 242/375 done, hidden = LeakyReLU, output = tanh, optim = SGD || accuracy: 0.502170741558075\n",
      "model 243/375 done, hidden = LeakyReLU, output = tanh, optim = SGD || accuracy: 0.502170741558075\n",
      "model 244/375 done, hidden = LeakyReLU, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 245/375 done, hidden = LeakyReLU, output = tanh, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 246/375 done, hidden = LeakyReLU, output = tanh, optim = RMSprop || accuracy: 0.8437047600746155\n",
      "model 247/375 done, hidden = LeakyReLU, output = tanh, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 248/375 done, hidden = LeakyReLU, output = tanh, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 249/375 done, hidden = LeakyReLU, output = tanh, optim = Adam || accuracy: 0.8958032131195068\n",
      "model 250/375 done, hidden = LeakyReLU, output = tanh, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 251/375 done, hidden = LeakyReLU, output = tanh, optim = Adagrad || accuracy: 0.7178003191947937\n",
      "model 252/375 done, hidden = LeakyReLU, output = tanh, optim = Adagrad || accuracy: 0.7076700329780579\n",
      "model 253/375 done, hidden = LeakyReLU, output = tanh, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 254/375 done, hidden = LeakyReLU, output = tanh, optim = Adamax || accuracy: 0.8958032131195068\n",
      "model 255/375 done, hidden = LeakyReLU, output = tanh, optim = Adamax || accuracy: 0.8972503542900085\n",
      "model 256/375 done, hidden = LeakyReLU, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 257/375 done, hidden = LeakyReLU, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 258/375 done, hidden = LeakyReLU, output = relu, optim = SGD || accuracy: 0.502170741558075\n",
      "model 259/375 done, hidden = LeakyReLU, output = relu, optim = RMSprop || accuracy: 0.9044862389564514\n",
      "model 260/375 done, hidden = LeakyReLU, output = relu, optim = RMSprop || accuracy: 0.9044862389564514\n",
      "model 261/375 done, hidden = LeakyReLU, output = relu, optim = RMSprop || accuracy: 0.9059334397315979\n",
      "model 262/375 done, hidden = LeakyReLU, output = relu, optim = Adam || accuracy: 0.9117221236228943\n",
      "model 263/375 done, hidden = LeakyReLU, output = relu, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 264/375 done, hidden = LeakyReLU, output = relu, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 265/375 done, hidden = LeakyReLU, output = relu, optim = Adagrad || accuracy: 0.7424023151397705\n",
      "model 266/375 done, hidden = LeakyReLU, output = relu, optim = Adagrad || accuracy: 0.7424023151397705\n",
      "model 267/375 done, hidden = LeakyReLU, output = relu, optim = Adagrad || accuracy: 0.7351664304733276\n",
      "model 268/375 done, hidden = LeakyReLU, output = relu, optim = Adamax || accuracy: 0.9015918970108032\n",
      "model 269/375 done, hidden = LeakyReLU, output = relu, optim = Adamax || accuracy: 0.9102749824523926\n",
      "model 270/375 done, hidden = LeakyReLU, output = relu, optim = Adamax || accuracy: 0.9059334397315979\n",
      "model 271/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 272/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 273/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 274/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.9044862389564514\n",
      "model 275/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.9030390977859497\n",
      "model 276/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.9044862389564514\n",
      "model 277/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adam || accuracy: 0.9059334397315979\n",
      "model 278/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adam || accuracy: 0.9030390977859497\n",
      "model 279/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adam || accuracy: 0.9146165251731873\n",
      "model 280/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7264833450317383\n",
      "model 281/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7149059176445007\n",
      "model 282/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7351664304733276\n",
      "model 283/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 284/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 285/375 done, hidden = LeakyReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 286/375 done, hidden = LeakyReLU, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 287/375 done, hidden = LeakyReLU, output = PReLU, optim = SGD || accuracy: 0.49927639961242676\n",
      "model 288/375 done, hidden = LeakyReLU, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 289/375 done, hidden = LeakyReLU, output = PReLU, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 290/375 done, hidden = LeakyReLU, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 291/375 done, hidden = LeakyReLU, output = PReLU, optim = RMSprop || accuracy: 0.9001446962356567\n",
      "model 292/375 done, hidden = LeakyReLU, output = PReLU, optim = Adam || accuracy: 0.502170741558075\n",
      "model 293/375 done, hidden = LeakyReLU, output = PReLU, optim = Adam || accuracy: 0.9117221236228943\n",
      "model 294/375 done, hidden = LeakyReLU, output = PReLU, optim = Adam || accuracy: 0.9015918970108032\n",
      "model 295/375 done, hidden = LeakyReLU, output = PReLU, optim = Adagrad || accuracy: 0.7235890030860901\n",
      "model 296/375 done, hidden = LeakyReLU, output = PReLU, optim = Adagrad || accuracy: 0.7206946611404419\n",
      "model 297/375 done, hidden = LeakyReLU, output = PReLU, optim = Adagrad || accuracy: 0.7178003191947937\n",
      "model 298/375 done, hidden = LeakyReLU, output = PReLU, optim = Adamax || accuracy: 0.9131693243980408\n",
      "model 299/375 done, hidden = LeakyReLU, output = PReLU, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 300/375 done, hidden = LeakyReLU, output = PReLU, optim = Adamax || accuracy: 0.8943560123443604\n",
      "model 301/375 done, hidden = PReLU, output = sigmoid, optim = SGD || accuracy: 0.6917510628700256\n",
      "model 302/375 done, hidden = PReLU, output = sigmoid, optim = SGD || accuracy: 0.7178003191947937\n",
      "model 303/375 done, hidden = PReLU, output = sigmoid, optim = SGD || accuracy: 0.6874095797538757\n",
      "model 304/375 done, hidden = PReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9073805809020996\n",
      "model 305/375 done, hidden = PReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9175108671188354\n",
      "model 306/375 done, hidden = PReLU, output = sigmoid, optim = RMSprop || accuracy: 0.9117221236228943\n",
      "model 307/375 done, hidden = PReLU, output = sigmoid, optim = Adam || accuracy: 0.9088277816772461\n",
      "model 308/375 done, hidden = PReLU, output = sigmoid, optim = Adam || accuracy: 0.9088277816772461\n",
      "model 309/375 done, hidden = PReLU, output = sigmoid, optim = Adam || accuracy: 0.9088277816772461\n",
      "model 310/375 done, hidden = PReLU, output = sigmoid, optim = Adagrad || accuracy: 0.6888567209243774\n",
      "model 311/375 done, hidden = PReLU, output = sigmoid, optim = Adagrad || accuracy: 0.6382055282592773\n",
      "model 312/375 done, hidden = PReLU, output = sigmoid, optim = Adagrad || accuracy: 0.6497828960418701\n",
      "model 313/375 done, hidden = PReLU, output = sigmoid, optim = Adamax || accuracy: 0.9001446962356567\n",
      "model 314/375 done, hidden = PReLU, output = sigmoid, optim = Adamax || accuracy: 0.9073805809020996\n",
      "model 315/375 done, hidden = PReLU, output = sigmoid, optim = Adamax || accuracy: 0.9030390977859497\n",
      "model 316/375 done, hidden = PReLU, output = tanh, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 317/375 done, hidden = PReLU, output = tanh, optim = SGD || accuracy: 0.7916063666343689\n",
      "model 318/375 done, hidden = PReLU, output = tanh, optim = SGD || accuracy: 0.7727930545806885\n",
      "model 319/375 done, hidden = PReLU, output = tanh, optim = RMSprop || accuracy: 0.80173659324646\n",
      "model 320/375 done, hidden = PReLU, output = tanh, optim = RMSprop || accuracy: 0.8567293882369995\n",
      "model 321/375 done, hidden = PReLU, output = tanh, optim = RMSprop || accuracy: 0.8350217342376709\n",
      "model 322/375 done, hidden = PReLU, output = tanh, optim = Adam || accuracy: 0.8885672688484192\n",
      "model 323/375 done, hidden = PReLU, output = tanh, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 324/375 done, hidden = PReLU, output = tanh, optim = Adam || accuracy: 0.8958032131195068\n",
      "model 325/375 done, hidden = PReLU, output = tanh, optim = Adagrad || accuracy: 0.713458776473999\n",
      "model 326/375 done, hidden = PReLU, output = tanh, optim = Adagrad || accuracy: 0.730824887752533\n",
      "model 327/375 done, hidden = PReLU, output = tanh, optim = Adagrad || accuracy: 0.7192474603652954\n",
      "model 328/375 done, hidden = PReLU, output = tanh, optim = Adamax || accuracy: 0.8885672688484192\n",
      "model 329/375 done, hidden = PReLU, output = tanh, optim = Adamax || accuracy: 0.9001446962356567\n",
      "model 330/375 done, hidden = PReLU, output = tanh, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 331/375 done, hidden = PReLU, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 332/375 done, hidden = PReLU, output = relu, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 333/375 done, hidden = PReLU, output = relu, optim = SGD || accuracy: 0.6975398063659668\n",
      "model 334/375 done, hidden = PReLU, output = relu, optim = RMSprop || accuracy: 0.9059334397315979\n",
      "model 335/375 done, hidden = PReLU, output = relu, optim = RMSprop || accuracy: 0.9030390977859497\n",
      "model 336/375 done, hidden = PReLU, output = relu, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 337/375 done, hidden = PReLU, output = relu, optim = Adam || accuracy: 0.9073805809020996\n",
      "model 338/375 done, hidden = PReLU, output = relu, optim = Adam || accuracy: 0.9044862389564514\n",
      "model 339/375 done, hidden = PReLU, output = relu, optim = Adam || accuracy: 0.9131693243980408\n",
      "model 340/375 done, hidden = PReLU, output = relu, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 341/375 done, hidden = PReLU, output = relu, optim = Adagrad || accuracy: 0.7395079731941223\n",
      "model 342/375 done, hidden = PReLU, output = relu, optim = Adagrad || accuracy: 0.7264833450317383\n",
      "model 343/375 done, hidden = PReLU, output = relu, optim = Adamax || accuracy: 0.49782922863960266\n",
      "model 344/375 done, hidden = PReLU, output = relu, optim = Adamax || accuracy: 0.9102749824523926\n",
      "model 345/375 done, hidden = PReLU, output = relu, optim = Adamax || accuracy: 0.9030390977859497\n",
      "model 346/375 done, hidden = PReLU, output = LeakyReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 347/375 done, hidden = PReLU, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 348/375 done, hidden = PReLU, output = LeakyReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 349/375 done, hidden = PReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.9001446962356567\n",
      "model 350/375 done, hidden = PReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.8972503542900085\n",
      "model 351/375 done, hidden = PReLU, output = LeakyReLU, optim = RMSprop || accuracy: 0.8871201276779175\n",
      "model 352/375 done, hidden = PReLU, output = LeakyReLU, optim = Adam || accuracy: 0.8972503542900085\n",
      "model 353/375 done, hidden = PReLU, output = LeakyReLU, optim = Adam || accuracy: 0.8929088115692139\n",
      "model 354/375 done, hidden = PReLU, output = LeakyReLU, optim = Adam || accuracy: 0.9146165251731873\n",
      "model 355/375 done, hidden = PReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 356/375 done, hidden = PReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.7206946611404419\n",
      "model 357/375 done, hidden = PReLU, output = LeakyReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 358/375 done, hidden = PReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.9073805809020996\n",
      "model 359/375 done, hidden = PReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.9117221236228943\n",
      "model 360/375 done, hidden = PReLU, output = LeakyReLU, optim = Adamax || accuracy: 0.9044862389564514\n",
      "model 361/375 done, hidden = PReLU, output = PReLU, optim = SGD || accuracy: 0.49782922863960266\n",
      "model 362/375 done, hidden = PReLU, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 363/375 done, hidden = PReLU, output = PReLU, optim = SGD || accuracy: 0.502170741558075\n",
      "model 364/375 done, hidden = PReLU, output = PReLU, optim = RMSprop || accuracy: 0.49782922863960266\n",
      "model 365/375 done, hidden = PReLU, output = PReLU, optim = RMSprop || accuracy: 0.898697555065155\n",
      "model 366/375 done, hidden = PReLU, output = PReLU, optim = RMSprop || accuracy: 0.9030390977859497\n",
      "model 367/375 done, hidden = PReLU, output = PReLU, optim = Adam || accuracy: 0.9001446962356567\n",
      "model 368/375 done, hidden = PReLU, output = PReLU, optim = Adam || accuracy: 0.9030390977859497\n",
      "model 369/375 done, hidden = PReLU, output = PReLU, optim = Adam || accuracy: 0.49782922863960266\n",
      "model 370/375 done, hidden = PReLU, output = PReLU, optim = Adagrad || accuracy: 0.7221418023109436\n",
      "model 371/375 done, hidden = PReLU, output = PReLU, optim = Adagrad || accuracy: 0.49782922863960266\n",
      "model 372/375 done, hidden = PReLU, output = PReLU, optim = Adagrad || accuracy: 0.710564374923706\n",
      "model 373/375 done, hidden = PReLU, output = PReLU, optim = Adamax || accuracy: 0.9030390977859497\n",
      "model 374/375 done, hidden = PReLU, output = PReLU, optim = Adamax || accuracy: 0.9073805809020996\n",
      "model 375/375 done, hidden = PReLU, output = PReLU, optim = Adamax || accuracy: 0.9088277816772461\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "hl_f = ['sigmoid','tanh','relu','LeakyReLU','PReLU']\n",
    "ol_f = ['sigmoid','tanh','relu','LeakyReLU','PReLU']\n",
    "act = ['SGD', 'RMSprop','Adam','Adagrad','Adamax']\n",
    "lstm_size = [32, 64, 128]\n",
    "\n",
    "\n",
    "RNN_table = defaultdict(list)\n",
    "count = 0\n",
    "for i in hl_f:\n",
    "    for j in ol_f:\n",
    "        for k in act:\n",
    "            for l in lstm_size:\n",
    "                RNN_table['hl_f'].append(i)\n",
    "                RNN_table['ol_f'].append(j)\n",
    "                RNN_table['k_f'].append(k)\n",
    "                RNN_table['lstm_size'].append(l)          \n",
    "\n",
    "                model = create_RNNModel(i, j, k, l)\n",
    "\n",
    "                history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=32)\n",
    "                loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "\n",
    "                RNN_table['accuracy'].append(accuracy)\n",
    "                RNN_table['loss'].append(loss)\n",
    "                count = count + 1\n",
    "                print(f'model {count}/375 done, hidden = {i}, output = {j}, optim = {k} || accuracy: {accuracy}')\n",
    "                tf.keras.backend.clear_session()\n",
    "                with open('RNN_accuracies.txt', 'a') as f:\n",
    "                    f.write('\\n'+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hl_f</th>\n",
       "      <th>ol_f</th>\n",
       "      <th>k_f</th>\n",
       "      <th>n_filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.502171</td>\n",
       "      <td>0.693044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.502171</td>\n",
       "      <td>0.692790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>0.516643</td>\n",
       "      <td>0.692992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>0.693040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.502171</td>\n",
       "      <td>0.692909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.905933</td>\n",
       "      <td>0.308327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>0.888567</td>\n",
       "      <td>0.420891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.895803</td>\n",
       "      <td>0.278415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>7.745959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>128</td>\n",
       "      <td>7</td>\n",
       "      <td>0.900145</td>\n",
       "      <td>0.372530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1125 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         hl_f     ol_f     k_f  n_filters  kernel_size  accuracy      loss\n",
       "0     sigmoid  sigmoid     SGD         32            3  0.502171  0.693044\n",
       "1     sigmoid  sigmoid     SGD         32            5  0.502171  0.692790\n",
       "2     sigmoid  sigmoid     SGD         32            7  0.516643  0.692992\n",
       "3     sigmoid  sigmoid     SGD         64            3  0.497829  0.693040\n",
       "4     sigmoid  sigmoid     SGD         64            5  0.502171  0.692909\n",
       "...       ...      ...     ...        ...          ...       ...       ...\n",
       "1120    PReLU    PReLU  Adamax         64            5  0.905933  0.308327\n",
       "1121    PReLU    PReLU  Adamax         64            7  0.888567  0.420891\n",
       "1122    PReLU    PReLU  Adamax        128            3  0.895803  0.278415\n",
       "1123    PReLU    PReLU  Adamax        128            5  0.497829  7.745959\n",
       "1124    PReLU    PReLU  Adamax        128            7  0.900145  0.372530\n",
       "\n",
       "[1125 rows x 7 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_CNN_res = pd.DataFrame(table)\n",
    "df_CNN_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hl_f</th>\n",
       "      <th>ol_f</th>\n",
       "      <th>k_f</th>\n",
       "      <th>n_filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>relu</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.920405</td>\n",
       "      <td>0.377860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>0.917511</td>\n",
       "      <td>0.862805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>relu</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.917511</td>\n",
       "      <td>0.360863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>0.916064</td>\n",
       "      <td>0.322013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>relu</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>0.916064</td>\n",
       "      <td>0.362213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>relu</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>7.745959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>7.745959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>relu</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>128</td>\n",
       "      <td>7</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>7.745959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>tanh</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>128</td>\n",
       "      <td>7</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>7.745959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>0.492041</td>\n",
       "      <td>0.693483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1125 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hl_f       ol_f      k_f  n_filters  kernel_size  accuracy      loss\n",
       "469     relu    sigmoid     Adam         32            5  0.920405  0.377860\n",
       "239     tanh    sigmoid  RMSprop         64            7  0.917511  0.862805\n",
       "472     relu    sigmoid     Adam         64            5  0.917511  0.360863\n",
       "20   sigmoid    sigmoid     Adam         32            7  0.916064  0.322013\n",
       "470     relu    sigmoid     Adam         32            7  0.916064  0.362213\n",
       "..       ...        ...      ...        ...          ...       ...       ...\n",
       "614     relu  LeakyReLU  Adagrad         32            7  0.497829  7.745959\n",
       "113  sigmoid       relu     Adam         64            7  0.497829  7.745959\n",
       "620     relu  LeakyReLU  Adagrad        128            7  0.497829  7.745959\n",
       "80   sigmoid       tanh  Adagrad        128            7  0.497829  7.745959\n",
       "34   sigmoid    sigmoid  Adagrad        128            5  0.492041  0.693483\n",
       "\n",
       "[1125 rows x 7 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_CNN_res.sort_values('accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hl_f</th>\n",
       "      <th>ol_f</th>\n",
       "      <th>k_f</th>\n",
       "      <th>lstm_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>32</td>\n",
       "      <td>0.519537</td>\n",
       "      <td>0.691516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>64</td>\n",
       "      <td>0.502171</td>\n",
       "      <td>0.692205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>128</td>\n",
       "      <td>0.502171</td>\n",
       "      <td>0.693376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>32</td>\n",
       "      <td>0.907381</td>\n",
       "      <td>1.237076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>64</td>\n",
       "      <td>0.916064</td>\n",
       "      <td>1.190194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>64</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>7.745959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>128</td>\n",
       "      <td>0.710564</td>\n",
       "      <td>0.602195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>32</td>\n",
       "      <td>0.903039</td>\n",
       "      <td>0.561781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>64</td>\n",
       "      <td>0.907381</td>\n",
       "      <td>0.484241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>128</td>\n",
       "      <td>0.908828</td>\n",
       "      <td>0.601306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hl_f     ol_f      k_f  lstm_size  accuracy      loss\n",
       "0    sigmoid  sigmoid      SGD         32  0.519537  0.691516\n",
       "1    sigmoid  sigmoid      SGD         64  0.502171  0.692205\n",
       "2    sigmoid  sigmoid      SGD        128  0.502171  0.693376\n",
       "3    sigmoid  sigmoid  RMSprop         32  0.907381  1.237076\n",
       "4    sigmoid  sigmoid  RMSprop         64  0.916064  1.190194\n",
       "..       ...      ...      ...        ...       ...       ...\n",
       "370    PReLU    PReLU  Adagrad         64  0.497829  7.745959\n",
       "371    PReLU    PReLU  Adagrad        128  0.710564  0.602195\n",
       "372    PReLU    PReLU   Adamax         32  0.903039  0.561781\n",
       "373    PReLU    PReLU   Adamax         64  0.907381  0.484241\n",
       "374    PReLU    PReLU   Adamax        128  0.908828  0.601306\n",
       "\n",
       "[375 rows x 6 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_RNN_res = pd.DataFrame(RNN_table)\n",
    "df_RNN_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hl_f</th>\n",
       "      <th>ol_f</th>\n",
       "      <th>k_f</th>\n",
       "      <th>lstm_size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>relu</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>32</td>\n",
       "      <td>0.917511</td>\n",
       "      <td>0.551390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>64</td>\n",
       "      <td>0.917511</td>\n",
       "      <td>1.183042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>64</td>\n",
       "      <td>0.916064</td>\n",
       "      <td>1.190194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>64</td>\n",
       "      <td>0.916064</td>\n",
       "      <td>1.042169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>Adam</td>\n",
       "      <td>128</td>\n",
       "      <td>0.914617</td>\n",
       "      <td>0.627071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>128</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>7.745959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adam</td>\n",
       "      <td>64</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>7.745959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adam</td>\n",
       "      <td>128</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>7.745959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>32</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>7.745959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>64</td>\n",
       "      <td>0.497829</td>\n",
       "      <td>7.745959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          hl_f       ol_f      k_f  lstm_size  accuracy      loss\n",
       "156       relu    sigmoid     Adam         32  0.917511  0.551390\n",
       "304      PReLU    sigmoid  RMSprop         64  0.917511  1.183042\n",
       "4      sigmoid    sigmoid  RMSprop         64  0.916064  1.190194\n",
       "79        tanh    sigmoid  RMSprop         64  0.916064  1.042169\n",
       "353      PReLU  LeakyReLU     Adam        128  0.914617  0.627071\n",
       "..         ...        ...      ...        ...       ...       ...\n",
       "65     sigmoid      PReLU  RMSprop        128  0.497829  7.745959\n",
       "67     sigmoid      PReLU     Adam         64  0.497829  7.745959\n",
       "68     sigmoid      PReLU     Adam        128  0.497829  7.745959\n",
       "69     sigmoid      PReLU  Adagrad         32  0.497829  7.745959\n",
       "298  LeakyReLU      PReLU   Adamax         64  0.497829  7.745959\n",
       "\n",
       "[375 rows x 6 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_RNN_res.sort_values('accuracy', ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RNN_res.to_csv('RNN_Experimentation_ENGLISH.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Tagalog_Headlines_RNN_TEXTRES.txt') as f:\n",
    "    lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "\n",
    "\n",
    "rnn_res_dic = defaultdict(list)\n",
    "for i in lines:\n",
    "    rnn_res_dic['hidden layer activation function'].append(re.search('hidden = (.*), output', i).group(1))\n",
    "    rnn_res_dic['outputlayer activation function'].append(re.search('output = (.*), optim', i).group(1))\n",
    "    rnn_res_dic['optimizer'].append(re.search('optim = (.*) accuracy', i).group(1)[:-3])\n",
    "    rnn_res_dic['accuracy'].append(float(re.search('accuracy: (.*)', i).group(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RNN_res = pd.DataFrame(rnn_res_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden layer activation function</th>\n",
       "      <th>outputlayer activation function</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.519537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.502171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.502171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.907381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.916064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.497829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.710564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>0.903039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>0.907381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>0.908828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden layer activation function outputlayer activation function  \\\n",
       "0                            sigmoid                         sigmoid   \n",
       "1                            sigmoid                         sigmoid   \n",
       "2                            sigmoid                         sigmoid   \n",
       "3                            sigmoid                         sigmoid   \n",
       "4                            sigmoid                         sigmoid   \n",
       "..                               ...                             ...   \n",
       "370                            PReLU                           PReLU   \n",
       "371                            PReLU                           PReLU   \n",
       "372                            PReLU                           PReLU   \n",
       "373                            PReLU                           PReLU   \n",
       "374                            PReLU                           PReLU   \n",
       "\n",
       "    optimizer  accuracy  \n",
       "0         SGD  0.519537  \n",
       "1         SGD  0.502171  \n",
       "2         SGD  0.502171  \n",
       "3     RMSprop  0.907381  \n",
       "4     RMSprop  0.916064  \n",
       "..        ...       ...  \n",
       "370   Adagrad  0.497829  \n",
       "371   Adagrad  0.710564  \n",
       "372    Adamax  0.903039  \n",
       "373    Adamax  0.907381  \n",
       "374    Adamax  0.908828  \n",
       "\n",
       "[375 rows x 4 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_RNN_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden layer activation function</th>\n",
       "      <th>outputlayer activation function</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>relu</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.917511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.917511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.916064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.916064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.914617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>relu</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.914617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.914617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.913169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>PReLU</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>0.913169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>PReLU</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.913169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.913169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>tanh</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.913169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.913169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.913169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.913169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden layer activation function outputlayer activation function  \\\n",
       "156                             relu                         sigmoid   \n",
       "304                            PReLU                         sigmoid   \n",
       "4                            sigmoid                         sigmoid   \n",
       "79                              tanh                         sigmoid   \n",
       "353                            PReLU                       LeakyReLU   \n",
       "153                             relu                         sigmoid   \n",
       "278                        LeakyReLU                       LeakyReLU   \n",
       "228                        LeakyReLU                         sigmoid   \n",
       "297                        LeakyReLU                           PReLU   \n",
       "338                            PReLU                            relu   \n",
       "80                              tanh                         sigmoid   \n",
       "81                              tanh                         sigmoid   \n",
       "36                           sigmoid                            relu   \n",
       "232                        LeakyReLU                         sigmoid   \n",
       "52                           sigmoid                       LeakyReLU   \n",
       "\n",
       "    optimizer  accuracy  \n",
       "156      Adam  0.917511  \n",
       "304   RMSprop  0.917511  \n",
       "4     RMSprop  0.916064  \n",
       "79    RMSprop  0.916064  \n",
       "353      Adam  0.914617  \n",
       "153   RMSprop  0.914617  \n",
       "278      Adam  0.914617  \n",
       "228   RMSprop  0.913169  \n",
       "297    Adamax  0.913169  \n",
       "338      Adam  0.913169  \n",
       "80    RMSprop  0.913169  \n",
       "81       Adam  0.913169  \n",
       "36       Adam  0.913169  \n",
       "232      Adam  0.913169  \n",
       "52       Adam  0.913169  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_RNN_res.sort_values('accuracy', ascending= False).iloc[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEST CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 100, 300)          3214200   \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 96, 128)           192128    \n",
      "                                                                 \n",
      " global_max_pooling1d_4 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,407,629\n",
      "Trainable params: 193,429\n",
      "Non-trainable params: 3,214,200\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "87/87 [==============================] - 2s 10ms/step - loss: 0.4729 - accuracy: 0.7367 - val_loss: 0.2952 - val_accuracy: 0.8828\n",
      "Epoch 2/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.1882 - accuracy: 0.9366 - val_loss: 0.2607 - val_accuracy: 0.8973\n",
      "Epoch 3/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.0845 - accuracy: 0.9797 - val_loss: 0.2835 - val_accuracy: 0.8929\n",
      "Epoch 4/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.0311 - accuracy: 0.9971 - val_loss: 0.3011 - val_accuracy: 0.9016\n",
      "Epoch 5/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.0124 - accuracy: 0.9993 - val_loss: 0.3059 - val_accuracy: 0.8973\n",
      "Epoch 6/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.3183 - val_accuracy: 0.9016\n",
      "Epoch 7/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.0045 - accuracy: 0.9996 - val_loss: 0.3255 - val_accuracy: 0.9045\n",
      "Epoch 8/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.0034 - accuracy: 0.9996 - val_loss: 0.3400 - val_accuracy: 0.9016\n",
      "Epoch 9/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.3409 - val_accuracy: 0.9001\n",
      "Epoch 10/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.3479 - val_accuracy: 0.9059\n",
      "Epoch 11/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.3478 - val_accuracy: 0.8958\n",
      "Epoch 12/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.3669 - val_accuracy: 0.8958\n",
      "Epoch 13/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3703 - val_accuracy: 0.9001\n",
      "Epoch 14/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 9.5044e-04 - accuracy: 1.0000 - val_loss: 0.3803 - val_accuracy: 0.8973\n",
      "Epoch 15/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 8.2036e-04 - accuracy: 1.0000 - val_loss: 0.3841 - val_accuracy: 0.9016\n",
      "Epoch 16/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 7.2729e-04 - accuracy: 1.0000 - val_loss: 0.3907 - val_accuracy: 0.9016\n",
      "Epoch 17/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 6.5762e-04 - accuracy: 1.0000 - val_loss: 0.3971 - val_accuracy: 0.9001\n",
      "Epoch 18/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 6.0094e-04 - accuracy: 1.0000 - val_loss: 0.4039 - val_accuracy: 0.9001\n",
      "Epoch 19/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 5.5552e-04 - accuracy: 1.0000 - val_loss: 0.4080 - val_accuracy: 0.9001\n",
      "Epoch 20/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 5.1910e-04 - accuracy: 1.0000 - val_loss: 0.4104 - val_accuracy: 0.9016\n",
      "Epoch 21/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 4.8842e-04 - accuracy: 1.0000 - val_loss: 0.4157 - val_accuracy: 0.9001\n",
      "Epoch 22/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 4.6209e-04 - accuracy: 1.0000 - val_loss: 0.4203 - val_accuracy: 0.9001\n",
      "Epoch 23/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 4.4037e-04 - accuracy: 1.0000 - val_loss: 0.4247 - val_accuracy: 0.9001\n",
      "Epoch 24/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 4.2148e-04 - accuracy: 1.0000 - val_loss: 0.4282 - val_accuracy: 0.9001\n",
      "Epoch 25/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 4.0415e-04 - accuracy: 1.0000 - val_loss: 0.4312 - val_accuracy: 0.9001\n",
      "Epoch 26/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.8956e-04 - accuracy: 1.0000 - val_loss: 0.4345 - val_accuracy: 0.9016\n",
      "Epoch 27/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.7681e-04 - accuracy: 1.0000 - val_loss: 0.4388 - val_accuracy: 0.9001\n",
      "Epoch 28/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.6137e-04 - accuracy: 1.0000 - val_loss: 0.4469 - val_accuracy: 0.9016\n",
      "Epoch 29/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.3075e-04 - accuracy: 1.0000 - val_loss: 0.4646 - val_accuracy: 0.8987\n",
      "Epoch 30/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.9572e-04 - accuracy: 1.0000 - val_loss: 0.4889 - val_accuracy: 0.8973\n",
      "Epoch 31/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.7495e-04 - accuracy: 1.0000 - val_loss: 0.5096 - val_accuracy: 0.8973\n",
      "Epoch 32/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.2473e-04 - accuracy: 1.0000 - val_loss: 0.5202 - val_accuracy: 0.8987\n",
      "Epoch 33/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.0050e-04 - accuracy: 1.0000 - val_loss: 0.5722 - val_accuracy: 0.9001\n",
      "Epoch 34/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.7001e-04 - accuracy: 1.0000 - val_loss: 0.5348 - val_accuracy: 0.9074\n",
      "Epoch 35/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 4.6575e-05 - accuracy: 1.0000 - val_loss: 0.5291 - val_accuracy: 0.9045\n",
      "Epoch 36/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.0556e-05 - accuracy: 1.0000 - val_loss: 0.5315 - val_accuracy: 0.9059\n",
      "Epoch 37/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.5178e-05 - accuracy: 1.0000 - val_loss: 0.5347 - val_accuracy: 0.9059\n",
      "Epoch 38/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.1762e-05 - accuracy: 1.0000 - val_loss: 0.5371 - val_accuracy: 0.9074\n",
      "Epoch 39/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.9181e-05 - accuracy: 1.0000 - val_loss: 0.5409 - val_accuracy: 0.9059\n",
      "Epoch 40/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.7195e-05 - accuracy: 1.0000 - val_loss: 0.5431 - val_accuracy: 0.9074\n",
      "Epoch 41/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.5507e-05 - accuracy: 1.0000 - val_loss: 0.5466 - val_accuracy: 0.9059\n",
      "Epoch 42/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.4134e-05 - accuracy: 1.0000 - val_loss: 0.5492 - val_accuracy: 0.9059\n",
      "Epoch 43/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.2951e-05 - accuracy: 1.0000 - val_loss: 0.5516 - val_accuracy: 0.9045\n",
      "Epoch 44/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.1885e-05 - accuracy: 1.0000 - val_loss: 0.5547 - val_accuracy: 0.9045\n",
      "Epoch 45/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.0985e-05 - accuracy: 1.0000 - val_loss: 0.5569 - val_accuracy: 0.9045\n",
      "Epoch 46/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.0173e-05 - accuracy: 1.0000 - val_loss: 0.5596 - val_accuracy: 0.9045\n",
      "Epoch 47/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 9.4386e-06 - accuracy: 1.0000 - val_loss: 0.5623 - val_accuracy: 0.9045\n",
      "Epoch 48/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 8.7981e-06 - accuracy: 1.0000 - val_loss: 0.5642 - val_accuracy: 0.9045\n",
      "Epoch 49/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 8.1920e-06 - accuracy: 1.0000 - val_loss: 0.5675 - val_accuracy: 0.9045\n",
      "Epoch 50/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 7.6689e-06 - accuracy: 1.0000 - val_loss: 0.5692 - val_accuracy: 0.9045\n",
      "Epoch 51/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 7.1750e-06 - accuracy: 1.0000 - val_loss: 0.5717 - val_accuracy: 0.9045\n",
      "Epoch 52/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 6.7317e-06 - accuracy: 1.0000 - val_loss: 0.5739 - val_accuracy: 0.9045\n",
      "Epoch 53/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 6.3024e-06 - accuracy: 1.0000 - val_loss: 0.5762 - val_accuracy: 0.9045\n",
      "Epoch 54/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 5.9295e-06 - accuracy: 1.0000 - val_loss: 0.5784 - val_accuracy: 0.9045\n",
      "Epoch 55/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 5.5733e-06 - accuracy: 1.0000 - val_loss: 0.5808 - val_accuracy: 0.9045\n",
      "Epoch 56/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 5.2420e-06 - accuracy: 1.0000 - val_loss: 0.5832 - val_accuracy: 0.9045\n",
      "Epoch 57/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 4.9412e-06 - accuracy: 1.0000 - val_loss: 0.5854 - val_accuracy: 0.9045\n",
      "Epoch 58/200\n",
      "87/87 [==============================] - 1s 12ms/step - loss: 4.6517e-06 - accuracy: 1.0000 - val_loss: 0.5877 - val_accuracy: 0.9045\n",
      "Epoch 59/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 4.3875e-06 - accuracy: 1.0000 - val_loss: 0.5896 - val_accuracy: 0.9045\n",
      "Epoch 60/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 4.1425e-06 - accuracy: 1.0000 - val_loss: 0.5920 - val_accuracy: 0.9045\n",
      "Epoch 61/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 3.9145e-06 - accuracy: 1.0000 - val_loss: 0.5941 - val_accuracy: 0.9045\n",
      "Epoch 62/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.7008e-06 - accuracy: 1.0000 - val_loss: 0.5964 - val_accuracy: 0.9045\n",
      "Epoch 63/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.5013e-06 - accuracy: 1.0000 - val_loss: 0.5985 - val_accuracy: 0.9045\n",
      "Epoch 64/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 3.3132e-06 - accuracy: 1.0000 - val_loss: 0.6011 - val_accuracy: 0.9045\n",
      "Epoch 65/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.1347e-06 - accuracy: 1.0000 - val_loss: 0.6031 - val_accuracy: 0.9045\n",
      "Epoch 66/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.9696e-06 - accuracy: 1.0000 - val_loss: 0.6057 - val_accuracy: 0.9045\n",
      "Epoch 67/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.8140e-06 - accuracy: 1.0000 - val_loss: 0.6075 - val_accuracy: 0.9045\n",
      "Epoch 68/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.6654e-06 - accuracy: 1.0000 - val_loss: 0.6096 - val_accuracy: 0.9045\n",
      "Epoch 69/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.5294e-06 - accuracy: 1.0000 - val_loss: 0.6115 - val_accuracy: 0.9045\n",
      "Epoch 70/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.3974e-06 - accuracy: 1.0000 - val_loss: 0.6140 - val_accuracy: 0.9045\n",
      "Epoch 71/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.2764e-06 - accuracy: 1.0000 - val_loss: 0.6157 - val_accuracy: 0.9045\n",
      "Epoch 72/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.1609e-06 - accuracy: 1.0000 - val_loss: 0.6180 - val_accuracy: 0.9045\n",
      "Epoch 73/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.0482e-06 - accuracy: 1.0000 - val_loss: 0.6201 - val_accuracy: 0.9030\n",
      "Epoch 74/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.9455e-06 - accuracy: 1.0000 - val_loss: 0.6227 - val_accuracy: 0.9045\n",
      "Epoch 75/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.8452e-06 - accuracy: 1.0000 - val_loss: 0.6247 - val_accuracy: 0.9030\n",
      "Epoch 76/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.7535e-06 - accuracy: 1.0000 - val_loss: 0.6269 - val_accuracy: 0.9030\n",
      "Epoch 77/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.6665e-06 - accuracy: 1.0000 - val_loss: 0.6289 - val_accuracy: 0.9030\n",
      "Epoch 78/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.5824e-06 - accuracy: 1.0000 - val_loss: 0.6307 - val_accuracy: 0.9030\n",
      "Epoch 79/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.5049e-06 - accuracy: 1.0000 - val_loss: 0.6331 - val_accuracy: 0.9016\n",
      "Epoch 80/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.4303e-06 - accuracy: 1.0000 - val_loss: 0.6354 - val_accuracy: 0.9016\n",
      "Epoch 81/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.3600e-06 - accuracy: 1.0000 - val_loss: 0.6375 - val_accuracy: 0.9016\n",
      "Epoch 82/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.2922e-06 - accuracy: 1.0000 - val_loss: 0.6397 - val_accuracy: 0.9016\n",
      "Epoch 83/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.2290e-06 - accuracy: 1.0000 - val_loss: 0.6420 - val_accuracy: 0.9016\n",
      "Epoch 84/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.1692e-06 - accuracy: 1.0000 - val_loss: 0.6442 - val_accuracy: 0.9016\n",
      "Epoch 85/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.1115e-06 - accuracy: 1.0000 - val_loss: 0.6460 - val_accuracy: 0.9016\n",
      "Epoch 86/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.0571e-06 - accuracy: 1.0000 - val_loss: 0.6480 - val_accuracy: 0.9016\n",
      "Epoch 87/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.0053e-06 - accuracy: 1.0000 - val_loss: 0.6503 - val_accuracy: 0.9016\n",
      "Epoch 88/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 9.5624e-07 - accuracy: 1.0000 - val_loss: 0.6524 - val_accuracy: 0.9016\n",
      "Epoch 89/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 9.0994e-07 - accuracy: 1.0000 - val_loss: 0.6543 - val_accuracy: 0.9016\n",
      "Epoch 90/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 8.6650e-07 - accuracy: 1.0000 - val_loss: 0.6567 - val_accuracy: 0.9016\n",
      "Epoch 91/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 8.2417e-07 - accuracy: 1.0000 - val_loss: 0.6586 - val_accuracy: 0.9016\n",
      "Epoch 92/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 7.8478e-07 - accuracy: 1.0000 - val_loss: 0.6610 - val_accuracy: 0.9016\n",
      "Epoch 93/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 7.4749e-07 - accuracy: 1.0000 - val_loss: 0.6633 - val_accuracy: 0.9016\n",
      "Epoch 94/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 7.1138e-07 - accuracy: 1.0000 - val_loss: 0.6652 - val_accuracy: 0.9016\n",
      "Epoch 95/200\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 6.7698e-07 - accuracy: 1.0000 - val_loss: 0.6674 - val_accuracy: 0.9016\n",
      "Epoch 96/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 6.4465e-07 - accuracy: 1.0000 - val_loss: 0.6696 - val_accuracy: 0.9016\n",
      "Epoch 97/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 6.1280e-07 - accuracy: 1.0000 - val_loss: 0.6714 - val_accuracy: 0.9016\n",
      "Epoch 98/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 5.8393e-07 - accuracy: 1.0000 - val_loss: 0.6736 - val_accuracy: 0.9016\n",
      "Epoch 99/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 5.5578e-07 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.9016\n",
      "Epoch 100/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 5.2925e-07 - accuracy: 1.0000 - val_loss: 0.6777 - val_accuracy: 0.9016\n",
      "Epoch 101/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 5.0530e-07 - accuracy: 1.0000 - val_loss: 0.6799 - val_accuracy: 0.9016\n",
      "Epoch 102/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 4.8054e-07 - accuracy: 1.0000 - val_loss: 0.6819 - val_accuracy: 0.9016\n",
      "Epoch 103/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 4.5712e-07 - accuracy: 1.0000 - val_loss: 0.6842 - val_accuracy: 0.9016\n",
      "Epoch 104/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 4.3520e-07 - accuracy: 1.0000 - val_loss: 0.6866 - val_accuracy: 0.9016\n",
      "Epoch 105/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 4.1490e-07 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.9016\n",
      "Epoch 106/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.9500e-07 - accuracy: 1.0000 - val_loss: 0.6904 - val_accuracy: 0.9016\n",
      "Epoch 107/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.7632e-07 - accuracy: 1.0000 - val_loss: 0.6925 - val_accuracy: 0.9016\n",
      "Epoch 108/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 3.5866e-07 - accuracy: 1.0000 - val_loss: 0.6949 - val_accuracy: 0.9016\n",
      "Epoch 109/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 3.4183e-07 - accuracy: 1.0000 - val_loss: 0.6969 - val_accuracy: 0.9016\n",
      "Epoch 110/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.2585e-07 - accuracy: 1.0000 - val_loss: 0.6987 - val_accuracy: 0.9030\n",
      "Epoch 111/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.1030e-07 - accuracy: 1.0000 - val_loss: 0.7012 - val_accuracy: 0.9016\n",
      "Epoch 112/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.9585e-07 - accuracy: 1.0000 - val_loss: 0.7032 - val_accuracy: 0.9030\n",
      "Epoch 113/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.8196e-07 - accuracy: 1.0000 - val_loss: 0.7056 - val_accuracy: 0.9016\n",
      "Epoch 114/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 2.6878e-07 - accuracy: 1.0000 - val_loss: 0.7073 - val_accuracy: 0.9030\n",
      "Epoch 115/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.5602e-07 - accuracy: 1.0000 - val_loss: 0.7093 - val_accuracy: 0.9030\n",
      "Epoch 116/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.4407e-07 - accuracy: 1.0000 - val_loss: 0.7117 - val_accuracy: 0.9030\n",
      "Epoch 117/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 2.3266e-07 - accuracy: 1.0000 - val_loss: 0.7140 - val_accuracy: 0.9030\n",
      "Epoch 118/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.2200e-07 - accuracy: 1.0000 - val_loss: 0.7156 - val_accuracy: 0.9030\n",
      "Epoch 119/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.1126e-07 - accuracy: 1.0000 - val_loss: 0.7178 - val_accuracy: 0.9030\n",
      "Epoch 120/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 2.0130e-07 - accuracy: 1.0000 - val_loss: 0.7204 - val_accuracy: 0.9030\n",
      "Epoch 121/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.9209e-07 - accuracy: 1.0000 - val_loss: 0.7223 - val_accuracy: 0.9030\n",
      "Epoch 122/200\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 1.8281e-07 - accuracy: 1.0000 - val_loss: 0.7240 - val_accuracy: 0.9030\n",
      "Epoch 123/200\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 1.7421e-07 - accuracy: 1.0000 - val_loss: 0.7264 - val_accuracy: 0.9030\n",
      "Epoch 124/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.6555e-07 - accuracy: 1.0000 - val_loss: 0.7286 - val_accuracy: 0.9030\n",
      "Epoch 125/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.5769e-07 - accuracy: 1.0000 - val_loss: 0.7305 - val_accuracy: 0.9030\n",
      "Epoch 126/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.5016e-07 - accuracy: 1.0000 - val_loss: 0.7324 - val_accuracy: 0.9030\n",
      "Epoch 127/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.4289e-07 - accuracy: 1.0000 - val_loss: 0.7350 - val_accuracy: 0.9030\n",
      "Epoch 128/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.3619e-07 - accuracy: 1.0000 - val_loss: 0.7368 - val_accuracy: 0.9030\n",
      "Epoch 129/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.2987e-07 - accuracy: 1.0000 - val_loss: 0.7386 - val_accuracy: 0.9030\n",
      "Epoch 130/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.2367e-07 - accuracy: 1.0000 - val_loss: 0.7410 - val_accuracy: 0.9030\n",
      "Epoch 131/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1788e-07 - accuracy: 1.0000 - val_loss: 0.7431 - val_accuracy: 0.9030\n",
      "Epoch 132/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.1229e-07 - accuracy: 1.0000 - val_loss: 0.7454 - val_accuracy: 0.9030\n",
      "Epoch 133/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.0710e-07 - accuracy: 1.0000 - val_loss: 0.7469 - val_accuracy: 0.9030\n",
      "Epoch 134/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.0205e-07 - accuracy: 1.0000 - val_loss: 0.7494 - val_accuracy: 0.9030\n",
      "Epoch 135/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 9.7332e-08 - accuracy: 1.0000 - val_loss: 0.7512 - val_accuracy: 0.9030\n",
      "Epoch 136/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 9.2650e-08 - accuracy: 1.0000 - val_loss: 0.7538 - val_accuracy: 0.9030\n",
      "Epoch 137/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 8.8485e-08 - accuracy: 1.0000 - val_loss: 0.7556 - val_accuracy: 0.9030\n",
      "Epoch 138/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 8.4262e-08 - accuracy: 1.0000 - val_loss: 0.7572 - val_accuracy: 0.9030\n",
      "Epoch 139/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 8.0293e-08 - accuracy: 1.0000 - val_loss: 0.7597 - val_accuracy: 0.9030\n",
      "Epoch 140/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 7.6598e-08 - accuracy: 1.0000 - val_loss: 0.7614 - val_accuracy: 0.9030\n",
      "Epoch 141/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 7.3019e-08 - accuracy: 1.0000 - val_loss: 0.7635 - val_accuracy: 0.9030\n",
      "Epoch 142/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 6.9667e-08 - accuracy: 1.0000 - val_loss: 0.7657 - val_accuracy: 0.9030\n",
      "Epoch 143/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 6.6440e-08 - accuracy: 1.0000 - val_loss: 0.7678 - val_accuracy: 0.9030\n",
      "Epoch 144/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 6.3388e-08 - accuracy: 1.0000 - val_loss: 0.7699 - val_accuracy: 0.9030\n",
      "Epoch 145/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 6.0452e-08 - accuracy: 1.0000 - val_loss: 0.7716 - val_accuracy: 0.9030\n",
      "Epoch 146/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 5.7701e-08 - accuracy: 1.0000 - val_loss: 0.7736 - val_accuracy: 0.9030\n",
      "Epoch 147/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 5.4965e-08 - accuracy: 1.0000 - val_loss: 0.7763 - val_accuracy: 0.9030\n",
      "Epoch 148/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 5.2475e-08 - accuracy: 1.0000 - val_loss: 0.7782 - val_accuracy: 0.9030\n",
      "Epoch 149/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 5.0060e-08 - accuracy: 1.0000 - val_loss: 0.7799 - val_accuracy: 0.9030\n",
      "Epoch 150/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 4.7768e-08 - accuracy: 1.0000 - val_loss: 0.7824 - val_accuracy: 0.9030\n",
      "Epoch 151/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 4.5595e-08 - accuracy: 1.0000 - val_loss: 0.7838 - val_accuracy: 0.9030\n",
      "Epoch 152/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 4.3522e-08 - accuracy: 1.0000 - val_loss: 0.7864 - val_accuracy: 0.9030\n",
      "Epoch 153/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 4.1531e-08 - accuracy: 1.0000 - val_loss: 0.7887 - val_accuracy: 0.9030\n",
      "Epoch 154/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.9691e-08 - accuracy: 1.0000 - val_loss: 0.7901 - val_accuracy: 0.9030\n",
      "Epoch 155/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 3.7869e-08 - accuracy: 1.0000 - val_loss: 0.7923 - val_accuracy: 0.9030\n",
      "Epoch 156/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.6158e-08 - accuracy: 1.0000 - val_loss: 0.7946 - val_accuracy: 0.9030\n",
      "Epoch 157/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.4541e-08 - accuracy: 1.0000 - val_loss: 0.7964 - val_accuracy: 0.9030\n",
      "Epoch 158/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.3023e-08 - accuracy: 1.0000 - val_loss: 0.7984 - val_accuracy: 0.9030\n",
      "Epoch 159/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 3.1532e-08 - accuracy: 1.0000 - val_loss: 0.8002 - val_accuracy: 0.9030\n",
      "Epoch 160/200\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 3.0128e-08 - accuracy: 1.0000 - val_loss: 0.8022 - val_accuracy: 0.9030\n",
      "Epoch 161/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.8806e-08 - accuracy: 1.0000 - val_loss: 0.8041 - val_accuracy: 0.9030\n",
      "Epoch 162/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 2.7492e-08 - accuracy: 1.0000 - val_loss: 0.8068 - val_accuracy: 0.9030\n",
      "Epoch 163/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.6268e-08 - accuracy: 1.0000 - val_loss: 0.8083 - val_accuracy: 0.9030\n",
      "Epoch 164/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.5116e-08 - accuracy: 1.0000 - val_loss: 0.8102 - val_accuracy: 0.9030\n",
      "Epoch 165/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.3957e-08 - accuracy: 1.0000 - val_loss: 0.8125 - val_accuracy: 0.9030\n",
      "Epoch 166/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.2903e-08 - accuracy: 1.0000 - val_loss: 0.8144 - val_accuracy: 0.9030\n",
      "Epoch 167/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.1909e-08 - accuracy: 1.0000 - val_loss: 0.8166 - val_accuracy: 0.9030\n",
      "Epoch 168/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.0900e-08 - accuracy: 1.0000 - val_loss: 0.8189 - val_accuracy: 0.9030\n",
      "Epoch 169/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 2.0006e-08 - accuracy: 1.0000 - val_loss: 0.8210 - val_accuracy: 0.9030\n",
      "Epoch 170/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.9161e-08 - accuracy: 1.0000 - val_loss: 0.8232 - val_accuracy: 0.9030\n",
      "Epoch 171/200\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 1.8349e-08 - accuracy: 1.0000 - val_loss: 0.8247 - val_accuracy: 0.9030\n",
      "Epoch 172/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.7527e-08 - accuracy: 1.0000 - val_loss: 0.8269 - val_accuracy: 0.9030\n",
      "Epoch 173/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.6808e-08 - accuracy: 1.0000 - val_loss: 0.8287 - val_accuracy: 0.9030\n",
      "Epoch 174/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.6096e-08 - accuracy: 1.0000 - val_loss: 0.8306 - val_accuracy: 0.9030\n",
      "Epoch 175/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.5417e-08 - accuracy: 1.0000 - val_loss: 0.8327 - val_accuracy: 0.9030\n",
      "Epoch 176/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.4712e-08 - accuracy: 1.0000 - val_loss: 0.8351 - val_accuracy: 0.9030\n",
      "Epoch 177/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.4175e-08 - accuracy: 1.0000 - val_loss: 0.8368 - val_accuracy: 0.9030\n",
      "Epoch 178/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.3555e-08 - accuracy: 1.0000 - val_loss: 0.8385 - val_accuracy: 0.9030\n",
      "Epoch 179/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.2994e-08 - accuracy: 1.0000 - val_loss: 0.8406 - val_accuracy: 0.9030\n",
      "Epoch 180/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.2487e-08 - accuracy: 1.0000 - val_loss: 0.8424 - val_accuracy: 0.9030\n",
      "Epoch 181/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.1982e-08 - accuracy: 1.0000 - val_loss: 0.8446 - val_accuracy: 0.9030\n",
      "Epoch 182/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 1.1502e-08 - accuracy: 1.0000 - val_loss: 0.8463 - val_accuracy: 0.9030\n",
      "Epoch 183/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.1036e-08 - accuracy: 1.0000 - val_loss: 0.8488 - val_accuracy: 0.9030\n",
      "Epoch 184/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 1.0655e-08 - accuracy: 1.0000 - val_loss: 0.8498 - val_accuracy: 0.9030\n",
      "Epoch 185/200\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 1.0200e-08 - accuracy: 1.0000 - val_loss: 0.8523 - val_accuracy: 0.9030\n",
      "Epoch 186/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 9.8979e-09 - accuracy: 1.0000 - val_loss: 0.8535 - val_accuracy: 0.9030\n",
      "Epoch 187/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 9.4985e-09 - accuracy: 1.0000 - val_loss: 0.8559 - val_accuracy: 0.9030\n",
      "Epoch 188/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 9.1477e-09 - accuracy: 1.0000 - val_loss: 0.8578 - val_accuracy: 0.9030\n",
      "Epoch 189/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 8.8305e-09 - accuracy: 1.0000 - val_loss: 0.8593 - val_accuracy: 0.9030\n",
      "Epoch 190/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 8.5156e-09 - accuracy: 1.0000 - val_loss: 0.8614 - val_accuracy: 0.9030\n",
      "Epoch 191/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 8.2282e-09 - accuracy: 1.0000 - val_loss: 0.8634 - val_accuracy: 0.9016\n",
      "Epoch 192/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 7.9789e-09 - accuracy: 1.0000 - val_loss: 0.8654 - val_accuracy: 0.9016\n",
      "Epoch 193/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 7.7126e-09 - accuracy: 1.0000 - val_loss: 0.8668 - val_accuracy: 0.9016\n",
      "Epoch 194/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 7.4499e-09 - accuracy: 1.0000 - val_loss: 0.8691 - val_accuracy: 0.9016\n",
      "Epoch 195/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 7.2096e-09 - accuracy: 1.0000 - val_loss: 0.8712 - val_accuracy: 0.9001\n",
      "Epoch 196/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 7.0309e-09 - accuracy: 1.0000 - val_loss: 0.8732 - val_accuracy: 0.9001\n",
      "Epoch 197/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 6.8452e-09 - accuracy: 1.0000 - val_loss: 0.8750 - val_accuracy: 0.9001\n",
      "Epoch 198/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 6.6612e-09 - accuracy: 1.0000 - val_loss: 0.8768 - val_accuracy: 0.9001\n",
      "Epoch 199/200\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 6.4414e-09 - accuracy: 1.0000 - val_loss: 0.8787 - val_accuracy: 0.9001\n",
      "Epoch 200/200\n",
      "87/87 [==============================] - 1s 8ms/step - loss: 6.2760e-09 - accuracy: 1.0000 - val_loss: 0.8808 - val_accuracy: 0.9001\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'Tagalog_CNN_Vectors.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 31\u001b[0m\n\u001b[0;32m     22\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X_train, y_train,\n\u001b[0;32m     23\u001b[0m                     epochs\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,\n\u001b[0;32m     24\u001b[0m                     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     25\u001b[0m                     validation_data\u001b[39m=\u001b[39m(X_test, y_test),\n\u001b[0;32m     26\u001b[0m                     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m \u001b[39m#, \u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m model\u001b[39m.\u001b[39;49mload_weights(bst_model_path)\n\u001b[0;32m     32\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_train, y_train, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining Accuracy: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(accuracy))\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages\\h5py\\_hl\\files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    558\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    560\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[0;32m    561\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[0;32m    562\u001b[0m                      meta_block_size\u001b[39m=\u001b[39mmeta_block_size,\n\u001b[0;32m    563\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    564\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[0;32m    565\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[0;32m    566\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 567\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39;49mswmr)\n\u001b[0;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    570\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages\\h5py\\_hl\\files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[0;32m    230\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 231\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[0;32m    232\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    233\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'Tagalog_CNN_Vectors.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen, \n",
    "                           trainable=False))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "STAMP = f'Tagalog_CNN_Vectors'\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=20)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "#, \n",
    "\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy:  0.9001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 300)          3214200   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              186880    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 24)                3096      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,404,201\n",
      "Trainable params: 3,404,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "87/87 [==============================] - 7s 36ms/step - loss: 0.4151 - accuracy: 0.8099 - val_loss: 0.3849 - val_accuracy: 0.8408\n",
      "Epoch 2/50\n",
      "12/87 [===>..........................] - ETA: 1s - loss: 0.1723 - accuracy: 0.9323"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m RNN_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m               loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      9\u001b[0m RNN_model\u001b[39m.\u001b[39msummary()\n\u001b[1;32m---> 10\u001b[0m history \u001b[39m=\u001b[39m RNN_model\u001b[39m.\u001b[39;49mfit(X_train, y_train,\n\u001b[0;32m     11\u001b[0m                     epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m     12\u001b[0m                     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     13\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(X_test, y_test),\n\u001b[0;32m     14\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n\u001b[0;32m     17\u001b[0m loss, accuracy \u001b[39m=\u001b[39m RNN_model\u001b[39m.\u001b[39mevaluate(X_train, y_train, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining Accuracy: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(accuracy))\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\tf-gpu-2.12\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RNN_model = Sequential()\n",
    "RNN_model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "RNN_model.add(layers.Bidirectional(layers.LSTM(64)))\n",
    "RNN_model.add(layers.Dense(24, activation='relu'))\n",
    "RNN_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "RNN_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "RNN_model.summary()\n",
    "history = RNN_model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=32)\n",
    "\n",
    "\n",
    "loss, accuracy = RNN_model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = RNN_model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 300)          3214200   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              439296    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                2570      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,656,077\n",
      "Trainable params: 441,877\n",
      "Non-trainable params: 3,214,200\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "87/87 [==============================] - 6s 30ms/step - loss: 0.3098 - accuracy: 0.8693 - val_loss: 0.2240 - val_accuracy: 0.9132\n",
      "Epoch 2/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 0.1374 - accuracy: 0.9507 - val_loss: 0.2298 - val_accuracy: 0.9074\n",
      "Epoch 3/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 0.0657 - accuracy: 0.9786 - val_loss: 0.2205 - val_accuracy: 0.9219\n",
      "Epoch 4/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 0.0320 - accuracy: 0.9891 - val_loss: 0.2854 - val_accuracy: 0.9132\n",
      "Epoch 5/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 0.0100 - accuracy: 0.9982 - val_loss: 0.3115 - val_accuracy: 0.9204\n",
      "Epoch 6/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.3219 - val_accuracy: 0.9132\n",
      "Epoch 7/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3651 - val_accuracy: 0.9219\n",
      "Epoch 8/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.9552e-04 - accuracy: 1.0000 - val_loss: 0.3760 - val_accuracy: 0.9233\n",
      "Epoch 9/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.5438e-04 - accuracy: 1.0000 - val_loss: 0.3967 - val_accuracy: 0.9233\n",
      "Epoch 10/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.7149e-04 - accuracy: 1.0000 - val_loss: 0.4110 - val_accuracy: 0.9219\n",
      "Epoch 11/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 2.2017e-04 - accuracy: 1.0000 - val_loss: 0.4137 - val_accuracy: 0.9219\n",
      "Epoch 12/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 1.7900e-04 - accuracy: 1.0000 - val_loss: 0.4232 - val_accuracy: 0.9219\n",
      "Epoch 13/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.4796e-04 - accuracy: 1.0000 - val_loss: 0.4394 - val_accuracy: 0.9219\n",
      "Epoch 14/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.2694e-04 - accuracy: 1.0000 - val_loss: 0.4407 - val_accuracy: 0.9233\n",
      "Epoch 15/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.0897e-04 - accuracy: 1.0000 - val_loss: 0.4501 - val_accuracy: 0.9219\n",
      "Epoch 16/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 9.2867e-05 - accuracy: 1.0000 - val_loss: 0.4593 - val_accuracy: 0.9204\n",
      "Epoch 17/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 8.0968e-05 - accuracy: 1.0000 - val_loss: 0.4637 - val_accuracy: 0.9204\n",
      "Epoch 18/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 7.1103e-05 - accuracy: 1.0000 - val_loss: 0.4738 - val_accuracy: 0.9204\n",
      "Epoch 19/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 6.3377e-05 - accuracy: 1.0000 - val_loss: 0.4775 - val_accuracy: 0.9204\n",
      "Epoch 20/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 5.6080e-05 - accuracy: 1.0000 - val_loss: 0.4830 - val_accuracy: 0.9204\n",
      "Epoch 21/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 5.0356e-05 - accuracy: 1.0000 - val_loss: 0.4891 - val_accuracy: 0.9204\n",
      "Epoch 22/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 4.4825e-05 - accuracy: 1.0000 - val_loss: 0.4976 - val_accuracy: 0.9204\n",
      "Epoch 23/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.0358e-05 - accuracy: 1.0000 - val_loss: 0.5029 - val_accuracy: 0.9204\n",
      "Epoch 24/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 3.6667e-05 - accuracy: 1.0000 - val_loss: 0.5110 - val_accuracy: 0.9190\n",
      "Epoch 25/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.3246e-05 - accuracy: 1.0000 - val_loss: 0.5140 - val_accuracy: 0.9204\n",
      "Epoch 26/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 3.0352e-05 - accuracy: 1.0000 - val_loss: 0.5186 - val_accuracy: 0.9204\n",
      "Epoch 27/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.7499e-05 - accuracy: 1.0000 - val_loss: 0.5210 - val_accuracy: 0.9204\n",
      "Epoch 28/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.5134e-05 - accuracy: 1.0000 - val_loss: 0.5291 - val_accuracy: 0.9204\n",
      "Epoch 29/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.3093e-05 - accuracy: 1.0000 - val_loss: 0.5344 - val_accuracy: 0.9204\n",
      "Epoch 30/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.1147e-05 - accuracy: 1.0000 - val_loss: 0.5392 - val_accuracy: 0.9204\n",
      "Epoch 31/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.9455e-05 - accuracy: 1.0000 - val_loss: 0.5440 - val_accuracy: 0.9190\n",
      "Epoch 32/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.7996e-05 - accuracy: 1.0000 - val_loss: 0.5478 - val_accuracy: 0.9204\n",
      "Epoch 33/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.6645e-05 - accuracy: 1.0000 - val_loss: 0.5527 - val_accuracy: 0.9190\n",
      "Epoch 34/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.5296e-05 - accuracy: 1.0000 - val_loss: 0.5565 - val_accuracy: 0.9190\n",
      "Epoch 35/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.4249e-05 - accuracy: 1.0000 - val_loss: 0.5627 - val_accuracy: 0.9175\n",
      "Epoch 36/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.3194e-05 - accuracy: 1.0000 - val_loss: 0.5657 - val_accuracy: 0.9190\n",
      "Epoch 37/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.2329e-05 - accuracy: 1.0000 - val_loss: 0.5687 - val_accuracy: 0.9204\n",
      "Epoch 38/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.1377e-05 - accuracy: 1.0000 - val_loss: 0.5763 - val_accuracy: 0.9190\n",
      "Epoch 39/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.0708e-05 - accuracy: 1.0000 - val_loss: 0.5801 - val_accuracy: 0.9190\n",
      "Epoch 40/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 9.8827e-06 - accuracy: 1.0000 - val_loss: 0.5814 - val_accuracy: 0.9190\n",
      "Epoch 41/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 9.2047e-06 - accuracy: 1.0000 - val_loss: 0.5862 - val_accuracy: 0.9190\n",
      "Epoch 42/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 8.5939e-06 - accuracy: 1.0000 - val_loss: 0.5885 - val_accuracy: 0.9175\n",
      "Epoch 43/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 8.0856e-06 - accuracy: 1.0000 - val_loss: 0.5932 - val_accuracy: 0.9190\n",
      "Epoch 44/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 7.5315e-06 - accuracy: 1.0000 - val_loss: 0.5969 - val_accuracy: 0.9190\n",
      "Epoch 45/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 7.0510e-06 - accuracy: 1.0000 - val_loss: 0.6025 - val_accuracy: 0.9190\n",
      "Epoch 46/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 6.5998e-06 - accuracy: 1.0000 - val_loss: 0.6034 - val_accuracy: 0.9190\n",
      "Epoch 47/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 6.1835e-06 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 0.9175\n",
      "Epoch 48/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 5.8070e-06 - accuracy: 1.0000 - val_loss: 0.6123 - val_accuracy: 0.9190\n",
      "Epoch 49/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 5.4955e-06 - accuracy: 1.0000 - val_loss: 0.6180 - val_accuracy: 0.9204\n",
      "Epoch 50/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 5.1203e-06 - accuracy: 1.0000 - val_loss: 0.6206 - val_accuracy: 0.9175\n",
      "Epoch 51/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 4.8106e-06 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 0.9190\n",
      "Epoch 52/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.5714e-06 - accuracy: 1.0000 - val_loss: 0.6284 - val_accuracy: 0.9204\n",
      "Epoch 53/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 4.2520e-06 - accuracy: 1.0000 - val_loss: 0.6329 - val_accuracy: 0.9190\n",
      "Epoch 54/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 4.0267e-06 - accuracy: 1.0000 - val_loss: 0.6355 - val_accuracy: 0.9204\n",
      "Epoch 55/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 3.7743e-06 - accuracy: 1.0000 - val_loss: 0.6403 - val_accuracy: 0.9190\n",
      "Epoch 56/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 3.5500e-06 - accuracy: 1.0000 - val_loss: 0.6426 - val_accuracy: 0.9204\n",
      "Epoch 57/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.3550e-06 - accuracy: 1.0000 - val_loss: 0.6450 - val_accuracy: 0.9204\n",
      "Epoch 58/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.1547e-06 - accuracy: 1.0000 - val_loss: 0.6475 - val_accuracy: 0.9190\n",
      "Epoch 59/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 2.9730e-06 - accuracy: 1.0000 - val_loss: 0.6522 - val_accuracy: 0.9204\n",
      "Epoch 60/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.8063e-06 - accuracy: 1.0000 - val_loss: 0.6566 - val_accuracy: 0.9190\n",
      "Epoch 61/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.6460e-06 - accuracy: 1.0000 - val_loss: 0.6607 - val_accuracy: 0.9190\n",
      "Epoch 62/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 2.4975e-06 - accuracy: 1.0000 - val_loss: 0.6624 - val_accuracy: 0.9219\n",
      "Epoch 63/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.3597e-06 - accuracy: 1.0000 - val_loss: 0.6672 - val_accuracy: 0.9190\n",
      "Epoch 64/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.2270e-06 - accuracy: 1.0000 - val_loss: 0.6699 - val_accuracy: 0.9190\n",
      "Epoch 65/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 2.1121e-06 - accuracy: 1.0000 - val_loss: 0.6732 - val_accuracy: 0.9190\n",
      "Epoch 66/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 1.9948e-06 - accuracy: 1.0000 - val_loss: 0.6781 - val_accuracy: 0.9190\n",
      "Epoch 67/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.8881e-06 - accuracy: 1.0000 - val_loss: 0.6805 - val_accuracy: 0.9190\n",
      "Epoch 68/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.7851e-06 - accuracy: 1.0000 - val_loss: 0.6857 - val_accuracy: 0.9190\n",
      "Epoch 69/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 1.6910e-06 - accuracy: 1.0000 - val_loss: 0.6859 - val_accuracy: 0.9204\n",
      "Epoch 70/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.5950e-06 - accuracy: 1.0000 - val_loss: 0.6889 - val_accuracy: 0.9175\n",
      "Epoch 71/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.5333e-06 - accuracy: 1.0000 - val_loss: 0.6932 - val_accuracy: 0.9190\n",
      "Epoch 72/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.4296e-06 - accuracy: 1.0000 - val_loss: 0.6975 - val_accuracy: 0.9190\n",
      "Epoch 73/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.3558e-06 - accuracy: 1.0000 - val_loss: 0.7016 - val_accuracy: 0.9190\n",
      "Epoch 74/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.2879e-06 - accuracy: 1.0000 - val_loss: 0.7059 - val_accuracy: 0.9190\n",
      "Epoch 75/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.2110e-06 - accuracy: 1.0000 - val_loss: 0.7093 - val_accuracy: 0.9190\n",
      "Epoch 76/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.1545e-06 - accuracy: 1.0000 - val_loss: 0.7102 - val_accuracy: 0.9175\n",
      "Epoch 77/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.0903e-06 - accuracy: 1.0000 - val_loss: 0.7150 - val_accuracy: 0.9190\n",
      "Epoch 78/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.0416e-06 - accuracy: 1.0000 - val_loss: 0.7185 - val_accuracy: 0.9190\n",
      "Epoch 79/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 9.7668e-07 - accuracy: 1.0000 - val_loss: 0.7232 - val_accuracy: 0.9190\n",
      "Epoch 80/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 9.3002e-07 - accuracy: 1.0000 - val_loss: 0.7252 - val_accuracy: 0.9190\n",
      "Epoch 81/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 8.8115e-07 - accuracy: 1.0000 - val_loss: 0.7265 - val_accuracy: 0.9161\n",
      "Epoch 82/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 8.4054e-07 - accuracy: 1.0000 - val_loss: 0.7305 - val_accuracy: 0.9175\n",
      "Epoch 83/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 7.9160e-07 - accuracy: 1.0000 - val_loss: 0.7351 - val_accuracy: 0.9190\n",
      "Epoch 84/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 7.4649e-07 - accuracy: 1.0000 - val_loss: 0.7361 - val_accuracy: 0.9161\n",
      "Epoch 85/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 7.0905e-07 - accuracy: 1.0000 - val_loss: 0.7415 - val_accuracy: 0.9175\n",
      "Epoch 86/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 6.7664e-07 - accuracy: 1.0000 - val_loss: 0.7445 - val_accuracy: 0.9175\n",
      "Epoch 87/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 6.4033e-07 - accuracy: 1.0000 - val_loss: 0.7479 - val_accuracy: 0.9175\n",
      "Epoch 88/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 6.0901e-07 - accuracy: 1.0000 - val_loss: 0.7494 - val_accuracy: 0.9161\n",
      "Epoch 89/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 5.7713e-07 - accuracy: 1.0000 - val_loss: 0.7522 - val_accuracy: 0.9161\n",
      "Epoch 90/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 5.4963e-07 - accuracy: 1.0000 - val_loss: 0.7581 - val_accuracy: 0.9175\n",
      "Epoch 91/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 5.2080e-07 - accuracy: 1.0000 - val_loss: 0.7602 - val_accuracy: 0.9175\n",
      "Epoch 92/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.9390e-07 - accuracy: 1.0000 - val_loss: 0.7627 - val_accuracy: 0.9161\n",
      "Epoch 93/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.6951e-07 - accuracy: 1.0000 - val_loss: 0.7672 - val_accuracy: 0.9161\n",
      "Epoch 94/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.4502e-07 - accuracy: 1.0000 - val_loss: 0.7699 - val_accuracy: 0.9161\n",
      "Epoch 95/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.2107e-07 - accuracy: 1.0000 - val_loss: 0.7725 - val_accuracy: 0.9161\n",
      "Epoch 96/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.0060e-07 - accuracy: 1.0000 - val_loss: 0.7759 - val_accuracy: 0.9161\n",
      "Epoch 97/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.8139e-07 - accuracy: 1.0000 - val_loss: 0.7788 - val_accuracy: 0.9161\n",
      "Epoch 98/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 3.6369e-07 - accuracy: 1.0000 - val_loss: 0.7817 - val_accuracy: 0.9161\n",
      "Epoch 99/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.4366e-07 - accuracy: 1.0000 - val_loss: 0.7859 - val_accuracy: 0.9146\n",
      "Epoch 100/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.2684e-07 - accuracy: 1.0000 - val_loss: 0.7880 - val_accuracy: 0.9161\n",
      "Epoch 101/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.1216e-07 - accuracy: 1.0000 - val_loss: 0.7919 - val_accuracy: 0.9161\n",
      "Epoch 102/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.9451e-07 - accuracy: 1.0000 - val_loss: 0.7927 - val_accuracy: 0.9161\n",
      "Epoch 103/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.8380e-07 - accuracy: 1.0000 - val_loss: 0.7966 - val_accuracy: 0.9161\n",
      "Epoch 104/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.6651e-07 - accuracy: 1.0000 - val_loss: 0.8006 - val_accuracy: 0.9146\n",
      "Epoch 105/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 2.5357e-07 - accuracy: 1.0000 - val_loss: 0.8040 - val_accuracy: 0.9146\n",
      "Epoch 106/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.4087e-07 - accuracy: 1.0000 - val_loss: 0.8082 - val_accuracy: 0.9161\n",
      "Epoch 107/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.3018e-07 - accuracy: 1.0000 - val_loss: 0.8101 - val_accuracy: 0.9146\n",
      "Epoch 108/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.1762e-07 - accuracy: 1.0000 - val_loss: 0.8140 - val_accuracy: 0.9161\n",
      "Epoch 109/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.0781e-07 - accuracy: 1.0000 - val_loss: 0.8152 - val_accuracy: 0.9146\n",
      "Epoch 110/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.9754e-07 - accuracy: 1.0000 - val_loss: 0.8185 - val_accuracy: 0.9146\n",
      "Epoch 111/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.8847e-07 - accuracy: 1.0000 - val_loss: 0.8208 - val_accuracy: 0.9161\n",
      "Epoch 112/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.7918e-07 - accuracy: 1.0000 - val_loss: 0.8252 - val_accuracy: 0.9146\n",
      "Epoch 113/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.7020e-07 - accuracy: 1.0000 - val_loss: 0.8285 - val_accuracy: 0.9146\n",
      "Epoch 114/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.6211e-07 - accuracy: 1.0000 - val_loss: 0.8328 - val_accuracy: 0.9146\n",
      "Epoch 115/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.5365e-07 - accuracy: 1.0000 - val_loss: 0.8344 - val_accuracy: 0.9146\n",
      "Epoch 116/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.4553e-07 - accuracy: 1.0000 - val_loss: 0.8370 - val_accuracy: 0.9161\n",
      "Epoch 117/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.3937e-07 - accuracy: 1.0000 - val_loss: 0.8415 - val_accuracy: 0.9161\n",
      "Epoch 118/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.3229e-07 - accuracy: 1.0000 - val_loss: 0.8445 - val_accuracy: 0.9161\n",
      "Epoch 119/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.2601e-07 - accuracy: 1.0000 - val_loss: 0.8472 - val_accuracy: 0.9146\n",
      "Epoch 120/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.1938e-07 - accuracy: 1.0000 - val_loss: 0.8499 - val_accuracy: 0.9146\n",
      "Epoch 121/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.1395e-07 - accuracy: 1.0000 - val_loss: 0.8553 - val_accuracy: 0.9146\n",
      "Epoch 122/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.0891e-07 - accuracy: 1.0000 - val_loss: 0.8565 - val_accuracy: 0.9146\n",
      "Epoch 123/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.0264e-07 - accuracy: 1.0000 - val_loss: 0.8606 - val_accuracy: 0.9161\n",
      "Epoch 124/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 9.8640e-08 - accuracy: 1.0000 - val_loss: 0.8634 - val_accuracy: 0.9161\n",
      "Epoch 125/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 9.3316e-08 - accuracy: 1.0000 - val_loss: 0.8659 - val_accuracy: 0.9146\n",
      "Epoch 126/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 8.9080e-08 - accuracy: 1.0000 - val_loss: 0.8693 - val_accuracy: 0.9146\n",
      "Epoch 127/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 8.4869e-08 - accuracy: 1.0000 - val_loss: 0.8728 - val_accuracy: 0.9146\n",
      "Epoch 128/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 8.1108e-08 - accuracy: 1.0000 - val_loss: 0.8752 - val_accuracy: 0.9146\n",
      "Epoch 129/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 7.6942e-08 - accuracy: 1.0000 - val_loss: 0.8782 - val_accuracy: 0.9146\n",
      "Epoch 130/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 7.3580e-08 - accuracy: 1.0000 - val_loss: 0.8813 - val_accuracy: 0.9146\n",
      "Epoch 131/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 6.9910e-08 - accuracy: 1.0000 - val_loss: 0.8834 - val_accuracy: 0.9146\n",
      "Epoch 132/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 6.6962e-08 - accuracy: 1.0000 - val_loss: 0.8867 - val_accuracy: 0.9146\n",
      "Epoch 133/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 6.3433e-08 - accuracy: 1.0000 - val_loss: 0.8899 - val_accuracy: 0.9161\n",
      "Epoch 134/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 6.0645e-08 - accuracy: 1.0000 - val_loss: 0.8933 - val_accuracy: 0.9146\n",
      "Epoch 135/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 5.7609e-08 - accuracy: 1.0000 - val_loss: 0.8971 - val_accuracy: 0.9146\n",
      "Epoch 136/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 5.5151e-08 - accuracy: 1.0000 - val_loss: 0.8994 - val_accuracy: 0.9161\n",
      "Epoch 137/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 5.2586e-08 - accuracy: 1.0000 - val_loss: 0.9015 - val_accuracy: 0.9161\n",
      "Epoch 138/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 4.9958e-08 - accuracy: 1.0000 - val_loss: 0.9056 - val_accuracy: 0.9161\n",
      "Epoch 139/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.7945e-08 - accuracy: 1.0000 - val_loss: 0.9081 - val_accuracy: 0.9161\n",
      "Epoch 140/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.5709e-08 - accuracy: 1.0000 - val_loss: 0.9114 - val_accuracy: 0.9161\n",
      "Epoch 141/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.3586e-08 - accuracy: 1.0000 - val_loss: 0.9142 - val_accuracy: 0.9161\n",
      "Epoch 142/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 4.1554e-08 - accuracy: 1.0000 - val_loss: 0.9169 - val_accuracy: 0.9161\n",
      "Epoch 143/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.9748e-08 - accuracy: 1.0000 - val_loss: 0.9197 - val_accuracy: 0.9161\n",
      "Epoch 144/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.7723e-08 - accuracy: 1.0000 - val_loss: 0.9226 - val_accuracy: 0.9161\n",
      "Epoch 145/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.6153e-08 - accuracy: 1.0000 - val_loss: 0.9257 - val_accuracy: 0.9161\n",
      "Epoch 146/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 3.4394e-08 - accuracy: 1.0000 - val_loss: 0.9286 - val_accuracy: 0.9161\n",
      "Epoch 147/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.3031e-08 - accuracy: 1.0000 - val_loss: 0.9323 - val_accuracy: 0.9161\n",
      "Epoch 148/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 3.1677e-08 - accuracy: 1.0000 - val_loss: 0.9350 - val_accuracy: 0.9161\n",
      "Epoch 149/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 3.0034e-08 - accuracy: 1.0000 - val_loss: 0.9371 - val_accuracy: 0.9161\n",
      "Epoch 150/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.8823e-08 - accuracy: 1.0000 - val_loss: 0.9409 - val_accuracy: 0.9161\n",
      "Epoch 151/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.7498e-08 - accuracy: 1.0000 - val_loss: 0.9430 - val_accuracy: 0.9161\n",
      "Epoch 152/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 2.6312e-08 - accuracy: 1.0000 - val_loss: 0.9458 - val_accuracy: 0.9161\n",
      "Epoch 153/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.4988e-08 - accuracy: 1.0000 - val_loss: 0.9490 - val_accuracy: 0.9161\n",
      "Epoch 154/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.3924e-08 - accuracy: 1.0000 - val_loss: 0.9515 - val_accuracy: 0.9161\n",
      "Epoch 155/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.2905e-08 - accuracy: 1.0000 - val_loss: 0.9547 - val_accuracy: 0.9161\n",
      "Epoch 156/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 2.1907e-08 - accuracy: 1.0000 - val_loss: 0.9570 - val_accuracy: 0.9146\n",
      "Epoch 157/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.0866e-08 - accuracy: 1.0000 - val_loss: 0.9606 - val_accuracy: 0.9161\n",
      "Epoch 158/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 2.0107e-08 - accuracy: 1.0000 - val_loss: 0.9632 - val_accuracy: 0.9161\n",
      "Epoch 159/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.9078e-08 - accuracy: 1.0000 - val_loss: 0.9656 - val_accuracy: 0.9146\n",
      "Epoch 160/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.8307e-08 - accuracy: 1.0000 - val_loss: 0.9689 - val_accuracy: 0.9161\n",
      "Epoch 161/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.7503e-08 - accuracy: 1.0000 - val_loss: 0.9707 - val_accuracy: 0.9146\n",
      "Epoch 162/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.6817e-08 - accuracy: 1.0000 - val_loss: 0.9746 - val_accuracy: 0.9146\n",
      "Epoch 163/200\n",
      "87/87 [==============================] - 2s 21ms/step - loss: 1.6011e-08 - accuracy: 1.0000 - val_loss: 0.9772 - val_accuracy: 0.9161\n",
      "Epoch 164/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.5312e-08 - accuracy: 1.0000 - val_loss: 0.9791 - val_accuracy: 0.9146\n",
      "Epoch 165/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.4634e-08 - accuracy: 1.0000 - val_loss: 0.9816 - val_accuracy: 0.9146\n",
      "Epoch 166/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.4024e-08 - accuracy: 1.0000 - val_loss: 0.9854 - val_accuracy: 0.9161\n",
      "Epoch 167/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.3414e-08 - accuracy: 1.0000 - val_loss: 0.9880 - val_accuracy: 0.9146\n",
      "Epoch 168/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.2904e-08 - accuracy: 1.0000 - val_loss: 0.9906 - val_accuracy: 0.9146\n",
      "Epoch 169/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.2381e-08 - accuracy: 1.0000 - val_loss: 0.9945 - val_accuracy: 0.9175\n",
      "Epoch 170/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.1934e-08 - accuracy: 1.0000 - val_loss: 0.9961 - val_accuracy: 0.9161\n",
      "Epoch 171/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.1537e-08 - accuracy: 1.0000 - val_loss: 0.9994 - val_accuracy: 0.9161\n",
      "Epoch 172/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.0908e-08 - accuracy: 1.0000 - val_loss: 1.0021 - val_accuracy: 0.9161\n",
      "Epoch 173/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.0506e-08 - accuracy: 1.0000 - val_loss: 1.0045 - val_accuracy: 0.9161\n",
      "Epoch 174/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 1.0018e-08 - accuracy: 1.0000 - val_loss: 1.0067 - val_accuracy: 0.9161\n",
      "Epoch 175/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 9.7082e-09 - accuracy: 1.0000 - val_loss: 1.0105 - val_accuracy: 0.9175\n",
      "Epoch 176/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 9.2850e-09 - accuracy: 1.0000 - val_loss: 1.0124 - val_accuracy: 0.9161\n",
      "Epoch 177/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 8.9462e-09 - accuracy: 1.0000 - val_loss: 1.0141 - val_accuracy: 0.9161\n",
      "Epoch 178/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 8.5396e-09 - accuracy: 1.0000 - val_loss: 1.0172 - val_accuracy: 0.9161\n",
      "Epoch 179/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 8.2877e-09 - accuracy: 1.0000 - val_loss: 1.0191 - val_accuracy: 0.9161\n",
      "Epoch 180/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 7.9498e-09 - accuracy: 1.0000 - val_loss: 1.0217 - val_accuracy: 0.9161\n",
      "Epoch 181/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 7.6453e-09 - accuracy: 1.0000 - val_loss: 1.0244 - val_accuracy: 0.9161\n",
      "Epoch 182/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 7.3522e-09 - accuracy: 1.0000 - val_loss: 1.0268 - val_accuracy: 0.9161\n",
      "Epoch 183/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 7.0728e-09 - accuracy: 1.0000 - val_loss: 1.0278 - val_accuracy: 0.9161\n",
      "Epoch 184/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 6.8056e-09 - accuracy: 1.0000 - val_loss: 1.0309 - val_accuracy: 0.9161\n",
      "Epoch 185/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 6.5770e-09 - accuracy: 1.0000 - val_loss: 1.0330 - val_accuracy: 0.9161\n",
      "Epoch 186/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 6.3771e-09 - accuracy: 1.0000 - val_loss: 1.0364 - val_accuracy: 0.9161\n",
      "Epoch 187/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 6.1059e-09 - accuracy: 1.0000 - val_loss: 1.0394 - val_accuracy: 0.9146\n",
      "Epoch 188/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 5.9176e-09 - accuracy: 1.0000 - val_loss: 1.0415 - val_accuracy: 0.9161\n",
      "Epoch 189/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 5.6879e-09 - accuracy: 1.0000 - val_loss: 1.0433 - val_accuracy: 0.9161\n",
      "Epoch 190/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 5.5220e-09 - accuracy: 1.0000 - val_loss: 1.0449 - val_accuracy: 0.9146\n",
      "Epoch 191/200\n",
      "87/87 [==============================] - 2s 20ms/step - loss: 5.3669e-09 - accuracy: 1.0000 - val_loss: 1.0477 - val_accuracy: 0.9146\n",
      "Epoch 192/200\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 5.1458e-09 - accuracy: 1.0000 - val_loss: 1.0507 - val_accuracy: 0.9161\n",
      "Epoch 193/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.9908e-09 - accuracy: 1.0000 - val_loss: 1.0537 - val_accuracy: 0.9146\n",
      "Epoch 194/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.8388e-09 - accuracy: 1.0000 - val_loss: 1.0543 - val_accuracy: 0.9132\n",
      "Epoch 195/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.6603e-09 - accuracy: 1.0000 - val_loss: 1.0580 - val_accuracy: 0.9161\n",
      "Epoch 196/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.5169e-09 - accuracy: 1.0000 - val_loss: 1.0600 - val_accuracy: 0.9161\n",
      "Epoch 197/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.4420e-09 - accuracy: 1.0000 - val_loss: 1.0629 - val_accuracy: 0.9161\n",
      "Epoch 198/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.3120e-09 - accuracy: 1.0000 - val_loss: 1.0662 - val_accuracy: 0.9161\n",
      "Epoch 199/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.1696e-09 - accuracy: 1.0000 - val_loss: 1.0683 - val_accuracy: 0.9161\n",
      "Epoch 200/200\n",
      "87/87 [==============================] - 2s 19ms/step - loss: 4.0385e-09 - accuracy: 1.0000 - val_loss: 1.0699 - val_accuracy: 0.9146\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy:  0.9146\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "RNN_model = Sequential()\n",
    "RNN_model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen, \n",
    "                           trainable=False))\n",
    "RNN_model.add(layers.Bidirectional(layers.LSTM(128)))\n",
    "RNN_model.add(layers.Dense(10, activation='relu'))\n",
    "RNN_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "#opt = optimizers.rmsprop_v2.RMSProp(learning_rate=0.001)\n",
    "RNN_model.compile(optimizer='Adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "RNN_model.summary()\n",
    "history = RNN_model.fit(X_train, y_train,\n",
    "                    epochs=200,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=32)\n",
    "\n",
    "\n",
    "loss, accuracy = RNN_model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = RNN_model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=5000, char_level=True)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.2, random_state=49)\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "X_train.shape\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = create_embedding_matrix(\n",
    "    'glove/glove.6B.100d.txt',\n",
    "    tokenizer.word_index, embedding_dim)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# model = Sequential()\n",
    "# model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "# model.add(layers.Conv1D(128, 5, activation='tanh'))\n",
    "# model.add(layers.GlobalMaxPooling1D())\n",
    "# model.add(layers.Dense(10, activation='tanh'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='RMSProp',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 100, 100)          8200      \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 512)              731136    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 805,129\n",
      "Trainable params: 805,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "328/328 [==============================] - 11s 23ms/step - loss: 3.1660 - accuracy: 0.5827 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 2/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 3/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 4/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 5/200\n",
      "328/328 [==============================] - 7s 22ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 6/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 7/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 8/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 9/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 10/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 11/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 12/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 13/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 14/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 15/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 16/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 17/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 18/200\n",
      "328/328 [==============================] - 7s 23ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 19/200\n",
      "328/328 [==============================] - 7s 22ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 20/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 21/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 22/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 23/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 24/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 25/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 26/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 27/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 28/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 29/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 30/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 31/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 32/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 33/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 34/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 35/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 36/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 37/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 38/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 39/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 40/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 41/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 42/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 43/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 44/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 45/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 46/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 47/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 48/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 49/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 50/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 51/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 52/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 53/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 54/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 55/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 56/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 57/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 58/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 59/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 60/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 61/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 62/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 63/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 64/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 65/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 66/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 67/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 68/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 69/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 70/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 71/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 72/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 73/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 74/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 75/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 76/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 77/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 78/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 79/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 80/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 81/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 82/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 83/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 84/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 85/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 86/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 87/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 88/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 89/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 90/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 91/200\n",
      "328/328 [==============================] - 7s 21ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 92/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 93/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 94/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 95/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 96/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 97/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 98/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 99/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 100/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 101/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 102/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 103/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 104/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 105/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 106/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 107/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 108/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 109/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 110/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 111/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 112/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 113/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 114/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 115/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 116/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 117/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 118/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 119/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 120/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 121/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 122/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 123/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 124/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 125/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 126/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 127/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 128/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 129/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 130/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 131/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 132/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 133/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 134/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 135/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 136/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 137/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 138/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 139/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 140/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 141/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 142/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 143/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 144/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 145/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 146/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 147/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 148/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 149/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 150/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 151/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 152/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 153/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 154/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 155/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 156/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 157/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 158/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 159/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 160/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 161/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 162/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 163/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 164/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 165/200\n",
      "328/328 [==============================] - 6s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 166/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 167/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 168/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 169/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 170/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 171/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 172/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 173/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 174/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 175/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 176/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 177/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 178/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 179/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 180/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 181/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 182/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 183/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 184/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 185/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 186/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 187/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 188/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 189/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 190/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 191/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 192/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 193/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 194/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 195/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 196/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 197/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 198/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 199/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Epoch 200/200\n",
      "328/328 [==============================] - 7s 20ms/step - loss: 7.7007 - accuracy: 0.5008 - val_loss: 7.7596 - val_accuracy: 0.4969\n",
      "Training Accuracy: 0.5008\n",
      "Testing Accuracy:  0.4969\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RNN_model = Sequential()\n",
    "RNN_model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "RNN_model.add(layers.Bidirectional(layers.LSTM(256)))\n",
    "RNN_model.add(layers.Dense(128, activation='relu'))\n",
    "RNN_model.add(layers.Dense(1, activation='tanh'))\n",
    "opt = optimizers.rmsprop_v2.RMSProp(learning_rate=0.001)\n",
    "RNN_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "RNN_model.summary()\n",
    "history = RNN_model.fit(X_train, y_train,\n",
    "                    epochs=200,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=16)\n",
    "\n",
    "\n",
    "loss, accuracy = RNN_model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = RNN_model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_2.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
