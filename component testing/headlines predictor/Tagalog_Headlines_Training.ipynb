{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "import spacy.lang.tl as s_tl\n",
    "tl_stopwords = s_tl.stop_words.STOP_WORDS\n",
    "\n",
    "from sacremoses import MosesTokenizer\n",
    "import simplemma as slma\n",
    "mt = MosesTokenizer('tl')\n",
    "\n",
    "def remove_tl_stopwords(x):\n",
    "    no_sw = [word for word in x if word not in tl_stopwords]\n",
    "    return no_sw\n",
    "\n",
    "def clean_data_untokenized(x):\n",
    "    x = re.sub(\"[^a-zA-Z0-9']+\", \" \", x)\n",
    "    tokens = x.lower().split()\n",
    "    # x = mt.tokenize(x)\n",
    "    #tokens = [token.lower() for token in x ]\n",
    "    # tokens = x.lower().split() #removes non alphabet, undercase, then splits\n",
    "    no_sw = remove_tl_stopwords(tokens)\n",
    "    #lemmatized_tokens = [slma.lemmatize(token, lang='tl') for token in no_sw]\n",
    "    cleaned = ' '.join(no_sw)\n",
    "    return cleaned\n",
    "\n",
    "def clean_tokenize(x):\n",
    "    x = mt.tokenize(x)\n",
    "    tokens = [token.lower() for token in x ]\n",
    "    #tokens = x.lower().split() #removes non alphabet, undercase, then splits\n",
    "    no_sw = remove_tl_stopwords(tokens)\n",
    "    #lemmatized_tokens = [slma.lemmatize(token, lang='tl') for token in no_sw]\n",
    "    return no_sw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('Tagalog_Headlines_EngFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Rating</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>cleaned tokenized</th>\n",
       "      <th>period%</th>\n",
       "      <th>comma%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>question mark%</th>\n",
       "      <th>exclamation mark%</th>\n",
       "      <th>dash%</th>\n",
       "      <th>apostrophe%</th>\n",
       "      <th>close parenthesis%</th>\n",
       "      <th>capitalized%</th>\n",
       "      <th>slang words%</th>\n",
       "      <th>curse words%</th>\n",
       "      <th>with numericals%</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lalaki patay sa pamamaril sa Tondo, Maynila</td>\n",
       "      <td>Real</td>\n",
       "      <td>lalaki patay pamamaril tondo maynila</td>\n",
       "      <td>[lalaki, patay, pamamaril, tondo, ,, maynila]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[lalaki patay, patay pamamaril, pamamaril tond...</td>\n",
       "      <td>[lalaki patay pamamaril, patay pamamaril tondo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50 Pinoy na naipit sa kaguluhan sa Sudan, nail...</td>\n",
       "      <td>Real</td>\n",
       "      <td>50 pinoy naipit kaguluhan sudan nailikas</td>\n",
       "      <td>[50, pinoy, naipit, kaguluhan, sudan, ,, naili...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>[50 pinoy, pinoy naipit, naipit kaguluhan, kag...</td>\n",
       "      <td>[50 pinoy naipit, pinoy naipit kaguluhan, naip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#BoyingResign: Netizens galit kay DOJ Chief Re...</td>\n",
       "      <td>Real</td>\n",
       "      <td>boyingresign netizens galit kay doj chief remu...</td>\n",
       "      <td>[#, boyingresign, :, netizens, galit, kay, doj...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[boyingresign netizens, netizens galit, galit ...</td>\n",
       "      <td>[boyingresign netizens galit, netizens galit k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‘Backdoor entry’ ng Grab sa motorcycle taxi pi...</td>\n",
       "      <td>Real</td>\n",
       "      <td>backdoor entry grab motorcycle taxi pilot pina...</td>\n",
       "      <td>[‘, backdoor, entry, ’, grab, motorcycle, taxi...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[backdoor entry, entry grab, grab motorcycle, ...</td>\n",
       "      <td>[backdoor entry grab, entry grab motorcycle, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doktor nangangamba na na-mild stroke si De Lima</td>\n",
       "      <td>Real</td>\n",
       "      <td>doktor nangangamba mild stroke de</td>\n",
       "      <td>[doktor, nangangamba, na-mild, stroke, de]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[doktor nangangamba, nangangamba mild, mild st...</td>\n",
       "      <td>[doktor nangangamba mild, nangangamba mild str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>Remembering Marky Cielo, Proud Igorot and Star...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>remembering marky cielo proud igorot and stars...</td>\n",
       "      <td>[remembering, marky, cielo, ,, proud, igorot, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[remembering marky, marky cielo, cielo proud, ...</td>\n",
       "      <td>[remembering marky cielo, marky cielo proud, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>Netizen, inilabas ang sekretong galit dahil la...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>netizen inilabas sekretong galit lang my day k...</td>\n",
       "      <td>[netizen, ,, inilabas, sekretong, galit, lang,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[netizen inilabas, inilabas sekretong, sekreto...</td>\n",
       "      <td>[netizen inilabas sekretong, inilabas sekreton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>Pinoy Vlogger na tumulong kay Nas Daily noon, ...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>pinoy vlogger tumulong kay nas daily nagsalita...</td>\n",
       "      <td>[pinoy, vlogger, tumulong, kay, nas, daily, ,,...</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[pinoy vlogger, vlogger tumulong, tumulong kay...</td>\n",
       "      <td>[pinoy vlogger tumulong, vlogger tumulong kay,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>GRABE! UNANG GABI NG LAMAY SA BUROL NI KRIS AQ...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>grabe unang gabi lamay burol kris aquino dinag...</td>\n",
       "      <td>[grabe, !, unang, gabi, lamay, burol, kris, aq...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[grabe unang, unang gabi, gabi lamay, lamay bu...</td>\n",
       "      <td>[grabe unang gabi, unang gabi lamay, gabi lama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>Isang tattoo artist may walong asawa at magka...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>tattoo artist walong asawa magkakasamang nanin...</td>\n",
       "      <td>[tattoo, artist, walong, asawa, magkakasamang,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[tattoo artist, artist walong, walong asawa, a...</td>\n",
       "      <td>[tattoo artist walong, artist walong asawa, wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3452 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Statement Rating  \\\n",
       "0           Lalaki patay sa pamamaril sa Tondo, Maynila   Real   \n",
       "1     50 Pinoy na naipit sa kaguluhan sa Sudan, nail...   Real   \n",
       "2     #BoyingResign: Netizens galit kay DOJ Chief Re...   Real   \n",
       "3     ‘Backdoor entry’ ng Grab sa motorcycle taxi pi...   Real   \n",
       "4       Doktor nangangamba na na-mild stroke si De Lima   Real   \n",
       "...                                                 ...    ...   \n",
       "3447  Remembering Marky Cielo, Proud Igorot and Star...   Fake   \n",
       "3448  Netizen, inilabas ang sekretong galit dahil la...   Fake   \n",
       "3449  Pinoy Vlogger na tumulong kay Nas Daily noon, ...   Fake   \n",
       "3450  GRABE! UNANG GABI NG LAMAY SA BUROL NI KRIS AQ...   Fake   \n",
       "3451   Isang tattoo artist may walong asawa at magka...   Fake   \n",
       "\n",
       "                                                cleaned  \\\n",
       "0                  lalaki patay pamamaril tondo maynila   \n",
       "1              50 pinoy naipit kaguluhan sudan nailikas   \n",
       "2     boyingresign netizens galit kay doj chief remu...   \n",
       "3     backdoor entry grab motorcycle taxi pilot pina...   \n",
       "4                     doktor nangangamba mild stroke de   \n",
       "...                                                 ...   \n",
       "3447  remembering marky cielo proud igorot and stars...   \n",
       "3448  netizen inilabas sekretong galit lang my day k...   \n",
       "3449  pinoy vlogger tumulong kay nas daily nagsalita...   \n",
       "3450  grabe unang gabi lamay burol kris aquino dinag...   \n",
       "3451  tattoo artist walong asawa magkakasamang nanin...   \n",
       "\n",
       "                                      cleaned tokenized   period%    comma%  \\\n",
       "0         [lalaki, patay, pamamaril, tondo, ,, maynila]  0.000000  0.125000   \n",
       "1     [50, pinoy, naipit, kaguluhan, sudan, ,, naili...  0.000000  0.090909   \n",
       "2     [#, boyingresign, :, netizens, galit, kay, doj...  0.000000  0.000000   \n",
       "3     [‘, backdoor, entry, ’, grab, motorcycle, taxi...  0.000000  0.000000   \n",
       "4            [doktor, nangangamba, na-mild, stroke, de]  0.000000  0.000000   \n",
       "...                                                 ...       ...       ...   \n",
       "3447  [remembering, marky, cielo, ,, proud, igorot, ...  0.000000  0.083333   \n",
       "3448  [netizen, ,, inilabas, sekretong, galit, lang,...  0.000000  0.055556   \n",
       "3449  [pinoy, vlogger, tumulong, kay, nas, daily, ,,...  0.041667  0.083333   \n",
       "3450  [grabe, !, unang, gabi, lamay, burol, kris, aq...  0.050000  0.050000   \n",
       "3451  [tattoo, artist, walong, asawa, magkakasamang,...  0.000000  0.000000   \n",
       "\n",
       "        colon%  semicolon%  question mark%  exclamation mark%     dash%  \\\n",
       "0     0.000000    0.000000             0.0           0.000000  0.000000   \n",
       "1     0.000000    0.000000             0.0           0.000000  0.000000   \n",
       "2     0.055556    0.000000             0.0           0.000000  0.055556   \n",
       "3     0.000000    0.000000             0.0           0.000000  0.000000   \n",
       "4     0.000000    0.000000             0.0           0.000000  0.125000   \n",
       "...        ...         ...             ...                ...       ...   \n",
       "3447  0.000000    0.000000             0.0           0.083333  0.000000   \n",
       "3448  0.000000    0.055556             0.0           0.000000  0.000000   \n",
       "3449  0.041667    0.000000             0.0           0.000000  0.000000   \n",
       "3450  0.000000    0.000000             0.0           0.050000  0.050000   \n",
       "3451  0.000000    0.000000             0.0           0.000000  0.000000   \n",
       "\n",
       "      apostrophe%  close parenthesis%  capitalized%  slang words%  \\\n",
       "0             0.0                 0.0      0.000000           0.0   \n",
       "1             0.0                 0.0      0.000000           0.0   \n",
       "2             0.0                 0.0      0.055556           0.0   \n",
       "3             0.0                 0.0      0.000000           0.0   \n",
       "4             0.0                 0.0      0.000000           0.0   \n",
       "...           ...                 ...           ...           ...   \n",
       "3447          0.0                 0.0      0.000000           0.0   \n",
       "3448          0.0                 0.0      0.000000           0.0   \n",
       "3449          0.0                 0.0      0.000000           0.0   \n",
       "3450          0.0                 0.0      0.850000           0.0   \n",
       "3451          0.0                 0.0      0.000000           0.0   \n",
       "\n",
       "      curse words%  with numericals%  \\\n",
       "0              0.0          0.000000   \n",
       "1              0.0          0.090909   \n",
       "2              0.0          0.000000   \n",
       "3              0.0          0.000000   \n",
       "4              0.0          0.000000   \n",
       "...            ...               ...   \n",
       "3447           0.0          0.000000   \n",
       "3448           0.0          0.000000   \n",
       "3449           0.0          0.000000   \n",
       "3450           0.0          0.000000   \n",
       "3451           0.0          0.000000   \n",
       "\n",
       "                                                bigrams  \\\n",
       "0     [lalaki patay, patay pamamaril, pamamaril tond...   \n",
       "1     [50 pinoy, pinoy naipit, naipit kaguluhan, kag...   \n",
       "2     [boyingresign netizens, netizens galit, galit ...   \n",
       "3     [backdoor entry, entry grab, grab motorcycle, ...   \n",
       "4     [doktor nangangamba, nangangamba mild, mild st...   \n",
       "...                                                 ...   \n",
       "3447  [remembering marky, marky cielo, cielo proud, ...   \n",
       "3448  [netizen inilabas, inilabas sekretong, sekreto...   \n",
       "3449  [pinoy vlogger, vlogger tumulong, tumulong kay...   \n",
       "3450  [grabe unang, unang gabi, gabi lamay, lamay bu...   \n",
       "3451  [tattoo artist, artist walong, walong asawa, a...   \n",
       "\n",
       "                                               trigrams  \n",
       "0     [lalaki patay pamamaril, patay pamamaril tondo...  \n",
       "1     [50 pinoy naipit, pinoy naipit kaguluhan, naip...  \n",
       "2     [boyingresign netizens galit, netizens galit k...  \n",
       "3     [backdoor entry grab, entry grab motorcycle, g...  \n",
       "4     [doktor nangangamba mild, nangangamba mild str...  \n",
       "...                                                 ...  \n",
       "3447  [remembering marky cielo, marky cielo proud, c...  \n",
       "3448  [netizen inilabas sekretong, inilabas sekreton...  \n",
       "3449  [pinoy vlogger tumulong, vlogger tumulong kay,...  \n",
       "3450  [grabe unang gabi, unang gabi lamay, gabi lama...  \n",
       "3451  [tattoo artist walong, artist walong asawa, wa...  \n",
       "\n",
       "[3452 rows x 19 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in df.columns.values if col != 'Rating' and col != 'bigrams' and \n",
    "        col != 'trigrams' and col != 'cleaned' \n",
    "        and col != 'cleaned tokenized' and col != 'tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['Rating'] = le.fit_transform(df.Rating.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = df[cols]\n",
    "y = df['Rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>period%</th>\n",
       "      <th>comma%</th>\n",
       "      <th>colon%</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>question mark%</th>\n",
       "      <th>exclamation mark%</th>\n",
       "      <th>dash%</th>\n",
       "      <th>apostrophe%</th>\n",
       "      <th>close parenthesis%</th>\n",
       "      <th>capitalized%</th>\n",
       "      <th>slang words%</th>\n",
       "      <th>curse words%</th>\n",
       "      <th>with numericals%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lalaki patay sa pamamaril sa Tondo, Maynila</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50 Pinoy na naipit sa kaguluhan sa Sudan, nail...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#BoyingResign: Netizens galit kay DOJ Chief Re...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‘Backdoor entry’ ng Grab sa motorcycle taxi pi...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doktor nangangamba na na-mild stroke si De Lima</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>Remembering Marky Cielo, Proud Igorot and Star...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>Netizen, inilabas ang sekretong galit dahil la...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>Pinoy Vlogger na tumulong kay Nas Daily noon, ...</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>GRABE! UNANG GABI NG LAMAY SA BUROL NI KRIS AQ...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>Isang tattoo artist may walong asawa at magka...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3452 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Statement   period%    comma%  \\\n",
       "0           Lalaki patay sa pamamaril sa Tondo, Maynila  0.000000  0.125000   \n",
       "1     50 Pinoy na naipit sa kaguluhan sa Sudan, nail...  0.000000  0.090909   \n",
       "2     #BoyingResign: Netizens galit kay DOJ Chief Re...  0.000000  0.000000   \n",
       "3     ‘Backdoor entry’ ng Grab sa motorcycle taxi pi...  0.000000  0.000000   \n",
       "4       Doktor nangangamba na na-mild stroke si De Lima  0.000000  0.000000   \n",
       "...                                                 ...       ...       ...   \n",
       "3447  Remembering Marky Cielo, Proud Igorot and Star...  0.000000  0.083333   \n",
       "3448  Netizen, inilabas ang sekretong galit dahil la...  0.000000  0.055556   \n",
       "3449  Pinoy Vlogger na tumulong kay Nas Daily noon, ...  0.041667  0.083333   \n",
       "3450  GRABE! UNANG GABI NG LAMAY SA BUROL NI KRIS AQ...  0.050000  0.050000   \n",
       "3451   Isang tattoo artist may walong asawa at magka...  0.000000  0.000000   \n",
       "\n",
       "        colon%  semicolon%  question mark%  exclamation mark%     dash%  \\\n",
       "0     0.000000    0.000000             0.0           0.000000  0.000000   \n",
       "1     0.000000    0.000000             0.0           0.000000  0.000000   \n",
       "2     0.055556    0.000000             0.0           0.000000  0.055556   \n",
       "3     0.000000    0.000000             0.0           0.000000  0.000000   \n",
       "4     0.000000    0.000000             0.0           0.000000  0.125000   \n",
       "...        ...         ...             ...                ...       ...   \n",
       "3447  0.000000    0.000000             0.0           0.083333  0.000000   \n",
       "3448  0.000000    0.055556             0.0           0.000000  0.000000   \n",
       "3449  0.041667    0.000000             0.0           0.000000  0.000000   \n",
       "3450  0.000000    0.000000             0.0           0.050000  0.050000   \n",
       "3451  0.000000    0.000000             0.0           0.000000  0.000000   \n",
       "\n",
       "      apostrophe%  close parenthesis%  capitalized%  slang words%  \\\n",
       "0             0.0                 0.0      0.000000           0.0   \n",
       "1             0.0                 0.0      0.000000           0.0   \n",
       "2             0.0                 0.0      0.055556           0.0   \n",
       "3             0.0                 0.0      0.000000           0.0   \n",
       "4             0.0                 0.0      0.000000           0.0   \n",
       "...           ...                 ...           ...           ...   \n",
       "3447          0.0                 0.0      0.000000           0.0   \n",
       "3448          0.0                 0.0      0.000000           0.0   \n",
       "3449          0.0                 0.0      0.000000           0.0   \n",
       "3450          0.0                 0.0      0.850000           0.0   \n",
       "3451          0.0                 0.0      0.000000           0.0   \n",
       "\n",
       "      curse words%  with numericals%  \n",
       "0              0.0          0.000000  \n",
       "1              0.0          0.090909  \n",
       "2              0.0          0.000000  \n",
       "3              0.0          0.000000  \n",
       "4              0.0          0.000000  \n",
       "...            ...               ...  \n",
       "3447           0.0          0.000000  \n",
       "3448           0.0          0.000000  \n",
       "3449           0.0          0.000000  \n",
       "3450           0.0          0.000000  \n",
       "3451           0.0          0.000000  \n",
       "\n",
       "[3452 rows x 14 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['Statement'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Lalaki patay sa pamamaril sa Tondo, Maynila',\n",
       "       '50 Pinoy na naipit sa kaguluhan sa Sudan, nailikas na',\n",
       "       '#BoyingResign: Netizens galit kay DOJ Chief Remulla matapos mahulihan ng ‘high-grade’ marijuana ang anak',\n",
       "       ...,\n",
       "       'Pinoy Vlogger na tumulong kay Nas Daily noon, nagsalita na: Parang ginamit lang ako, naabuso. Hindi ko na sya kilala',\n",
       "       'GRABE! UNANG GABI NG LAMAY SA BUROL NI KRIS AQUINO DINAGSA NG MGA TAO, KAIBIGAN AT KAMAG-ANAK .',\n",
       "       ' Isang tattoo artist may walong asawa at magkakasamang naninirahan sa iisang bubong'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=clean_tokenize)\n",
    "tfidf_vecs = vectorizer.fit_transform(sentences).toarray()\n",
    "X = pd.concat([pd.DataFrame(tfidf_vecs), x_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>semicolon%</th>\n",
       "      <th>question mark%</th>\n",
       "      <th>exclamation mark%</th>\n",
       "      <th>dash%</th>\n",
       "      <th>apostrophe%</th>\n",
       "      <th>close parenthesis%</th>\n",
       "      <th>capitalized%</th>\n",
       "      <th>slang words%</th>\n",
       "      <th>curse words%</th>\n",
       "      <th>with numericals%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>0.130961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>0.141574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3452 rows × 10555 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1    2    3    4    5    6    7    8    9  ...  \\\n",
       "0     0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1     0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2     0.000000  0.270047  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3     0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4     0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "...        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "3447  0.130961  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3448  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3449  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3450  0.141574  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3451  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "      semicolon%  question mark%  exclamation mark%     dash%  apostrophe%  \\\n",
       "0       0.000000             0.0           0.000000  0.000000          0.0   \n",
       "1       0.000000             0.0           0.000000  0.000000          0.0   \n",
       "2       0.000000             0.0           0.000000  0.055556          0.0   \n",
       "3       0.000000             0.0           0.000000  0.000000          0.0   \n",
       "4       0.000000             0.0           0.000000  0.125000          0.0   \n",
       "...          ...             ...                ...       ...          ...   \n",
       "3447    0.000000             0.0           0.083333  0.000000          0.0   \n",
       "3448    0.055556             0.0           0.000000  0.000000          0.0   \n",
       "3449    0.000000             0.0           0.000000  0.000000          0.0   \n",
       "3450    0.000000             0.0           0.050000  0.050000          0.0   \n",
       "3451    0.000000             0.0           0.000000  0.000000          0.0   \n",
       "\n",
       "      close parenthesis%  capitalized%  slang words%  curse words%  \\\n",
       "0                    0.0      0.000000           0.0           0.0   \n",
       "1                    0.0      0.000000           0.0           0.0   \n",
       "2                    0.0      0.055556           0.0           0.0   \n",
       "3                    0.0      0.000000           0.0           0.0   \n",
       "4                    0.0      0.000000           0.0           0.0   \n",
       "...                  ...           ...           ...           ...   \n",
       "3447                 0.0      0.000000           0.0           0.0   \n",
       "3448                 0.0      0.000000           0.0           0.0   \n",
       "3449                 0.0      0.000000           0.0           0.0   \n",
       "3450                 0.0      0.850000           0.0           0.0   \n",
       "3451                 0.0      0.000000           0.0           0.0   \n",
       "\n",
       "      with numericals%  \n",
       "0             0.000000  \n",
       "1             0.090909  \n",
       "2             0.000000  \n",
       "3             0.000000  \n",
       "4             0.000000  \n",
       "...                ...  \n",
       "3447          0.000000  \n",
       "3448          0.000000  \n",
       "3449          0.000000  \n",
       "3450          0.000000  \n",
       "3451          0.000000  \n",
       "\n",
       "[3452 rows x 10555 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=clean_tokenize)\n",
    "tfidf_vecs = vectorizer.fit_transform(sentences).toarray()\n",
    "#X = pd.concat([pd.DataFrame(tfidf_vecs), x_df], axis = 1)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import joblib\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ct = ColumnTransformer([('standard_scaler', StandardScaler(), [col for col in x_df if col != 'Statement'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3452, 13)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_feat = ct.fit_transform(x_df)\n",
    "transformed_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3452, 10541)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3452, 10554)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.concatenate((tfidf_vecs, transformed_feat), axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score\n",
    "\n",
    "clfXGB = Pipeline(steps = [('XGBoost', XGBClassifier())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "clfSVM = Pipeline(steps = [('SVM', SVC())])\n",
    "#logistic regression\n",
    "clfLOGREG = Pipeline(steps = [('Logistic Regression', LogisticRegression())])\n",
    "#XGBoost\n",
    "clfXGB = Pipeline(steps = [('XGBoost', XGBClassifier())])\n",
    "#XGBoost with tuned params\n",
    "clfXGB_best =  Pipeline(steps = [('XGBoost_tuned', XGBClassifier(colsample_bytree= 0.5,\n",
    "                                                            gamma=0.3,\n",
    "                                                            learning_rate=0.1,\n",
    "                                                            max_depth=6,\n",
    "                                                            min_child_weight=1))])\n",
    "\n",
    "#Naive Bayes\n",
    "clfNB = Pipeline(steps = [('Naive Bayes', GaussianNB())])\n",
    "#Random forest\n",
    "clfRFC = Pipeline(steps = [('RFC', RandomForestClassifier())])\n",
    "#neural netTODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 done\n",
      "model 2 done\n",
      "model 3 done\n",
      "model 4 done\n",
      "model 5 done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score\n",
    "models = [clfSVM, clfLOGREG, clfXGB, clfNB, clfRFC]\n",
    "\n",
    "model_scores={}\n",
    "model_rand_acc = {}\n",
    "\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    model.fit(X_train, y_train)\n",
    "    name = list(model.named_steps)[0]\n",
    "    \n",
    "    score = model.score(X_test, y_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    \n",
    "    model_scores[name] = score, mae, mse, f1\n",
    "\n",
    "    \n",
    "\n",
    "    joblib.dump(model, f'headline_classifiers_tagalog/{name}Classifier.joblib')\n",
    "    print(f'model {idx+1} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SVM': (0.7916063675832128,\n",
       "  0.20839363241678727,\n",
       "  0.20839363241678727,\n",
       "  0.7900874635568512),\n",
       " 'Logistic Regression': (0.9015918958031838,\n",
       "  0.09840810419681621,\n",
       "  0.09840810419681621,\n",
       "  0.9022988505747127),\n",
       " 'XGBoost': (0.8784370477568741,\n",
       "  0.12156295224312591,\n",
       "  0.12156295224312591,\n",
       "  0.8789625360230547),\n",
       " 'Naive Bayes': (0.8480463096960926,\n",
       "  0.15195369030390737,\n",
       "  0.15195369030390737,\n",
       "  0.8416289592760181),\n",
       " 'RFC': (0.8798842257597684,\n",
       "  0.12011577424023155,\n",
       "  0.12011577424023155,\n",
       "  0.8815977175463625)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hasnt been run yet come back to this later pwease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding best params for LOGREG (it has the best performance out of all the tested ML models)   (ONLY TESTING THESE PARAMS AS THEY WERE DEEMED THE MOST IMPORTANT TO SAVE RESOURCE ALLOCATION)\n",
    "params={\n",
    " \"max_iter\" : [70, 90, 100, 500, 1000, 2000],\n",
    " \"C\"        : np.logspace(-4, 4, 50),\n",
    " \"penalty\"  : ['l1', 'l2', 'elasticnet']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_clfLOGREG = LogisticRegression()\n",
    "grid_search = GridSearchCV(\n",
    "    estimator= new_clfLOGREG,\n",
    "    param_grid=params,\n",
    "    scoring = 'roc_auc',\n",
    "    verbose=True,\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 900 candidates, totalling 4500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "3000 fits failed out of a total of 4500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1500 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1500 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.7180791         nan        nan 0.7180791         nan\n",
      "        nan 0.7180791         nan        nan 0.7180791         nan\n",
      "        nan 0.7180791         nan        nan 0.7180791         nan\n",
      "        nan 0.71879279        nan        nan 0.71879279        nan\n",
      "        nan 0.71879279        nan        nan 0.71879279        nan\n",
      "        nan 0.71879279        nan        nan 0.71879279        nan\n",
      "        nan 0.71947278        nan        nan 0.71947278        nan\n",
      "        nan 0.71947278        nan        nan 0.71947278        nan\n",
      "        nan 0.71947278        nan        nan 0.71947278        nan\n",
      "        nan 0.72063303        nan        nan 0.72063303        nan\n",
      "        nan 0.72063303        nan        nan 0.72063303        nan\n",
      "        nan 0.72063303        nan        nan 0.72063303        nan\n",
      "        nan 0.72188537        nan        nan 0.72188537        nan\n",
      "        nan 0.72188537        nan        nan 0.72188537        nan\n",
      "        nan 0.72188537        nan        nan 0.72188537        nan\n",
      "        nan 0.72366186        nan        nan 0.72366186        nan\n",
      "        nan 0.72366186        nan        nan 0.72366186        nan\n",
      "        nan 0.72366186        nan        nan 0.72366186        nan\n",
      "        nan 0.72516395        nan        nan 0.72516395        nan\n",
      "        nan 0.72516395        nan        nan 0.72516395        nan\n",
      "        nan 0.72516395        nan        nan 0.72516395        nan\n",
      "        nan 0.72740082        nan        nan 0.72740082        nan\n",
      "        nan 0.72740082        nan        nan 0.72740082        nan\n",
      "        nan 0.72740082        nan        nan 0.72740082        nan\n",
      "        nan 0.72991836        nan        nan 0.72991836        nan\n",
      "        nan 0.72991836        nan        nan 0.72991836        nan\n",
      "        nan 0.72991836        nan        nan 0.72991836        nan\n",
      "        nan 0.73301777        nan        nan 0.73301777        nan\n",
      "        nan 0.73301777        nan        nan 0.73301777        nan\n",
      "        nan 0.73301777        nan        nan 0.73301777        nan\n",
      "        nan 0.73719093        nan        nan 0.73719093        nan\n",
      "        nan 0.73719093        nan        nan 0.73719093        nan\n",
      "        nan 0.73719093        nan        nan 0.73719093        nan\n",
      "        nan 0.74263374        nan        nan 0.74263374        nan\n",
      "        nan 0.74263374        nan        nan 0.74263374        nan\n",
      "        nan 0.74263374        nan        nan 0.74263374        nan\n",
      "        nan 0.74922361        nan        nan 0.74922361        nan\n",
      "        nan 0.74922361        nan        nan 0.74922361        nan\n",
      "        nan 0.74922361        nan        nan 0.74922361        nan\n",
      "        nan 0.7569702         nan        nan 0.7569702         nan\n",
      "        nan 0.7569702         nan        nan 0.7569702         nan\n",
      "        nan 0.7569702         nan        nan 0.7569702         nan\n",
      "        nan 0.76634079        nan        nan 0.76634079        nan\n",
      "        nan 0.76634079        nan        nan 0.76634079        nan\n",
      "        nan 0.76634079        nan        nan 0.76634079        nan\n",
      "        nan 0.77787316        nan        nan 0.77787316        nan\n",
      "        nan 0.77787316        nan        nan 0.77787316        nan\n",
      "        nan 0.77787316        nan        nan 0.77787316        nan\n",
      "        nan 0.79254346        nan        nan 0.79254346        nan\n",
      "        nan 0.79254346        nan        nan 0.79254346        nan\n",
      "        nan 0.79254346        nan        nan 0.79254346        nan\n",
      "        nan 0.80943057        nan        nan 0.80943057        nan\n",
      "        nan 0.80943057        nan        nan 0.80943057        nan\n",
      "        nan 0.80943057        nan        nan 0.80943057        nan\n",
      "        nan 0.82913546        nan        nan 0.82913546        nan\n",
      "        nan 0.82913546        nan        nan 0.82913546        nan\n",
      "        nan 0.82913546        nan        nan 0.82913546        nan\n",
      "        nan 0.85073974        nan        nan 0.85073974        nan\n",
      "        nan 0.85073974        nan        nan 0.85073974        nan\n",
      "        nan 0.85073974        nan        nan 0.85073974        nan\n",
      "        nan 0.87253286        nan        nan 0.87253286        nan\n",
      "        nan 0.87253286        nan        nan 0.87253286        nan\n",
      "        nan 0.87253286        nan        nan 0.87253286        nan\n",
      "        nan 0.89360988        nan        nan 0.89360988        nan\n",
      "        nan 0.89360988        nan        nan 0.89360988        nan\n",
      "        nan 0.89360988        nan        nan 0.89360988        nan\n",
      "        nan 0.9118184         nan        nan 0.9118184         nan\n",
      "        nan 0.9118184         nan        nan 0.9118184         nan\n",
      "        nan 0.9118184         nan        nan 0.9118184         nan\n",
      "        nan 0.92713042        nan        nan 0.92713042        nan\n",
      "        nan 0.92713042        nan        nan 0.92713042        nan\n",
      "        nan 0.92713042        nan        nan 0.92713042        nan\n",
      "        nan 0.93897856        nan        nan 0.93898381        nan\n",
      "        nan 0.93898381        nan        nan 0.93898381        nan\n",
      "        nan 0.93898381        nan        nan 0.93898381        nan\n",
      "        nan 0.94808429        nan        nan 0.94808169        nan\n",
      "        nan 0.94808169        nan        nan 0.94808169        nan\n",
      "        nan 0.94808169        nan        nan 0.94808169        nan\n",
      "        nan 0.95482069        nan        nan 0.95483905        nan\n",
      "        nan 0.95483905        nan        nan 0.95483905        nan\n",
      "        nan 0.95483905        nan        nan 0.95483905        nan\n",
      "        nan 0.95976785        nan        nan 0.9597836         nan\n",
      "        nan 0.95978622        nan        nan 0.95978097        nan\n",
      "        nan 0.95978097        nan        nan 0.95978097        nan\n",
      "        nan 0.963175          nan        nan 0.96318554        nan\n",
      "        nan 0.96318554        nan        nan 0.96317503        nan\n",
      "        nan 0.96317503        nan        nan 0.96317503        nan\n",
      "        nan 0.96540988        nan        nan 0.96542561        nan\n",
      "        nan 0.96543611        nan        nan 0.96543349        nan\n",
      "        nan 0.96543349        nan        nan 0.96543349        nan\n",
      "        nan 0.96704921        nan        nan 0.96705426        nan\n",
      "        nan 0.96703594        nan        nan 0.96706215        nan\n",
      "        nan 0.96706215        nan        nan 0.96706215        nan\n",
      "        nan 0.96794864        nan        nan 0.96808751        nan\n",
      "        nan 0.96812694        nan        nan 0.96814265        nan\n",
      "        nan 0.96814265        nan        nan 0.96814265        nan\n",
      "        nan 0.96875412        nan        nan 0.96886397        nan\n",
      "        nan 0.96888241        nan        nan 0.96890075        nan\n",
      "        nan 0.96890075        nan        nan 0.96890075        nan\n",
      "        nan 0.96954893        nan        nan 0.96956703        nan\n",
      "        nan 0.96949612        nan        nan 0.96954346        nan\n",
      "        nan 0.96954346        nan        nan 0.96954346        nan\n",
      "        nan 0.96992676        nan        nan 0.97004455        nan\n",
      "        nan 0.96996325        nan        nan 0.97008129        nan\n",
      "        nan 0.97008129        nan        nan 0.97008129        nan\n",
      "        nan 0.96981381        nan        nan 0.97028596        nan\n",
      "        nan 0.97032787        nan        nan 0.97045908        nan\n",
      "        nan 0.97045908        nan        nan 0.97045908        nan\n",
      "        nan 0.97033609        nan        nan 0.97050371        nan\n",
      "        nan 0.97045625        nan        nan 0.97070829        nan\n",
      "        nan 0.97070829        nan        nan 0.97070829        nan\n",
      "        nan 0.97048534        nan        nan 0.97028902        nan\n",
      "        nan 0.97071893        nan        nan 0.97094174        nan\n",
      "        nan 0.97094174        nan        nan 0.97094174        nan\n",
      "        nan 0.97071915        nan        nan 0.9704961         nan\n",
      "        nan 0.97090008        nan        nan 0.97106242        nan\n",
      "        nan 0.97106242        nan        nan 0.97106242        nan\n",
      "        nan 0.97023314        nan        nan 0.97078432        nan\n",
      "        nan 0.97091841        nan        nan 0.97117784        nan\n",
      "        nan 0.97117784        nan        nan 0.97117784        nan\n",
      "        nan 0.97053812        nan        nan 0.97073195        nan\n",
      "        nan 0.97091532        nan        nan 0.97129591        nan\n",
      "        nan 0.97129591        nan        nan 0.97129591        nan\n",
      "        nan 0.97066883        nan        nan 0.97107578        nan\n",
      "        nan 0.97112009        nan        nan 0.97142183        nan\n",
      "        nan 0.97142183        nan        nan 0.97142183        nan\n",
      "        nan 0.97088932        nan        nan 0.97112823        nan\n",
      "        nan 0.97112016        nan        nan 0.97151626        nan\n",
      "        nan 0.97151626        nan        nan 0.97151626        nan\n",
      "        nan 0.97031688        nan        nan 0.97034329        nan\n",
      "        nan 0.97071578        nan        nan 0.97157911        nan\n",
      "        nan 0.97157911        nan        nan 0.97157911        nan\n",
      "        nan 0.97038059        nan        nan 0.97114689        nan\n",
      "        nan 0.9711174         nan        nan 0.97165252        nan\n",
      "        nan 0.97165252        nan        nan 0.97165252        nan\n",
      "        nan 0.97026473        nan        nan 0.97088179        nan\n",
      "        nan 0.97085231        nan        nan 0.97169183        nan\n",
      "        nan 0.97169183        nan        nan 0.97169183        nan\n",
      "        nan 0.97010273        nan        nan 0.97035651        nan\n",
      "        nan 0.97081555        nan        nan 0.97175217        nan\n",
      "        nan 0.97175217        nan        nan 0.97175217        nan\n",
      "        nan 0.97049086        nan        nan 0.97092081        nan\n",
      "        nan 0.97075037        nan        nan 0.97182301        nan\n",
      "        nan 0.97182301        nan        nan 0.97182301        nan\n",
      "        nan 0.97041979        nan        nan 0.97058535        nan\n",
      "        nan 0.97096337        nan        nan 0.97183094        nan\n",
      "        nan 0.97183094        nan        nan 0.97183094        nan\n",
      "        nan 0.97053186        nan        nan 0.97053265        nan\n",
      "        nan 0.9709704         nan        nan 0.97185455        nan\n",
      "        nan 0.97185455        nan        nan 0.97185455        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan 0.72708033        nan        nan 0.72708033        nan\n",
      "        nan 0.72708033        nan        nan 0.72708033        nan\n",
      "        nan 0.72708033        nan        nan 0.72708033        nan\n",
      "        nan 0.72772702        nan        nan 0.72772702        nan\n",
      "        nan 0.72772702        nan        nan 0.72772702        nan\n",
      "        nan 0.72772702        nan        nan 0.72772702        nan\n",
      "        nan 0.72856936        nan        nan 0.72856936        nan\n",
      "        nan 0.72856936        nan        nan 0.72856936        nan\n",
      "        nan 0.72856936        nan        nan 0.72856936        nan\n",
      "        nan 0.72969286        nan        nan 0.72969286        nan\n",
      "        nan 0.72969286        nan        nan 0.72969286        nan\n",
      "        nan 0.72969286        nan        nan 0.72969286        nan\n",
      "        nan 0.73120693        nan        nan 0.73120693        nan\n",
      "        nan 0.73120693        nan        nan 0.73120693        nan\n",
      "        nan 0.73120693        nan        nan 0.73120693        nan\n",
      "        nan 0.73308558        nan        nan 0.73308558        nan\n",
      "        nan 0.73308558        nan        nan 0.73308558        nan\n",
      "        nan 0.73308558        nan        nan 0.73308558        nan\n",
      "        nan 0.73552323        nan        nan 0.73552323        nan\n",
      "        nan 0.73552323        nan        nan 0.73552323        nan\n",
      "        nan 0.73552323        nan        nan 0.73552323        nan\n",
      "        nan 0.73843274        nan        nan 0.73843274        nan\n",
      "        nan 0.73843274        nan        nan 0.73843274        nan\n",
      "        nan 0.73843274        nan        nan 0.73843274        nan\n",
      "        nan 0.74185183        nan        nan 0.74185183        nan\n",
      "        nan 0.74185183        nan        nan 0.74185183        nan\n",
      "        nan 0.74185183        nan        nan 0.74185183        nan\n",
      "        nan 0.74607519        nan        nan 0.74607519        nan\n",
      "        nan 0.74607519        nan        nan 0.74607519        nan\n",
      "        nan 0.74607519        nan        nan 0.74607519        nan\n",
      "        nan 0.75127301        nan        nan 0.75127301        nan\n",
      "        nan 0.75127301        nan        nan 0.75127301        nan\n",
      "        nan 0.75127301        nan        nan 0.75127301        nan\n",
      "        nan 0.75779915        nan        nan 0.75779915        nan\n",
      "        nan 0.75779915        nan        nan 0.75779915        nan\n",
      "        nan 0.75779915        nan        nan 0.75779915        nan\n",
      "        nan 0.76571821        nan        nan 0.76571821        nan\n",
      "        nan 0.76571821        nan        nan 0.76571821        nan\n",
      "        nan 0.76571821        nan        nan 0.76571821        nan\n",
      "        nan 0.77519919        nan        nan 0.77519919        nan\n",
      "        nan 0.77519919        nan        nan 0.77519919        nan\n",
      "        nan 0.77519919        nan        nan 0.77519919        nan\n",
      "        nan 0.78675241        nan        nan 0.78675241        nan\n",
      "        nan 0.78675241        nan        nan 0.78675241        nan\n",
      "        nan 0.78675241        nan        nan 0.78675241        nan\n",
      "        nan 0.80124823        nan        nan 0.80124823        nan\n",
      "        nan 0.80124823        nan        nan 0.80124823        nan\n",
      "        nan 0.80124823        nan        nan 0.80124823        nan\n",
      "        nan 0.81927919        nan        nan 0.81927919        nan\n",
      "        nan 0.81927919        nan        nan 0.81927919        nan\n",
      "        nan 0.81927919        nan        nan 0.81927919        nan\n",
      "        nan 0.84048152        nan        nan 0.84048152        nan\n",
      "        nan 0.84048152        nan        nan 0.84048152        nan\n",
      "        nan 0.84048152        nan        nan 0.84048152        nan\n",
      "        nan 0.86439719        nan        nan 0.86439719        nan\n",
      "        nan 0.86439719        nan        nan 0.86439719        nan\n",
      "        nan 0.86439719        nan        nan 0.86439719        nan\n",
      "        nan 0.88961678        nan        nan 0.88961678        nan\n",
      "        nan 0.88961678        nan        nan 0.88961678        nan\n",
      "        nan 0.88961678        nan        nan 0.88961678        nan\n",
      "        nan 0.91475887        nan        nan 0.91475887        nan\n",
      "        nan 0.91475887        nan        nan 0.91475887        nan\n",
      "        nan 0.91475887        nan        nan 0.91475887        nan\n",
      "        nan 0.93787358        nan        nan 0.93787358        nan\n",
      "        nan 0.93787358        nan        nan 0.93787358        nan\n",
      "        nan 0.93787358        nan        nan 0.93787358        nan\n",
      "        nan 0.95686972        nan        nan 0.95686972        nan\n",
      "        nan 0.95686972        nan        nan 0.95686972        nan\n",
      "        nan 0.95686972        nan        nan 0.95686972        nan\n",
      "        nan 0.97097833        nan        nan 0.97097833        nan\n",
      "        nan 0.97097833        nan        nan 0.97097833        nan\n",
      "        nan 0.97097833        nan        nan 0.97097833        nan\n",
      "        nan 0.98120599        nan        nan 0.98120566        nan\n",
      "        nan 0.98120566        nan        nan 0.98120566        nan\n",
      "        nan 0.98120566        nan        nan 0.98120566        nan\n",
      "        nan 0.98871061        nan        nan 0.98871257        nan\n",
      "        nan 0.98871257        nan        nan 0.98871257        nan\n",
      "        nan 0.98871257        nan        nan 0.98871257        nan\n",
      "        nan 0.99396795        nan        nan 0.99397008        nan\n",
      "        nan 0.99396959        nan        nan 0.99396959        nan\n",
      "        nan 0.99396959        nan        nan 0.99396959        nan\n",
      "        nan 0.9975421         nan        nan 0.99754128        nan\n",
      "        nan 0.99754144        nan        nan 0.99754128        nan\n",
      "        nan 0.99754128        nan        nan 0.99754128        nan\n",
      "        nan 0.99941713        nan        nan 0.99942466        nan\n",
      "        nan 0.99942499        nan        nan 0.99942434        nan\n",
      "        nan 0.99942434        nan        nan 0.99942434        nan\n",
      "        nan 0.99995686        nan        nan 0.99995309        nan\n",
      "        nan 0.99995375        nan        nan 0.99995375        nan\n",
      "        nan 0.99995375        nan        nan 0.99995375        nan\n",
      "        nan 0.99999918        nan        nan 0.99999918        nan\n",
      "        nan 0.99999934        nan        nan 0.99999934        nan\n",
      "        nan 0.99999934        nan        nan 0.99999934        nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan\n",
      "        nan 1.                nan        nan 1.                nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('Logistic Regression',\n",
       "                 GridSearchCV(estimator=LogisticRegression(),\n",
       "                              param_grid={'C': array([1.00000000e-04, 1.45634848e-04, 2.12095089e-04, 3.08884360e-04,\n",
       "       4.49843267e-04, 6.55128557e-04, 9.54095476e-04, 1.38949549e-03,\n",
       "       2.02358965e-03, 2.94705170e-03, 4.29193426e-03, 6.25055193e-03,\n",
       "       9.10298178e-03, 1.32571137e-02, 1.93069773e-02, 2.81176870e-02,\n",
       "       4.094...\n",
       "       1.67683294e+01, 2.44205309e+01, 3.55648031e+01, 5.17947468e+01,\n",
       "       7.54312006e+01, 1.09854114e+02, 1.59985872e+02, 2.32995181e+02,\n",
       "       3.39322177e+02, 4.94171336e+02, 7.19685673e+02, 1.04811313e+03,\n",
       "       1.52641797e+03, 2.22299648e+03, 3.23745754e+03, 4.71486636e+03,\n",
       "       6.86648845e+03, 1.00000000e+04]),\n",
       "                                          'max_iter': [70, 90, 100, 500, 1000,\n",
       "                                                       2000],\n",
       "                                          'penalty': ['l1', 'l2',\n",
       "                                                      'elasticnet']},\n",
       "                              return_train_score=True, scoring='roc_auc',\n",
       "                              verbose=True))])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfLOGREG_gs = Pipeline(steps = [('Logistic Regression', grid_search)])\n",
    "clfLOGREG_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10000.0, 'max_iter': 500, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9718545526840459"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918958031837916 0.08104196816208394 0.08104196816208394 0.9166666666666667\n"
     ]
    }
   ],
   "source": [
    "model = grid_search.best_estimator_\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(score, mae, mse, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.12756796, 0.58702912, 0.06610842, 0.06880441, 0.29840722,\n",
       "        0.06456161, 0.06880665, 0.32025313, 0.06732206, 0.07543149,\n",
       "        0.31489344, 0.07200732, 0.07419448, 0.29589267, 0.06548305,\n",
       "        0.06402874, 0.34119015, 0.0682126 , 0.06641731, 0.324928  ,\n",
       "        0.06761961, 0.06887784, 0.27985516, 0.06469707, 0.06283755,\n",
       "        0.28194304, 0.06062675, 0.06652327, 0.27108541, 0.06353307,\n",
       "        0.0646246 , 0.2986505 , 0.06163993, 0.06302953, 0.26928458,\n",
       "        0.06263814, 0.0618413 , 0.27157493, 0.06134138, 0.06341414,\n",
       "        0.28949399, 0.06570091, 0.06143017, 0.28663802, 0.06780791,\n",
       "        0.0642312 , 0.30354776, 0.06177268, 0.06236215, 0.28756404,\n",
       "        0.06035275, 0.06322355, 0.27148414, 0.0612957 , 0.0624331 ,\n",
       "        0.28029075, 0.06578012, 0.06715703, 0.29620667, 0.05865016,\n",
       "        0.0644505 , 0.30967646, 0.08104901, 0.07082367, 0.29392362,\n",
       "        0.06829462, 0.06666646, 0.34819937, 0.06814733, 0.06802349,\n",
       "        0.28035951, 0.06087933, 0.06384401, 0.3077002 , 0.07806444,\n",
       "        0.0734138 , 0.31505947, 0.06243811, 0.06721444, 0.31373472,\n",
       "        0.05585599, 0.06284242, 0.27447872, 0.05695415, 0.06176758,\n",
       "        0.31055412, 0.05295243, 0.06323085, 0.27457318, 0.05446019,\n",
       "        0.06342626, 0.28167672, 0.05781684, 0.06874709, 0.3171524 ,\n",
       "        0.06184368, 0.06721773, 0.32144723, 0.05685558, 0.06226215,\n",
       "        0.2660481 , 0.05514202, 0.06234179, 0.32203345, 0.05973167,\n",
       "        0.06961632, 0.29460764, 0.05885415, 0.06961365, 0.26487837,\n",
       "        0.05729852, 0.06483808, 0.26319327, 0.06060891, 0.05666366,\n",
       "        0.29322729, 0.05822821, 0.06247191, 0.32675419, 0.06321197,\n",
       "        0.05921564, 0.27526431, 0.06959596, 0.06522608, 0.31316314,\n",
       "        0.07889199, 0.07240644, 0.25450802, 0.05597134, 0.06005406,\n",
       "        0.24314532, 0.05398984, 0.05544648, 0.27117085, 0.05766959,\n",
       "        0.06232967, 0.25278029, 0.05664034, 0.0562861 , 0.25379238,\n",
       "        0.05291648, 0.05883832, 0.26864662, 0.05763984, 0.05485444,\n",
       "        0.24281583, 0.05701547, 0.06345963, 0.24006386, 0.05405569,\n",
       "        0.05959148, 0.25850329, 0.0558557 , 0.06334963, 0.23906255,\n",
       "        0.05437288, 0.05457735, 0.24407649, 0.05312786, 0.06105533,\n",
       "        0.26071029, 0.05455918, 0.05698519, 0.25431466, 0.05855336,\n",
       "        0.05859833, 0.25760732, 0.05545278, 0.05304899, 0.25621028,\n",
       "        0.05366335, 0.05813894, 0.27585683, 0.05944176, 0.06882057,\n",
       "        0.26346312, 0.05516553, 0.05247903, 0.26859555, 0.0540823 ,\n",
       "        0.06016784, 0.27890868, 0.0567163 , 0.06672215, 0.25044184,\n",
       "        0.05495572, 0.05629678, 0.26947389, 0.05472355, 0.05997605,\n",
       "        0.30279136, 0.05945177, 0.06363368, 0.26523895, 0.05818853,\n",
       "        0.06017318, 0.26749806, 0.06197414, 0.06940222, 0.30044174,\n",
       "        0.05811486, 0.06454663, 0.29531908, 0.05737114, 0.06317267,\n",
       "        0.28549581, 0.05639873, 0.06702423, 0.30139771, 0.05445657,\n",
       "        0.06282992, 0.28425012, 0.0529417 , 0.06214643, 0.28203549,\n",
       "        0.05981064, 0.05744104, 0.31719398, 0.05993347, 0.06552157,\n",
       "        0.34031825, 0.08719006, 0.07313957, 0.38871179, 0.07078776,\n",
       "        0.06775866, 0.33163123, 0.07510982, 0.06901464, 0.30279784,\n",
       "        0.06140003, 0.06437869, 0.28703175, 0.08183994, 0.061307  ,\n",
       "        0.33191705, 0.07175536, 0.06378298, 0.43823285, 0.07312903,\n",
       "        0.08471055, 0.50261822, 0.08019266, 0.09883838, 0.5051054 ,\n",
       "        0.0779551 , 0.08497343, 0.3609159 , 0.05945339, 0.07241158,\n",
       "        0.39615488, 0.08267031, 0.06479177, 0.38841691, 0.05944457,\n",
       "        0.06958847, 0.41867571, 0.06034265, 0.06296096, 0.37619243,\n",
       "        0.0641006 , 0.06679654, 0.37838879, 0.05944676, 0.06571078,\n",
       "        0.39254494, 0.05918922, 0.06363988, 0.36852341, 0.05681992,\n",
       "        0.06893821, 0.42040067, 0.05545287, 0.06205359, 0.47665   ,\n",
       "        0.05809822, 0.06383924, 0.42129374, 0.05831151, 0.06588354,\n",
       "        0.42699013, 0.05823541, 0.06547861, 0.49127002, 0.05740371,\n",
       "        0.06079412, 0.42128139, 0.05896206, 0.0683217 , 0.46645761,\n",
       "        0.05643735, 0.06523409, 0.48272529, 0.05959139, 0.05974422,\n",
       "        0.50065818, 0.05604959, 0.06697145, 0.51342139, 0.05916138,\n",
       "        0.06189733, 0.49115548, 0.06102929, 0.06204233, 0.4596952 ,\n",
       "        0.05651107, 0.06562834, 0.52914219, 0.05635881, 0.06045389,\n",
       "        0.52965021, 0.05526719, 0.06423187, 0.57439513, 0.05714378,\n",
       "        0.0633718 , 0.53498149, 0.0573607 , 0.06687031, 0.5401474 ,\n",
       "        0.0579144 , 0.06078253, 0.57414484, 0.05851431, 0.05988002,\n",
       "        0.641044  , 0.05620666, 0.06612725, 0.6334259 , 0.05585561,\n",
       "        0.06672983, 0.68428345, 0.06727448, 0.05917339, 0.7632237 ,\n",
       "        0.07473435, 0.06059427, 0.62824574, 0.05709438, 0.06531186,\n",
       "        0.64444814, 0.05772867, 0.06622605, 0.86555848, 0.07886767,\n",
       "        0.06302342, 0.77248831, 0.06507921, 0.06710405, 0.88188939,\n",
       "        0.07104464, 0.08369021, 0.75648918, 0.07066088, 0.07220225,\n",
       "        0.92135286, 0.07393885, 0.06555371, 0.70547762, 0.06063886,\n",
       "        0.06722431, 0.90903654, 0.06535058, 0.06423912, 0.77348742,\n",
       "        0.06380277, 0.06833696, 0.91280317, 0.06155939, 0.06783395,\n",
       "        0.82586446, 0.06527996, 0.07033153, 0.80347977, 0.06822081,\n",
       "        0.07110043, 0.81174622, 0.06231942, 0.06374359, 0.90663972,\n",
       "        0.06239419, 0.06262078, 0.86731439, 0.06159005, 0.06319842,\n",
       "        0.86191826, 0.05859542, 0.06365867, 0.84490891, 0.0572166 ,\n",
       "        0.06438899, 0.842765  , 0.05706854, 0.06791368, 0.8366601 ,\n",
       "        0.06017895, 0.06214871, 1.00657663, 0.06085305, 0.06291442,\n",
       "        0.98758259, 0.05778933, 0.06088581, 0.93943481, 0.05920644,\n",
       "        0.06809015, 0.99058771, 0.06176295, 0.06956272, 1.17626624,\n",
       "        0.06865888, 0.07131562, 1.12686858, 0.06120143, 0.06804261,\n",
       "        1.11357436, 0.06741657, 0.06813741, 1.18505402, 0.06488886,\n",
       "        0.06597977, 1.15043449, 0.0614222 , 0.06406679, 1.18304706,\n",
       "        0.06104131, 0.06507144, 1.1391223 , 0.06001086, 0.06706638,\n",
       "        1.12866039, 0.0593874 , 0.0771832 , 1.33900294, 0.0611135 ,\n",
       "        0.06659455, 1.3391973 , 0.06220264, 0.06622663, 1.35269642,\n",
       "        0.06141262, 0.06414847, 1.3562427 , 0.06551666, 0.06887975,\n",
       "        1.35438995, 0.06309414, 0.06723833, 1.32235527, 0.06035829,\n",
       "        0.0679492 , 1.39675779, 0.06201172, 0.06832776, 1.51954336,\n",
       "        0.0628871 , 0.06355343, 1.484024  , 0.06640778, 0.06731367,\n",
       "        1.50135221, 0.0642097 , 0.06655717, 1.49474001, 0.06347785,\n",
       "        0.06699524, 1.50606408, 0.06318998, 0.06669741, 1.42457638,\n",
       "        0.06495099, 0.06806579, 1.72891273, 0.06671934, 0.0747189 ,\n",
       "        1.72664952, 0.06397481, 0.06655679, 1.73253546, 0.06967616,\n",
       "        0.06925368, 1.78015718, 0.06316643, 0.06629357, 1.73631992,\n",
       "        0.06349788, 0.06763549, 1.49893723, 0.05936441, 0.06563506,\n",
       "        1.83680539, 0.06022477, 0.06680737, 1.98352089, 0.06223521,\n",
       "        0.06870055, 2.00675983, 0.06931276, 0.07324038, 2.0364728 ,\n",
       "        0.06706438, 0.0699769 , 2.22129998, 0.06745672, 0.0714159 ,\n",
       "        1.44803185, 0.06727967, 0.07132626, 1.79274197, 0.06948848,\n",
       "        0.06854677, 2.01320019, 0.07111688, 0.07055445, 2.06880531,\n",
       "        0.06181312, 0.06805201, 2.08834171, 0.06320796, 0.06555443,\n",
       "        2.08102694, 0.09721699, 0.06884961, 1.54731045, 0.07091246,\n",
       "        0.07671885, 1.90410748, 0.06498165, 0.06718497, 1.97490129,\n",
       "        0.06139588, 0.06353726, 2.79428253, 0.10218468, 0.07470293,\n",
       "        2.45272193, 0.06160188, 0.06717086, 2.37701802, 0.0631999 ,\n",
       "        0.06881452, 1.45746808, 0.06132202, 0.06433964, 1.78244767,\n",
       "        0.05983419, 0.06353364, 2.0008348 , 0.06030564, 0.06383142,\n",
       "        2.58111291, 0.06291776, 0.06525421, 2.60043545, 0.06218996,\n",
       "        0.06181779, 2.63918371, 0.05941901, 0.06634412, 1.46972008,\n",
       "        0.06336107, 0.06580215, 1.81287117, 0.06349921, 0.06462517,\n",
       "        1.98090143, 0.06093144, 0.06496191, 2.90772195, 0.06174717,\n",
       "        0.06831231, 2.85663695, 0.06299176, 0.06761498, 2.85912929,\n",
       "        0.06061721, 0.06642685, 1.51547961, 0.07000489, 0.0658978 ,\n",
       "        1.89548426, 0.06713376, 0.07277112, 2.01325378, 0.0605916 ,\n",
       "        0.06629977, 3.21064115, 0.06417298, 0.06594105, 3.21846762,\n",
       "        0.06652837, 0.06393042, 3.21345282, 0.06366963, 0.06600823,\n",
       "        1.44214635, 0.06020575, 0.06937375, 1.75966134, 0.06052799,\n",
       "        0.07087641, 1.99638815, 0.05760674, 0.06203308, 3.52038231,\n",
       "        0.0631845 , 0.06406817, 3.45370054, 0.06161337, 0.06798706,\n",
       "        3.44626226, 0.06431193, 0.0673017 , 1.46387882, 0.06310377,\n",
       "        0.06434503, 1.7540998 , 0.06562977, 0.06663909, 2.02126389,\n",
       "        0.06656342, 0.06619639, 3.94053173, 0.06748581, 0.07084403,\n",
       "        3.92404065, 0.06814933, 0.06961136, 3.86824989, 0.0712029 ,\n",
       "        0.07410688, 1.45371614, 0.06478333, 0.06751661, 1.7400836 ,\n",
       "        0.06418715, 0.07272568, 1.99517641, 0.06898489, 0.07490277,\n",
       "        4.05801024, 0.0724751 , 0.07131968, 4.00675435, 0.06420026,\n",
       "        0.06825562, 4.01851096, 0.06243267, 0.06822586, 1.45320516,\n",
       "        0.06378274, 0.0673399 , 1.75569081, 0.06317434, 0.07389607,\n",
       "        1.96906853, 0.07440705, 0.08795652, 4.25559015, 0.06241994,\n",
       "        0.06462359, 4.32907839, 0.06100321, 0.0678371 , 4.24334669,\n",
       "        0.06229725, 0.06482463, 1.41585822, 0.06217985, 0.0656496 ,\n",
       "        1.83154006, 0.06508303, 0.06277018, 1.78650527, 0.05862889,\n",
       "        0.06264319, 4.15964022, 0.06099014, 0.06527777, 4.6301198 ,\n",
       "        0.06908479, 0.07001023, 4.1763885 , 0.05775728, 0.06264119,\n",
       "        1.36764326, 0.05920291, 0.06233425, 1.64983978, 0.05511651,\n",
       "        0.06812468, 1.82936082, 0.05724554, 0.0627562 , 4.85213065,\n",
       "        0.06753931, 0.07550941, 4.49602389, 0.06445947, 0.07707553,\n",
       "        4.49069986, 0.06596851, 0.07121539, 1.334794  , 0.05957966,\n",
       "        0.06384315, 1.65384789, 0.06112037, 0.06303816, 1.79095602,\n",
       "        0.06079435, 0.06223145, 4.66023951, 0.06380782, 0.06248446,\n",
       "        5.1466608 , 0.09674163, 0.07173696, 5.40254884, 0.09506125,\n",
       "        0.07979527, 1.48889737, 0.06102867, 0.07420554, 1.77717924,\n",
       "        0.05939078, 0.06396713, 1.76393099, 0.06218891, 0.06303387,\n",
       "        4.65981822, 0.05652652, 0.06294913, 4.67646408, 0.06332083,\n",
       "        0.06633401, 5.55461555, 0.0666882 , 0.06971703, 1.65841689,\n",
       "        0.07076859, 0.07480969, 1.86211047, 0.06289287, 0.06702189,\n",
       "        2.00665364, 0.07859054, 0.06842971, 6.27393756, 0.07492771,\n",
       "        0.07611022, 6.10230589, 0.06330976, 0.06817288, 6.07744694,\n",
       "        0.06671104, 0.07758169, 1.5131063 , 0.0644671 , 0.07082062,\n",
       "        1.7344173 , 0.06441917, 0.07250905, 2.21262674, 0.07362823,\n",
       "        0.06804953, 6.02816839, 0.06217566, 0.06648817, 5.90919976,\n",
       "        0.06217055, 0.06506329, 6.0618289 , 0.06334519, 0.07572312,\n",
       "        1.58903017, 0.07408094, 0.07126212, 1.85644145, 0.0690002 ,\n",
       "        0.08022008, 2.02192283, 0.06141806, 0.06839385, 6.18249135,\n",
       "        0.06332378, 0.06770706, 6.46781416, 0.06488872, 0.06417212,\n",
       "        6.46881633, 0.06070318, 0.06748648, 1.39304175, 0.06475663,\n",
       "        0.06930475, 1.70863843, 0.06449504, 0.06504469, 1.90295973,\n",
       "        0.0705389 , 0.07220802, 6.34385562, 0.060043  , 0.06779661,\n",
       "        6.14051967, 0.06571803, 0.07081065, 6.20321722, 0.06340914,\n",
       "        0.06854057, 1.44273973, 0.06307554, 0.06494312, 1.78196387,\n",
       "        0.06329918, 0.06802816, 1.9415216 , 0.0598537 , 0.0665535 ,\n",
       "        6.60691729, 0.06511517, 0.06621985, 6.70495009, 0.0618412 ,\n",
       "        0.07120337, 6.71385679, 0.06199331, 0.06443729, 1.46436543,\n",
       "        0.07337775, 0.06830435, 1.88886104, 0.07120972, 0.064432  ,\n",
       "        1.96876822, 0.06417856, 0.06747427, 6.4612957 , 0.06370773,\n",
       "        0.06892409, 6.4341064 , 0.05850205, 0.06662617, 5.99170227,\n",
       "        0.0604434 , 0.06322308, 1.4656179 , 0.06237011, 0.064042  ,\n",
       "        1.96402464, 0.06728873, 0.06862354, 2.23214593, 0.08510151,\n",
       "        0.06571636, 5.89081936, 0.07943211, 0.07120981, 6.02847295,\n",
       "        0.06382384, 0.0644413 , 5.79494505, 0.07780271, 0.0690156 ,\n",
       "        1.50736923, 0.0638247 , 0.08018541, 1.86154518, 0.06342983,\n",
       "        0.07041245, 2.00196204, 0.07519937, 0.08577652, 6.0016778 ,\n",
       "        0.06683121, 0.07799144, 6.51849041, 0.07011852, 0.07929001,\n",
       "        6.32272387, 0.07996922, 0.07656393, 1.56958585, 0.0825973 ,\n",
       "        0.0675972 , 1.64160581, 0.06094179, 0.06302838, 1.87419634,\n",
       "        0.06304703, 0.0660234 , 6.37890954, 0.06629462, 0.0690464 ,\n",
       "        6.09413905, 0.06219196, 0.06243162, 5.96560998, 0.0598043 ]),\n",
       " 'std_fit_time': array([1.09559083e-01, 5.24233079e-01, 2.37432899e-03, 1.09283893e-03,\n",
       "        9.14699310e-03, 5.67359667e-03, 2.35348847e-03, 3.01146324e-02,\n",
       "        5.74339506e-03, 1.05588620e-02, 4.84333229e-02, 3.36181486e-03,\n",
       "        9.99880370e-03, 2.52816448e-02, 3.05411826e-03, 3.98874354e-04,\n",
       "        7.58385749e-02, 2.78354829e-03, 2.23194723e-03, 4.33997733e-02,\n",
       "        1.71607220e-03, 6.52544771e-04, 1.32749200e-02, 4.31635930e-03,\n",
       "        1.21663873e-03, 7.72557176e-03, 4.68934032e-03, 5.26367273e-03,\n",
       "        3.76361574e-03, 1.65386837e-03, 4.47463740e-03, 5.59956801e-02,\n",
       "        2.55477660e-03, 1.32288659e-03, 2.88689668e-03, 3.88074196e-04,\n",
       "        1.22112146e-03, 2.44646290e-03, 3.04878657e-03, 4.80170411e-04,\n",
       "        3.34474709e-02, 2.16085288e-03, 4.43583295e-03, 2.47427008e-02,\n",
       "        4.42132060e-03, 4.83298475e-04, 4.87143057e-02, 1.50636971e-04,\n",
       "        4.58612792e-03, 3.64549968e-02, 4.51984309e-03, 1.01762740e-03,\n",
       "        5.65547761e-03, 8.00574903e-04, 2.94743542e-03, 7.08017534e-03,\n",
       "        3.46746703e-03, 2.61776695e-03, 2.71876674e-02, 4.52085971e-03,\n",
       "        4.22704978e-03, 1.30571630e-02, 1.78665593e-02, 2.95180342e-03,\n",
       "        1.14955275e-02, 4.31541059e-03, 2.55519628e-03, 4.65175765e-02,\n",
       "        2.89849601e-03, 8.96760057e-03, 7.35741769e-03, 9.11037590e-03,\n",
       "        8.83887426e-04, 1.39087383e-02, 1.56138687e-02, 7.18310104e-03,\n",
       "        3.90748901e-02, 5.22129806e-03, 7.32151822e-03, 2.84458490e-02,\n",
       "        4.24357689e-03, 2.76508353e-03, 6.48989241e-03, 4.05822781e-03,\n",
       "        7.63096499e-03, 5.85991100e-02, 2.59888743e-03, 1.35281847e-03,\n",
       "        8.99798066e-03, 4.01994190e-03, 3.91518210e-03, 1.72301597e-02,\n",
       "        3.64550921e-03, 7.69827294e-03, 3.24033789e-02, 6.50936209e-03,\n",
       "        4.53978678e-03, 4.09380534e-02, 4.19006197e-03, 8.09800468e-04,\n",
       "        1.36602661e-02, 3.94239967e-03, 5.64716123e-03, 4.97305428e-02,\n",
       "        4.27155232e-03, 7.98888114e-04, 2.37466055e-02, 4.58967344e-03,\n",
       "        2.54616496e-03, 1.18474176e-02, 3.71804721e-03, 1.97701198e-03,\n",
       "        7.80294866e-03, 1.35835919e-03, 2.31075157e-03, 5.16581311e-02,\n",
       "        3.03375052e-03, 7.10143884e-03, 5.41973294e-02, 4.32240907e-03,\n",
       "        6.10450898e-03, 9.09735369e-03, 1.39465202e-02, 1.04525053e-02,\n",
       "        4.65447960e-02, 6.82930994e-03, 4.90993718e-03, 1.68287245e-02,\n",
       "        3.36703403e-03, 3.64652316e-03, 4.69057835e-03, 2.75257977e-03,\n",
       "        5.79095132e-03, 3.39111012e-02, 5.34940810e-03, 7.15247226e-03,\n",
       "        2.20437462e-02, 4.66677153e-03, 5.73409539e-03, 1.40089319e-02,\n",
       "        1.70407215e-03, 4.76017011e-03, 5.52234849e-02, 6.58300356e-03,\n",
       "        5.42163692e-03, 5.43789040e-03, 4.23237778e-03, 3.67273061e-04,\n",
       "        3.54028517e-03, 2.70617596e-03, 4.80297362e-03, 3.69890503e-02,\n",
       "        6.14270610e-03, 3.42811272e-03, 4.39883968e-03, 3.96497478e-03,\n",
       "        5.30949645e-03, 5.52698961e-03, 2.78871699e-03, 4.60415314e-03,\n",
       "        3.87698252e-02, 3.70423991e-03, 6.11486970e-03, 3.39316156e-03,\n",
       "        7.93269448e-03, 7.02855170e-03, 6.12828004e-03, 1.84941678e-03,\n",
       "        3.95847397e-03, 4.01848507e-03, 2.48279160e-03, 7.34336224e-03,\n",
       "        4.17458291e-02, 7.31828655e-03, 8.96807479e-04, 1.19545679e-02,\n",
       "        1.80605490e-03, 3.22487797e-03, 1.34499039e-02, 2.95550485e-03,\n",
       "        5.23611818e-03, 5.41833640e-02, 5.99079186e-03, 3.93073763e-03,\n",
       "        6.06549508e-03, 2.75759808e-03, 4.82634784e-03, 1.17117164e-02,\n",
       "        3.50835952e-03, 6.83875658e-03, 3.98320212e-02, 3.12517824e-03,\n",
       "        5.26390833e-03, 8.10906095e-03, 3.07879042e-03, 6.50523497e-03,\n",
       "        7.86617312e-03, 4.62001131e-03, 1.03418828e-03, 3.71454333e-02,\n",
       "        1.45776367e-03, 7.34574512e-03, 1.71602090e-02, 2.72120139e-03,\n",
       "        7.98368483e-03, 3.06615072e-02, 3.13802174e-03, 6.89565596e-03,\n",
       "        3.64962480e-02, 2.15058974e-03, 4.49365023e-03, 2.90169460e-02,\n",
       "        2.82139906e-03, 8.98934639e-03, 8.67231365e-03, 9.64322279e-03,\n",
       "        5.17929115e-03, 3.63891405e-02, 4.44274045e-03, 6.12556653e-03,\n",
       "        1.62028470e-02, 2.00591781e-02, 1.42699103e-02, 1.02518212e-01,\n",
       "        8.93722582e-03, 6.52434055e-03, 5.73887036e-02, 2.51262562e-02,\n",
       "        1.23846118e-02, 5.41809601e-02, 5.84456538e-03, 5.39136940e-03,\n",
       "        8.16110542e-03, 1.30696367e-02, 5.04779753e-03, 1.25126201e-02,\n",
       "        5.90224938e-03, 8.84644171e-03, 6.89277558e-02, 5.01220223e-03,\n",
       "        6.38363018e-03, 1.09543209e-01, 8.95451197e-03, 2.13083640e-02,\n",
       "        1.06247828e-01, 8.88529418e-03, 1.62951362e-02, 4.47328840e-02,\n",
       "        3.55062171e-03, 9.90275628e-03, 4.57498004e-02, 1.07210913e-02,\n",
       "        7.20145699e-03, 1.43430122e-02, 2.99541121e-03, 4.73764387e-03,\n",
       "        3.58958833e-02, 4.27443600e-03, 4.80677646e-03, 1.49135741e-02,\n",
       "        5.61149616e-03, 7.94006672e-03, 1.63466300e-02, 3.20341063e-03,\n",
       "        4.64131868e-03, 5.60861641e-02, 3.77584705e-03, 4.00987785e-03,\n",
       "        1.17581623e-02, 3.36481721e-03, 3.97653267e-03, 1.59850853e-02,\n",
       "        2.77522283e-03, 5.46325167e-03, 6.20329153e-02, 4.35009612e-03,\n",
       "        5.01253553e-03, 8.95066255e-03, 2.87793320e-03, 3.95423948e-03,\n",
       "        6.40587203e-03, 4.70325724e-03, 6.10342124e-03, 1.25078139e-01,\n",
       "        2.75765509e-03, 6.06428109e-03, 5.83644152e-03, 2.99864265e-03,\n",
       "        3.23863715e-03, 2.48207993e-02, 3.98505682e-03, 5.22841863e-03,\n",
       "        4.68081786e-02, 7.59962135e-03, 5.34621356e-03, 1.87981707e-02,\n",
       "        2.39693529e-03, 5.92233751e-03, 8.18283536e-02, 2.43522032e-03,\n",
       "        6.74712165e-03, 3.19849056e-02, 6.23976321e-03, 6.14074870e-03,\n",
       "        2.81709196e-02, 2.15318017e-03, 4.36428898e-03, 2.98243956e-02,\n",
       "        2.69834482e-03, 6.38282702e-03, 3.37537012e-02, 2.73598516e-03,\n",
       "        4.81988920e-03, 7.17643608e-02, 2.29545202e-03, 5.48126510e-03,\n",
       "        3.59337263e-02, 2.14232048e-03, 6.01302817e-03, 2.84303135e-02,\n",
       "        2.49125344e-03, 4.70807687e-03, 6.72395260e-02, 4.17381652e-03,\n",
       "        5.63840989e-03, 3.55155797e-02, 2.31590371e-03, 5.82658001e-03,\n",
       "        4.20084613e-02, 3.16507935e-03, 6.65553156e-03, 8.61212187e-02,\n",
       "        7.56630970e-03, 7.89979276e-03, 8.99435667e-02, 9.63797726e-03,\n",
       "        4.55239100e-03, 3.73559732e-02, 4.04819131e-03, 4.29297553e-03,\n",
       "        4.74238417e-02, 3.61499328e-03, 4.30610180e-03, 9.57855322e-02,\n",
       "        1.16526464e-02, 4.74147357e-03, 6.77532261e-02, 4.66738390e-03,\n",
       "        4.39806701e-03, 8.52860140e-02, 2.02934319e-02, 9.74343040e-03,\n",
       "        5.46586023e-02, 3.72416041e-03, 5.55861548e-03, 1.80836035e-01,\n",
       "        1.42097595e-02, 4.10143667e-03, 7.10180849e-03, 4.82218054e-03,\n",
       "        4.19687165e-03, 6.88300516e-02, 1.25473252e-02, 6.22617885e-03,\n",
       "        2.31564247e-02, 5.80165882e-03, 4.90430786e-03, 5.15181080e-02,\n",
       "        6.24211477e-03, 1.26128531e-03, 7.56033103e-02, 2.68075504e-03,\n",
       "        1.04529601e-03, 3.69571186e-02, 2.91333264e-03, 2.95064343e-03,\n",
       "        1.83702679e-02, 1.39396080e-02, 1.05641250e-02, 1.42795074e-01,\n",
       "        4.71518265e-03, 1.64145477e-03, 1.26949584e-01, 4.10584018e-03,\n",
       "        1.73977545e-03, 8.26320223e-02, 5.13448755e-03, 9.86728654e-04,\n",
       "        7.45190483e-02, 4.11656969e-03, 7.83262621e-04, 8.48039286e-02,\n",
       "        3.85532515e-03, 3.43555524e-03, 2.99024039e-02, 3.42618999e-03,\n",
       "        2.76226468e-03, 4.86134958e-02, 4.85996998e-03, 4.06377649e-03,\n",
       "        4.29324448e-02, 8.24285143e-04, 5.45742325e-03, 2.00021959e-02,\n",
       "        4.16101336e-03, 4.39198957e-03, 4.54808963e-02, 6.09767028e-03,\n",
       "        1.59304757e-03, 1.40706924e-01, 8.73370106e-03, 5.15088684e-03,\n",
       "        1.10188504e-01, 4.64117017e-03, 2.83363690e-03, 1.17009331e-01,\n",
       "        5.76421566e-03, 3.06833215e-03, 1.32470977e-01, 3.63426917e-03,\n",
       "        4.26222681e-03, 6.81560813e-02, 4.05043470e-03, 3.06720187e-03,\n",
       "        1.31170504e-01, 3.35583086e-03, 4.09714605e-03, 8.96621793e-02,\n",
       "        4.24156486e-03, 1.91734519e-03, 8.94722785e-02, 3.78118063e-03,\n",
       "        1.21438949e-02, 5.52853500e-02, 5.06419252e-03, 1.97307438e-04,\n",
       "        9.02433633e-02, 6.27451658e-03, 3.69868885e-03, 1.12816036e-01,\n",
       "        3.79697386e-03, 2.71410742e-03, 9.06475752e-02, 7.05542036e-03,\n",
       "        9.34567568e-04, 9.41919337e-02, 6.06012931e-03, 1.29896043e-03,\n",
       "        8.25965759e-02, 4.60707484e-03, 7.87514541e-04, 3.06981128e-02,\n",
       "        4.86614780e-03, 3.25809600e-03, 1.63250554e-01, 4.30535047e-03,\n",
       "        4.78758554e-03, 1.14155462e-01, 8.29223089e-03, 9.98574805e-04,\n",
       "        1.73477687e-01, 5.57707106e-03, 2.42694446e-03, 1.10519813e-01,\n",
       "        5.24858336e-03, 1.09010955e-03, 1.32265929e-01, 3.16844904e-03,\n",
       "        1.17287105e-03, 5.70385206e-02, 5.13291997e-03, 1.22571856e-03,\n",
       "        5.75934439e-02, 3.57833150e-03, 1.00458507e-02, 6.91084905e-02,\n",
       "        5.53543653e-03, 2.39734129e-03, 6.60788730e-02, 8.33167836e-03,\n",
       "        9.63694651e-03, 1.14748733e-01, 4.45146727e-03, 2.81246998e-03,\n",
       "        6.05701593e-02, 5.38872801e-03, 7.36722320e-04, 6.79578824e-02,\n",
       "        3.13220232e-03, 2.62159076e-03, 7.43657859e-02, 3.46352549e-03,\n",
       "        8.85366096e-04, 9.21053516e-02, 2.97250236e-03, 1.99063517e-03,\n",
       "        8.95311222e-02, 6.67094975e-03, 1.25678809e-03, 1.05545000e-01,\n",
       "        5.59731722e-03, 4.75986751e-03, 1.80049391e-01, 4.03490998e-03,\n",
       "        1.35804572e-03, 3.05015804e-02, 5.19013965e-03, 7.87861877e-04,\n",
       "        4.50038743e-02, 5.37628567e-03, 6.37511139e-03, 8.38830623e-02,\n",
       "        4.47893548e-03, 7.28867338e-04, 1.56386948e-01, 5.98428481e-03,\n",
       "        7.31772291e-04, 1.55975611e-01, 5.50189295e-03, 4.54469332e-03,\n",
       "        1.23296059e-01, 3.78488091e-02, 3.07915371e-03, 8.09844182e-02,\n",
       "        2.01005011e-02, 1.27659053e-02, 8.58874141e-02, 8.91662550e-03,\n",
       "        1.17418071e-02, 7.24246593e-02, 5.82447311e-03, 5.03352843e-03,\n",
       "        2.43805924e-01, 3.45334315e-02, 1.48267291e-02, 1.57389109e-01,\n",
       "        5.83834556e-03, 1.93395388e-03, 1.25894911e-01, 6.06481383e-03,\n",
       "        1.29904359e-03, 8.94501225e-02, 6.12283700e-03, 5.51408460e-03,\n",
       "        6.40290903e-02, 2.97853463e-03, 6.16272196e-03, 8.83022846e-02,\n",
       "        4.49280288e-03, 5.07723610e-03, 1.40594302e-01, 3.84735660e-03,\n",
       "        4.78939558e-03, 1.91644521e-01, 5.15026163e-03, 4.86910760e-03,\n",
       "        1.72371246e-01, 4.17608477e-03, 6.32551739e-04, 7.30816978e-02,\n",
       "        5.55962828e-03, 4.50693962e-03, 8.31876462e-02, 5.63797076e-03,\n",
       "        5.62677077e-03, 7.69927698e-02, 2.83836340e-03, 5.93099885e-03,\n",
       "        1.29076219e-01, 4.91119017e-03, 3.16945786e-03, 1.22915549e-01,\n",
       "        5.55168840e-03, 6.75501072e-03, 1.84922646e-01, 4.64207999e-03,\n",
       "        5.72515415e-04, 1.43119821e-01, 2.91546163e-03, 3.96336500e-03,\n",
       "        1.38273908e-01, 5.55229501e-03, 5.54966971e-03, 9.41141357e-02,\n",
       "        3.53853659e-03, 1.00005803e-03, 1.45404445e-01, 5.00549090e-03,\n",
       "        4.02730266e-03, 1.21580414e-01, 7.81135528e-03, 7.52671211e-03,\n",
       "        1.04486171e-01, 5.91556368e-03, 3.09609676e-03, 8.22093921e-02,\n",
       "        3.17218266e-03, 5.76748952e-03, 6.10753441e-02, 3.25426162e-03,\n",
       "        4.93370879e-03, 1.01860598e-01, 2.65688851e-03, 3.99402533e-04,\n",
       "        2.61750555e-01, 4.80483321e-03, 4.35762431e-03, 2.18953435e-01,\n",
       "        4.65043529e-03, 1.07700722e-03, 2.32068455e-01, 5.81544871e-03,\n",
       "        1.04657638e-03, 1.08339948e-01, 7.93629520e-03, 2.80158757e-03,\n",
       "        5.48533162e-02, 4.36808455e-03, 4.53428999e-03, 9.84677617e-02,\n",
       "        4.54115230e-03, 1.82025809e-03, 4.27650036e-01, 5.91927683e-03,\n",
       "        8.43343149e-04, 4.27819448e-01, 5.27504185e-03, 2.44784627e-03,\n",
       "        3.76861117e-01, 4.14111521e-03, 6.10405864e-04, 6.44312562e-02,\n",
       "        3.92022572e-03, 6.28955431e-04, 5.94319209e-02, 6.07687884e-03,\n",
       "        4.98465324e-03, 1.12940485e-01, 5.65212689e-03, 1.12336203e-03,\n",
       "        2.59060035e-01, 4.79521390e-03, 5.89402794e-03, 2.88606568e-01,\n",
       "        5.34573805e-03, 2.95977061e-03, 3.08608104e-01, 4.39661050e-03,\n",
       "        1.84427044e-03, 8.79996299e-02, 5.06295218e-03, 2.86027568e-03,\n",
       "        6.25718516e-02, 4.79066091e-03, 1.49038496e-02, 5.51401195e-02,\n",
       "        4.21228057e-03, 1.07858975e-02, 1.93665653e-01, 6.21104927e-03,\n",
       "        4.62542865e-03, 1.77164311e-01, 6.20546211e-03, 2.01165151e-03,\n",
       "        1.59781909e-01, 5.88440406e-03, 4.32624183e-03, 6.01176372e-02,\n",
       "        8.19829105e-03, 1.13122797e-02, 2.09367237e-01, 5.92804574e-03,\n",
       "        1.15517297e-03, 5.51569939e-02, 4.47867835e-03, 1.70468566e-03,\n",
       "        3.32213470e-01, 5.18672312e-03, 7.85950446e-03, 5.88600589e-01,\n",
       "        1.11929702e-03, 7.55325733e-04, 4.31646679e-01, 3.86785139e-03,\n",
       "        9.75016333e-04, 1.23985394e-01, 5.42884049e-03, 1.78774992e-03,\n",
       "        4.02246100e-02, 1.11333615e-03, 2.34942734e-03, 4.93046191e-02,\n",
       "        2.98006598e-03, 1.08541522e-03, 2.73960624e-01, 5.89589627e-03,\n",
       "        3.06527362e-03, 2.60334679e-01, 4.76575090e-03, 1.23187017e-02,\n",
       "        2.70405775e-01, 6.53428689e-03, 3.30370497e-03, 6.47296083e-02,\n",
       "        4.20654185e-03, 4.03623794e-03, 6.00844489e-02, 2.91801662e-03,\n",
       "        2.29233727e-03, 3.20388031e-02, 5.25201941e-03, 1.00462930e-03,\n",
       "        2.69607559e-01, 5.72601368e-03, 1.69224729e-03, 4.59037612e-01,\n",
       "        4.10148207e-02, 2.24803773e-03, 5.88560553e-01, 2.91543022e-02,\n",
       "        3.92010153e-03, 1.32785392e-01, 6.00574967e-03, 1.50700413e-02,\n",
       "        1.11241793e-01, 2.93641880e-03, 1.47966866e-03, 1.91977353e-02,\n",
       "        7.37739970e-03, 7.60116681e-04, 2.88830690e-01, 3.75982515e-03,\n",
       "        4.70918506e-04, 3.72370842e-01, 4.79480557e-03, 3.70769740e-03,\n",
       "        1.01060086e+00, 5.03945804e-03, 2.47445983e-03, 2.74940583e-01,\n",
       "        5.05334834e-03, 6.72851323e-03, 1.81484863e-01, 4.52916771e-03,\n",
       "        2.01828475e-03, 1.27585902e-01, 5.54967299e-03, 5.96284843e-03,\n",
       "        6.24107113e-01, 6.19167390e-03, 8.15977155e-03, 6.89371359e-01,\n",
       "        6.94451036e-03, 1.98365839e-03, 8.25493604e-01, 1.03206094e-02,\n",
       "        3.80320272e-03, 1.59142230e-01, 6.82538880e-03, 2.08831533e-03,\n",
       "        6.28904751e-02, 5.76221869e-03, 9.61389585e-03, 2.37566720e-01,\n",
       "        8.76866926e-03, 3.67227978e-03, 4.06838850e-01, 5.50060191e-03,\n",
       "        1.28699971e-03, 4.13742130e-01, 9.28660378e-03, 3.65842147e-03,\n",
       "        2.59078014e-01, 4.64253442e-03, 3.26385312e-03, 1.78418904e-01,\n",
       "        2.29590297e-03, 3.90493977e-03, 1.03151053e-01, 5.75667960e-03,\n",
       "        1.07099120e-02, 1.21135220e-01, 5.41832563e-03, 1.46672718e-03,\n",
       "        2.06759557e-01, 4.33149962e-03, 1.56332177e-03, 6.32481590e-01,\n",
       "        6.19788021e-03, 6.81432288e-03, 4.37536475e-01, 5.89991123e-03,\n",
       "        1.06324429e-03, 7.54240450e-02, 8.62908838e-03, 1.31148513e-03,\n",
       "        8.09492312e-02, 5.10355067e-03, 2.62232774e-03, 6.87857372e-02,\n",
       "        6.56466037e-03, 3.44633820e-03, 3.96657496e-01, 4.24392582e-03,\n",
       "        1.25208945e-03, 5.66794611e-01, 6.85587394e-03, 1.41788907e-03,\n",
       "        3.31741795e-01, 5.66697018e-03, 1.36380945e-03, 4.85201928e-02,\n",
       "        6.92664966e-03, 2.38755445e-03, 9.34111482e-02, 5.43126258e-03,\n",
       "        1.32137273e-03, 6.82175779e-02, 3.47119709e-03, 8.27146791e-04,\n",
       "        2.69011378e-01, 4.11121283e-03, 7.49286522e-04, 4.77771943e-01,\n",
       "        6.07903951e-03, 6.89720625e-03, 4.69050847e-01, 4.00931591e-03,\n",
       "        5.67860671e-03, 1.18090307e-01, 1.00637526e-02, 8.83743062e-04,\n",
       "        8.11015616e-02, 5.66300221e-03, 5.51835277e-03, 8.64615473e-02,\n",
       "        6.08411828e-03, 2.91474326e-03, 6.36828865e-01, 4.57166908e-03,\n",
       "        1.16188224e-03, 5.46100809e-01, 4.40944616e-03, 4.01066671e-04,\n",
       "        5.04698546e-01, 4.99197696e-03, 4.91581332e-04, 8.88080472e-02,\n",
       "        6.09920615e-03, 4.69333976e-03, 5.19024767e-02, 1.50164249e-03,\n",
       "        3.38564026e-03, 3.18128178e-01, 3.60466476e-02, 2.13336753e-03,\n",
       "        3.55810661e-01, 2.17018322e-02, 2.05355799e-03, 5.72776223e-01,\n",
       "        4.32565554e-03, 1.02233614e-03, 3.63386565e-01, 3.02487852e-02,\n",
       "        1.81214834e-03, 1.71084923e-01, 3.98710837e-03, 1.38051046e-02,\n",
       "        1.34003392e-01, 5.57742194e-03, 1.84915451e-03, 1.83537257e-01,\n",
       "        7.37196187e-03, 2.91965490e-02, 5.19581350e-01, 8.00928501e-03,\n",
       "        1.70120277e-02, 5.50616362e-01, 5.56209551e-03, 5.98595235e-03,\n",
       "        4.98746388e-01, 1.00647984e-02, 4.53267922e-03, 8.29542781e-02,\n",
       "        1.27482973e-02, 4.00089419e-03, 4.08855430e-02, 5.05181710e-03,\n",
       "        1.92095173e-03, 9.66591389e-02, 4.82936899e-03, 1.45295615e-03,\n",
       "        4.57203612e-01, 2.39362341e-03, 1.32383553e-03, 5.20447089e-01,\n",
       "        4.77070143e-03, 3.37136131e-03, 4.01203716e-01, 5.26340654e-03]),\n",
       " 'mean_score_time': array([0.        , 0.01427183, 0.        , 0.        , 0.01057134,\n",
       "        0.        , 0.        , 0.0109704 , 0.        , 0.        ,\n",
       "        0.01296506, 0.        , 0.        , 0.01098619, 0.        ,\n",
       "        0.        , 0.01077127, 0.        , 0.        , 0.01037674,\n",
       "        0.        , 0.        , 0.00977373, 0.        , 0.        ,\n",
       "        0.00997338, 0.        , 0.        , 0.00957465, 0.        ,\n",
       "        0.        , 0.00997343, 0.        , 0.        , 0.00937467,\n",
       "        0.        , 0.        , 0.01017261, 0.        , 0.        ,\n",
       "        0.00957446, 0.        , 0.        , 0.0103723 , 0.        ,\n",
       "        0.        , 0.01057177, 0.        , 0.        , 0.00937514,\n",
       "        0.        , 0.        , 0.00977387, 0.        , 0.        ,\n",
       "        0.00937901, 0.        , 0.        , 0.00977416, 0.        ,\n",
       "        0.        , 0.01037226, 0.        , 0.        , 0.01018033,\n",
       "        0.        , 0.        , 0.01077108, 0.        , 0.        ,\n",
       "        0.01017261, 0.        , 0.        , 0.01117053, 0.        ,\n",
       "        0.        , 0.01077132, 0.        , 0.        , 0.01116996,\n",
       "        0.        , 0.        , 0.00997677, 0.        , 0.        ,\n",
       "        0.00977387, 0.        , 0.        , 0.00967526, 0.        ,\n",
       "        0.        , 0.00997682, 0.        , 0.        , 0.01077113,\n",
       "        0.        , 0.        , 0.01196795, 0.        , 0.        ,\n",
       "        0.00928016, 0.        , 0.        , 0.01037254, 0.        ,\n",
       "        0.        , 0.01087303, 0.        , 0.        , 0.00998998,\n",
       "        0.        , 0.        , 0.00977392, 0.        , 0.        ,\n",
       "        0.010392  , 0.        , 0.        , 0.01138291, 0.        ,\n",
       "        0.        , 0.00977364, 0.        , 0.        , 0.01136923,\n",
       "        0.        , 0.        , 0.01018443, 0.        , 0.        ,\n",
       "        0.00997348, 0.        , 0.        , 0.00978575, 0.        ,\n",
       "        0.        , 0.00942039, 0.        , 0.        , 0.01037116,\n",
       "        0.        , 0.        , 0.00987515, 0.        , 0.        ,\n",
       "        0.00946794, 0.        , 0.        , 0.00977359, 0.        ,\n",
       "        0.        , 0.01017265, 0.        , 0.        , 0.01027498,\n",
       "        0.        , 0.        , 0.00938416, 0.        , 0.        ,\n",
       "        0.00957422, 0.        , 0.        , 0.00977378, 0.        ,\n",
       "        0.        , 0.00957432, 0.        , 0.        , 0.00977383,\n",
       "        0.        , 0.        , 0.009375  , 0.        , 0.        ,\n",
       "        0.00959539, 0.        , 0.        , 0.0093781 , 0.        ,\n",
       "        0.        , 0.01017604, 0.        , 0.        , 0.00957432,\n",
       "        0.        , 0.        , 0.00978003, 0.        , 0.        ,\n",
       "        0.01276517, 0.        , 0.        , 0.00977383, 0.        ,\n",
       "        0.        , 0.01037216, 0.        , 0.        , 0.0103723 ,\n",
       "        0.        , 0.        , 0.00997314, 0.        , 0.        ,\n",
       "        0.00988693, 0.        , 0.        , 0.00977392, 0.        ,\n",
       "        0.        , 0.00997338, 0.        , 0.        , 0.01017933,\n",
       "        0.        , 0.        , 0.00937471, 0.        , 0.        ,\n",
       "        0.01057625, 0.        , 0.        , 0.01138201, 0.        ,\n",
       "        0.        , 0.01017227, 0.        , 0.        , 0.01097021,\n",
       "        0.        , 0.        , 0.00997329, 0.        , 0.        ,\n",
       "        0.00997343, 0.        , 0.        , 0.01117086, 0.        ,\n",
       "        0.        , 0.01317821, 0.        , 0.        , 0.01197214,\n",
       "        0.        , 0.        , 0.01007419, 0.        , 0.        ,\n",
       "        0.01386452, 0.        , 0.        , 0.01077971, 0.        ,\n",
       "        0.        , 0.01097054, 0.        , 0.        , 0.00997815,\n",
       "        0.        , 0.        , 0.0103725 , 0.        , 0.        ,\n",
       "        0.01057158, 0.        , 0.        , 0.01017237, 0.        ,\n",
       "        0.        , 0.00998006, 0.        , 0.        , 0.01038246,\n",
       "        0.        , 0.        , 0.00977378, 0.        , 0.        ,\n",
       "        0.00997357, 0.        , 0.        , 0.01037207, 0.        ,\n",
       "        0.        , 0.01017294, 0.        , 0.        , 0.0103723 ,\n",
       "        0.        , 0.        , 0.01036954, 0.        , 0.        ,\n",
       "        0.01108108, 0.        , 0.        , 0.01187153, 0.        ,\n",
       "        0.        , 0.01017323, 0.        , 0.        , 0.01077151,\n",
       "        0.        , 0.        , 0.01038361, 0.        , 0.        ,\n",
       "        0.01077991, 0.        , 0.        , 0.01057925, 0.        ,\n",
       "        0.        , 0.01017957, 0.        , 0.        , 0.0110774 ,\n",
       "        0.        , 0.        , 0.01048555, 0.        , 0.        ,\n",
       "        0.01057172, 0.        , 0.        , 0.010182  , 0.        ,\n",
       "        0.        , 0.01695275, 0.        , 0.        , 0.01536613,\n",
       "        0.        , 0.        , 0.01060019, 0.        , 0.        ,\n",
       "        0.01057196, 0.        , 0.        , 0.01506109, 0.        ,\n",
       "        0.        , 0.01097841, 0.        , 0.        , 0.01316438,\n",
       "        0.        , 0.        , 0.01038094, 0.        , 0.        ,\n",
       "        0.01398368, 0.        , 0.        , 0.00957437, 0.        ,\n",
       "        0.        , 0.01396298, 0.        , 0.        , 0.00987945,\n",
       "        0.        , 0.        , 0.00978107, 0.        , 0.        ,\n",
       "        0.00958443, 0.        , 0.        , 0.01017261, 0.        ,\n",
       "        0.        , 0.01018744, 0.        , 0.        , 0.01018882,\n",
       "        0.        , 0.        , 0.01038327, 0.        , 0.        ,\n",
       "        0.00937495, 0.        , 0.        , 0.01018009, 0.        ,\n",
       "        0.        , 0.00976644, 0.        , 0.        , 0.01017599,\n",
       "        0.        , 0.        , 0.01018081, 0.        , 0.        ,\n",
       "        0.00978746, 0.        , 0.        , 0.00959296, 0.        ,\n",
       "        0.        , 0.01007786, 0.        , 0.        , 0.01098723,\n",
       "        0.        , 0.        , 0.01019039, 0.        , 0.        ,\n",
       "        0.00987492, 0.        , 0.        , 0.01077738, 0.        ,\n",
       "        0.        , 0.01078806, 0.        , 0.        , 0.01017241,\n",
       "        0.        , 0.        , 0.01068044, 0.        , 0.        ,\n",
       "        0.01048613, 0.        , 0.        , 0.00998487, 0.        ,\n",
       "        0.        , 0.01037879, 0.        , 0.        , 0.01037855,\n",
       "        0.        , 0.        , 0.01038384, 0.        , 0.        ,\n",
       "        0.01078634, 0.        , 0.        , 0.01038494, 0.        ,\n",
       "        0.        , 0.01038795, 0.        , 0.        , 0.01078296,\n",
       "        0.        , 0.        , 0.01079159, 0.        , 0.        ,\n",
       "        0.01069245, 0.        , 0.        , 0.01019578, 0.        ,\n",
       "        0.        , 0.01098285, 0.        , 0.        , 0.01038551,\n",
       "        0.        , 0.        , 0.01038971, 0.        , 0.        ,\n",
       "        0.01008854, 0.        , 0.        , 0.01021485, 0.        ,\n",
       "        0.        , 0.01039915, 0.        , 0.        , 0.01019001,\n",
       "        0.        , 0.        , 0.01057744, 0.        , 0.        ,\n",
       "        0.01068091, 0.        , 0.        , 0.01088834, 0.        ,\n",
       "        0.        , 0.01008215, 0.        , 0.        , 0.01069555,\n",
       "        0.        , 0.        , 0.0187849 , 0.        , 0.        ,\n",
       "        0.00977702, 0.        , 0.        , 0.01028805, 0.        ,\n",
       "        0.        , 0.01058683, 0.        , 0.        , 0.01078763,\n",
       "        0.        , 0.        , 0.00999198, 0.        , 0.        ,\n",
       "        0.01018505, 0.        , 0.        , 0.01019487, 0.        ,\n",
       "        0.        , 0.01078386, 0.        , 0.        , 0.00999117,\n",
       "        0.        , 0.        , 0.011976  , 0.        , 0.        ,\n",
       "        0.00969582, 0.        , 0.        , 0.00969152, 0.        ,\n",
       "        0.        , 0.01017275, 0.        , 0.        , 0.01018806,\n",
       "        0.        , 0.        , 0.01039367, 0.        , 0.        ,\n",
       "        0.0106802 , 0.        , 0.        , 0.00968575, 0.        ,\n",
       "        0.        , 0.01018968, 0.        , 0.        , 0.0105845 ,\n",
       "        0.        , 0.        , 0.0103744 , 0.        , 0.        ,\n",
       "        0.01028538, 0.        , 0.        , 0.00999122, 0.        ,\n",
       "        0.        , 0.00969787, 0.        , 0.        , 0.00968604,\n",
       "        0.        , 0.        , 0.01078978, 0.        , 0.        ,\n",
       "        0.01038523, 0.        , 0.        , 0.0101809 , 0.        ,\n",
       "        0.        , 0.00999413, 0.        , 0.        , 0.00998802,\n",
       "        0.        , 0.        , 0.01059155, 0.        , 0.        ,\n",
       "        0.00979042, 0.        , 0.        , 0.0103889 , 0.        ,\n",
       "        0.        , 0.00978913, 0.        , 0.        , 0.01018858,\n",
       "        0.        , 0.        , 0.01029372, 0.        , 0.        ,\n",
       "        0.0099997 , 0.        , 0.        , 0.01038494, 0.        ,\n",
       "        0.        , 0.00979037, 0.        , 0.        , 0.01069398,\n",
       "        0.        , 0.        , 0.01078305, 0.        , 0.        ,\n",
       "        0.01018677, 0.        , 0.        , 0.01018186, 0.        ,\n",
       "        0.        , 0.01079116, 0.        , 0.        , 0.01059389,\n",
       "        0.        , 0.        , 0.01019921, 0.        , 0.        ,\n",
       "        0.00989051, 0.        , 0.        , 0.0107841 , 0.        ,\n",
       "        0.        , 0.00997324, 0.        , 0.        , 0.00998502,\n",
       "        0.        , 0.        , 0.01029172, 0.        , 0.        ,\n",
       "        0.01048274, 0.        , 0.        , 0.00979285, 0.        ,\n",
       "        0.        , 0.00998898, 0.        , 0.        , 0.00979028,\n",
       "        0.        , 0.        , 0.010182  , 0.        , 0.        ,\n",
       "        0.01078854, 0.        , 0.        , 0.00978637, 0.        ,\n",
       "        0.        , 0.00999522, 0.        , 0.        , 0.01019449,\n",
       "        0.        , 0.        , 0.00967541, 0.        , 0.        ,\n",
       "        0.00978389, 0.        , 0.        , 0.00959959, 0.        ,\n",
       "        0.        , 0.00978127, 0.        , 0.        , 0.01018806,\n",
       "        0.        , 0.        , 0.00939178, 0.        , 0.        ,\n",
       "        0.00958958, 0.        , 0.        , 0.00959654, 0.        ,\n",
       "        0.        , 0.00928617, 0.        , 0.        , 0.00958443,\n",
       "        0.        , 0.        , 0.0093977 , 0.        , 0.        ,\n",
       "        0.0103837 , 0.        , 0.        , 0.01028881, 0.        ,\n",
       "        0.        , 0.00938649, 0.        , 0.        , 0.01038857,\n",
       "        0.        , 0.        , 0.00919237, 0.        , 0.        ,\n",
       "        0.00989785, 0.        , 0.        , 0.00959606, 0.        ,\n",
       "        0.        , 0.01059823, 0.        , 0.        , 0.01108718,\n",
       "        0.        , 0.        , 0.00958662, 0.        , 0.        ,\n",
       "        0.01359186, 0.        , 0.        , 0.01059504, 0.        ,\n",
       "        0.        , 0.00998507, 0.        , 0.        , 0.01058621,\n",
       "        0.        , 0.        , 0.00998745, 0.        , 0.        ,\n",
       "        0.01018906, 0.        , 0.        , 0.01077094, 0.        ,\n",
       "        0.        , 0.00989628, 0.        , 0.        , 0.01019983,\n",
       "        0.        , 0.        , 0.01089673, 0.        , 0.        ,\n",
       "        0.01059694, 0.        , 0.        , 0.01068664, 0.        ,\n",
       "        0.        , 0.01078005, 0.        , 0.        , 0.01019135,\n",
       "        0.        , 0.        , 0.01119351, 0.        , 0.        ,\n",
       "        0.01039228, 0.        , 0.        , 0.01416597, 0.        ,\n",
       "        0.        , 0.00978646, 0.        , 0.        , 0.01040359,\n",
       "        0.        , 0.        , 0.01018977, 0.        , 0.        ,\n",
       "        0.0110939 , 0.        , 0.        , 0.01097817, 0.        ,\n",
       "        0.        , 0.00979042, 0.        , 0.        , 0.01018643,\n",
       "        0.        , 0.        , 0.01038537, 0.        , 0.        ,\n",
       "        0.0103899 , 0.        , 0.        , 0.00957437, 0.        ,\n",
       "        0.        , 0.01167936, 0.        , 0.        , 0.01038399,\n",
       "        0.        , 0.        , 0.01018014, 0.        , 0.        ,\n",
       "        0.01058469, 0.        , 0.        , 0.01049609, 0.        ,\n",
       "        0.        , 0.01059084, 0.        , 0.        , 0.00959296,\n",
       "        0.        , 0.        , 0.0103723 , 0.        , 0.        ,\n",
       "        0.01028976, 0.        , 0.        , 0.01079292, 0.        ,\n",
       "        0.        , 0.01028552, 0.        , 0.        , 0.009375  ,\n",
       "        0.        , 0.        , 0.00977397, 0.        , 0.        ,\n",
       "        0.01017232, 0.        , 0.        , 0.01057138, 0.        ,\n",
       "        0.        , 0.00977387, 0.        , 0.        , 0.01017265,\n",
       "        0.        , 0.        , 0.01019301, 0.        , 0.        ,\n",
       "        0.01089492, 0.        , 0.        , 0.01178336, 0.        ,\n",
       "        0.        , 0.00938282, 0.        , 0.        , 0.00977349,\n",
       "        0.        , 0.        , 0.01496029, 0.        , 0.        ,\n",
       "        0.01018596, 0.        , 0.        , 0.00988631, 0.        ]),\n",
       " 'std_score_time': array([0.00000000e+00, 5.67298488e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.88986511e-04, 0.00000000e+00, 0.00000000e+00, 6.30901916e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.49895011e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 8.92695498e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.77427839e-04, 0.00000000e+00, 0.00000000e+00, 8.01104710e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.46352978e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.30223292e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.88324624e-04, 0.00000000e+00, 0.00000000e+00, 3.56832255e-07,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.88869803e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.46098143e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.88558179e-04, 0.00000000e+00, 0.00000000e+00, 1.01692454e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.35308441e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.88577726e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.98945905e-04, 0.00000000e+00, 0.00000000e+00, 4.93145847e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.16314553e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.88830761e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.95342366e-04, 0.00000000e+00, 0.00000000e+00, 7.46556902e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.98397717e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.93360022e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.46110934e-04, 0.00000000e+00, 0.00000000e+00, 1.46587567e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.30790764e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.99423026e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.98344522e-04, 0.00000000e+00, 0.00000000e+00, 1.25846534e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.46952016e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.02518002e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.97375226e-04, 0.00000000e+00, 0.00000000e+00, 7.98273336e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.91440470e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.36215464e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.46531379e-04, 0.00000000e+00, 0.00000000e+00, 4.90535329e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.10444119e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.98946104e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.98404553e-04, 0.00000000e+00, 0.00000000e+00, 7.53729297e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.30901808e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.55561389e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.49149757e-04, 0.00000000e+00, 0.00000000e+00, 4.89142439e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.60400487e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.46167170e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.98922337e-04, 0.00000000e+00, 0.00000000e+00, 3.98969722e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.00145242e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.81115885e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.88266661e-04, 0.00000000e+00, 0.00000000e+00, 7.46302106e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.88538730e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.99041187e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.88694457e-04, 0.00000000e+00, 0.00000000e+00, 4.89204738e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.85421810e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.40411599e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.88636109e-04, 0.00000000e+00, 0.00000000e+00, 4.02193321e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.69990780e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.46378485e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.88908861e-04, 0.00000000e+00, 0.00000000e+00, 7.98094342e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.23406696e-07, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.01113667e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.46595123e-04, 0.00000000e+00, 0.00000000e+00, 6.30600305e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.44040044e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.88344078e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.92878202e-04, 0.00000000e+00, 0.00000000e+00, 1.96816629e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.98803367e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.54507987e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.30977281e-04, 0.00000000e+00, 0.00000000e+00, 3.23406696e-07,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.16339914e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.15712441e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        8.87202471e-04, 0.00000000e+00, 0.00000000e+00, 1.01651159e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.63883164e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.79453326e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.09271050e-03, 0.00000000e+00, 0.00000000e+00, 6.23161926e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.88928267e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.88597206e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.99351248e-04, 0.00000000e+00, 0.00000000e+00, 6.31347408e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.88845152e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.99017676e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.30826535e-04, 0.00000000e+00, 0.00000000e+00, 4.88500262e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.46391234e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.97915478e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.90683948e-04, 0.00000000e+00, 0.00000000e+00, 6.60224631e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.80175308e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.46251061e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.46353023e-04, 0.00000000e+00, 0.00000000e+00, 4.88421301e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.03663127e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.94914512e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.07582022e-04, 0.00000000e+00, 0.00000000e+00, 1.02682804e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.38430549e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.88616597e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.15704607e-03, 0.00000000e+00, 0.00000000e+00, 1.34706252e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.89223247e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.71853646e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.88616714e-04, 0.00000000e+00, 0.00000000e+00, 6.98309866e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.08926664e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.47749917e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.70454000e-04, 0.00000000e+00, 0.00000000e+00, 3.52137259e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.88674980e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.08539884e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.96631700e-04, 0.00000000e+00, 0.00000000e+00, 7.43515553e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.96609864e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.46416677e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.00113613e-04, 0.00000000e+00, 0.00000000e+00, 9.84496801e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.10008498e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.88733421e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.95577209e-04, 0.00000000e+00, 0.00000000e+00, 2.65546182e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.97472172e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.04880143e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.91642722e-04, 0.00000000e+00, 0.00000000e+00, 8.07556106e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.86484861e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.54796177e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.97129009e-04, 0.00000000e+00, 0.00000000e+00, 6.60615534e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.02467104e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.46549397e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.45830912e-04, 0.00000000e+00, 0.00000000e+00, 8.78892003e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.68232995e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.33840599e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.01530467e-03, 0.00000000e+00, 0.00000000e+00, 1.02065934e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00894868e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.95986874e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.85709349e-04, 0.00000000e+00, 0.00000000e+00, 4.93783409e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.96955693e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.99656859e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.00737472e-04, 0.00000000e+00, 0.00000000e+00, 4.16206499e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.30600918e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.93284976e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.91395894e-04, 0.00000000e+00, 0.00000000e+00, 7.99141316e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.51858409e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.94710653e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.15522121e-04, 0.00000000e+00, 0.00000000e+00, 4.81402973e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.46308026e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.16736470e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.93994196e-04, 0.00000000e+00, 0.00000000e+00, 1.32390932e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.61122657e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.43175079e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        8.75148414e-04, 0.00000000e+00, 0.00000000e+00, 4.85733504e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.07432006e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.29770166e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.93209630e-04, 0.00000000e+00, 0.00000000e+00, 9.80769195e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.50293630e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.50928550e-06, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.78118303e-03, 0.00000000e+00, 0.00000000e+00, 4.04423931e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.74220174e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.98802796e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.43722694e-04, 0.00000000e+00, 0.00000000e+00, 4.90038367e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.85434925e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.10142352e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.43241256e-04, 0.00000000e+00, 0.00000000e+00, 4.89721317e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 5.04789748e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.98712471e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.37340542e-04, 0.00000000e+00, 0.00000000e+00, 6.02359553e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.97850745e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.49327233e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.86921381e-04, 0.00000000e+00, 0.00000000e+00, 3.94592916e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.30025371e-05, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.31622492e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.97456932e-04, 0.00000000e+00, 0.00000000e+00, 3.92263213e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.02512569e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.41321212e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.53402559e-04, 0.00000000e+00, 0.00000000e+00, 3.99965113e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.80118254e-05, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.85402605e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.97636316e-04, 0.00000000e+00, 0.00000000e+00, 1.26326745e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.83603328e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.02893725e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.95103293e-04, 0.00000000e+00, 0.00000000e+00, 4.00194931e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.94712312e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.52743641e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        6.64343442e-04, 0.00000000e+00, 0.00000000e+00, 7.44689238e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.30374059e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.36227390e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.04090883e-04, 0.00000000e+00, 0.00000000e+00, 6.30052777e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.99855079e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.24969117e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.50177573e-04, 0.00000000e+00, 0.00000000e+00, 7.42068895e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.07637642e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.40369395e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.04930574e-04, 0.00000000e+00, 0.00000000e+00, 3.98544861e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.98096108e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.03987252e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.98111696e-04, 0.00000000e+00, 0.00000000e+00, 7.43898559e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.01228290e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.82826715e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.84497166e-04, 0.00000000e+00, 0.00000000e+00, 5.02125227e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.90562759e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.70778008e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.86240331e-04, 0.00000000e+00, 0.00000000e+00, 1.84528561e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.06893772e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.85731463e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.89912300e-04, 0.00000000e+00, 0.00000000e+00, 3.98688068e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.90042803e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.91424392e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.20768924e-03, 0.00000000e+00, 0.00000000e+00, 6.64589435e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.91281749e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.32957038e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        7.95163091e-04, 0.00000000e+00, 0.00000000e+00, 8.88433424e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.34698297e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.32076614e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.02767302e-04, 0.00000000e+00, 0.00000000e+00, 9.76921896e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.53918509e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.52637910e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.20889216e-04, 0.00000000e+00, 0.00000000e+00, 8.07154026e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.05026970e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.54279707e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.97731317e-04, 0.00000000e+00, 0.00000000e+00, 1.46889124e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.92729673e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.88631868e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.95240958e-04, 0.00000000e+00, 0.00000000e+00, 5.04916794e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.48437306e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 8.07729569e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.54252329e-03, 0.00000000e+00, 0.00000000e+00, 7.48251997e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.92334793e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.94563046e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.37028334e-03, 0.00000000e+00, 0.00000000e+00, 4.88772448e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.64039310e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 8.02971909e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.75641996e-04, 0.00000000e+00, 0.00000000e+00, 8.05876757e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.36328288e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.83933511e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.07422395e-03, 0.00000000e+00, 0.00000000e+00, 1.01683104e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.96649119e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.48216114e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        4.06000802e-04, 0.00000000e+00, 0.00000000e+00, 4.88305129e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.46735333e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.16312103e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.19667062e-03, 0.00000000e+00, 0.00000000e+00, 3.98826742e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.46404071e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.55705251e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        8.03138916e-04, 0.00000000e+00, 0.00000000e+00, 1.71359726e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.89788542e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.46225503e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.04792993e-02, 0.00000000e+00, 0.00000000e+00, 3.92927880e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.02949614e-04, 0.00000000e+00]),\n",
       " 'param_C': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
       "                    0.0001, 0.0001, 0.0001, 0.0001, 0.00014563484775012445,\n",
       "                    0.00014563484775012445, 0.00014563484775012445,\n",
       "                    0.00014563484775012445, 0.00014563484775012445,\n",
       "                    0.00014563484775012445, 0.00014563484775012445,\n",
       "                    0.00014563484775012445, 0.00014563484775012445,\n",
       "                    0.00014563484775012445, 0.00014563484775012445,\n",
       "                    0.00014563484775012445, 0.00014563484775012445,\n",
       "                    0.00014563484775012445, 0.00014563484775012445,\n",
       "                    0.00014563484775012445, 0.00014563484775012445,\n",
       "                    0.00014563484775012445, 0.00021209508879201905,\n",
       "                    0.00021209508879201905, 0.00021209508879201905,\n",
       "                    0.00021209508879201905, 0.00021209508879201905,\n",
       "                    0.00021209508879201905, 0.00021209508879201905,\n",
       "                    0.00021209508879201905, 0.00021209508879201905,\n",
       "                    0.00021209508879201905, 0.00021209508879201905,\n",
       "                    0.00021209508879201905, 0.00021209508879201905,\n",
       "                    0.00021209508879201905, 0.00021209508879201905,\n",
       "                    0.00021209508879201905, 0.00021209508879201905,\n",
       "                    0.00021209508879201905, 0.00030888435964774815,\n",
       "                    0.00030888435964774815, 0.00030888435964774815,\n",
       "                    0.00030888435964774815, 0.00030888435964774815,\n",
       "                    0.00030888435964774815, 0.00030888435964774815,\n",
       "                    0.00030888435964774815, 0.00030888435964774815,\n",
       "                    0.00030888435964774815, 0.00030888435964774815,\n",
       "                    0.00030888435964774815, 0.00030888435964774815,\n",
       "                    0.00030888435964774815, 0.00030888435964774815,\n",
       "                    0.00030888435964774815, 0.00030888435964774815,\n",
       "                    0.00030888435964774815, 0.0004498432668969444,\n",
       "                    0.0004498432668969444, 0.0004498432668969444,\n",
       "                    0.0004498432668969444, 0.0004498432668969444,\n",
       "                    0.0004498432668969444, 0.0004498432668969444,\n",
       "                    0.0004498432668969444, 0.0004498432668969444,\n",
       "                    0.0004498432668969444, 0.0004498432668969444,\n",
       "                    0.0004498432668969444, 0.0004498432668969444,\n",
       "                    0.0004498432668969444, 0.0004498432668969444,\n",
       "                    0.0004498432668969444, 0.0004498432668969444,\n",
       "                    0.0004498432668969444, 0.0006551285568595509,\n",
       "                    0.0006551285568595509, 0.0006551285568595509,\n",
       "                    0.0006551285568595509, 0.0006551285568595509,\n",
       "                    0.0006551285568595509, 0.0006551285568595509,\n",
       "                    0.0006551285568595509, 0.0006551285568595509,\n",
       "                    0.0006551285568595509, 0.0006551285568595509,\n",
       "                    0.0006551285568595509, 0.0006551285568595509,\n",
       "                    0.0006551285568595509, 0.0006551285568595509,\n",
       "                    0.0006551285568595509, 0.0006551285568595509,\n",
       "                    0.0006551285568595509, 0.0009540954763499944,\n",
       "                    0.0009540954763499944, 0.0009540954763499944,\n",
       "                    0.0009540954763499944, 0.0009540954763499944,\n",
       "                    0.0009540954763499944, 0.0009540954763499944,\n",
       "                    0.0009540954763499944, 0.0009540954763499944,\n",
       "                    0.0009540954763499944, 0.0009540954763499944,\n",
       "                    0.0009540954763499944, 0.0009540954763499944,\n",
       "                    0.0009540954763499944, 0.0009540954763499944,\n",
       "                    0.0009540954763499944, 0.0009540954763499944,\n",
       "                    0.0009540954763499944, 0.0013894954943731374,\n",
       "                    0.0013894954943731374, 0.0013894954943731374,\n",
       "                    0.0013894954943731374, 0.0013894954943731374,\n",
       "                    0.0013894954943731374, 0.0013894954943731374,\n",
       "                    0.0013894954943731374, 0.0013894954943731374,\n",
       "                    0.0013894954943731374, 0.0013894954943731374,\n",
       "                    0.0013894954943731374, 0.0013894954943731374,\n",
       "                    0.0013894954943731374, 0.0013894954943731374,\n",
       "                    0.0013894954943731374, 0.0013894954943731374,\n",
       "                    0.0013894954943731374, 0.0020235896477251557,\n",
       "                    0.0020235896477251557, 0.0020235896477251557,\n",
       "                    0.0020235896477251557, 0.0020235896477251557,\n",
       "                    0.0020235896477251557, 0.0020235896477251557,\n",
       "                    0.0020235896477251557, 0.0020235896477251557,\n",
       "                    0.0020235896477251557, 0.0020235896477251557,\n",
       "                    0.0020235896477251557, 0.0020235896477251557,\n",
       "                    0.0020235896477251557, 0.0020235896477251557,\n",
       "                    0.0020235896477251557, 0.0020235896477251557,\n",
       "                    0.0020235896477251557, 0.0029470517025518097,\n",
       "                    0.0029470517025518097, 0.0029470517025518097,\n",
       "                    0.0029470517025518097, 0.0029470517025518097,\n",
       "                    0.0029470517025518097, 0.0029470517025518097,\n",
       "                    0.0029470517025518097, 0.0029470517025518097,\n",
       "                    0.0029470517025518097, 0.0029470517025518097,\n",
       "                    0.0029470517025518097, 0.0029470517025518097,\n",
       "                    0.0029470517025518097, 0.0029470517025518097,\n",
       "                    0.0029470517025518097, 0.0029470517025518097,\n",
       "                    0.0029470517025518097, 0.004291934260128779,\n",
       "                    0.004291934260128779, 0.004291934260128779,\n",
       "                    0.004291934260128779, 0.004291934260128779,\n",
       "                    0.004291934260128779, 0.004291934260128779,\n",
       "                    0.004291934260128779, 0.004291934260128779,\n",
       "                    0.004291934260128779, 0.004291934260128779,\n",
       "                    0.004291934260128779, 0.004291934260128779,\n",
       "                    0.004291934260128779, 0.004291934260128779,\n",
       "                    0.004291934260128779, 0.004291934260128779,\n",
       "                    0.004291934260128779, 0.0062505519252739694,\n",
       "                    0.0062505519252739694, 0.0062505519252739694,\n",
       "                    0.0062505519252739694, 0.0062505519252739694,\n",
       "                    0.0062505519252739694, 0.0062505519252739694,\n",
       "                    0.0062505519252739694, 0.0062505519252739694,\n",
       "                    0.0062505519252739694, 0.0062505519252739694,\n",
       "                    0.0062505519252739694, 0.0062505519252739694,\n",
       "                    0.0062505519252739694, 0.0062505519252739694,\n",
       "                    0.0062505519252739694, 0.0062505519252739694,\n",
       "                    0.0062505519252739694, 0.009102981779915217,\n",
       "                    0.009102981779915217, 0.009102981779915217,\n",
       "                    0.009102981779915217, 0.009102981779915217,\n",
       "                    0.009102981779915217, 0.009102981779915217,\n",
       "                    0.009102981779915217, 0.009102981779915217,\n",
       "                    0.009102981779915217, 0.009102981779915217,\n",
       "                    0.009102981779915217, 0.009102981779915217,\n",
       "                    0.009102981779915217, 0.009102981779915217,\n",
       "                    0.009102981779915217, 0.009102981779915217,\n",
       "                    0.009102981779915217, 0.013257113655901081,\n",
       "                    0.013257113655901081, 0.013257113655901081,\n",
       "                    0.013257113655901081, 0.013257113655901081,\n",
       "                    0.013257113655901081, 0.013257113655901081,\n",
       "                    0.013257113655901081, 0.013257113655901081,\n",
       "                    0.013257113655901081, 0.013257113655901081,\n",
       "                    0.013257113655901081, 0.013257113655901081,\n",
       "                    0.013257113655901081, 0.013257113655901081,\n",
       "                    0.013257113655901081, 0.013257113655901081,\n",
       "                    0.013257113655901081, 0.019306977288832496,\n",
       "                    0.019306977288832496, 0.019306977288832496,\n",
       "                    0.019306977288832496, 0.019306977288832496,\n",
       "                    0.019306977288832496, 0.019306977288832496,\n",
       "                    0.019306977288832496, 0.019306977288832496,\n",
       "                    0.019306977288832496, 0.019306977288832496,\n",
       "                    0.019306977288832496, 0.019306977288832496,\n",
       "                    0.019306977288832496, 0.019306977288832496,\n",
       "                    0.019306977288832496, 0.019306977288832496,\n",
       "                    0.019306977288832496, 0.02811768697974228,\n",
       "                    0.02811768697974228, 0.02811768697974228,\n",
       "                    0.02811768697974228, 0.02811768697974228,\n",
       "                    0.02811768697974228, 0.02811768697974228,\n",
       "                    0.02811768697974228, 0.02811768697974228,\n",
       "                    0.02811768697974228, 0.02811768697974228,\n",
       "                    0.02811768697974228, 0.02811768697974228,\n",
       "                    0.02811768697974228, 0.02811768697974228,\n",
       "                    0.02811768697974228, 0.02811768697974228,\n",
       "                    0.02811768697974228, 0.040949150623804234,\n",
       "                    0.040949150623804234, 0.040949150623804234,\n",
       "                    0.040949150623804234, 0.040949150623804234,\n",
       "                    0.040949150623804234, 0.040949150623804234,\n",
       "                    0.040949150623804234, 0.040949150623804234,\n",
       "                    0.040949150623804234, 0.040949150623804234,\n",
       "                    0.040949150623804234, 0.040949150623804234,\n",
       "                    0.040949150623804234, 0.040949150623804234,\n",
       "                    0.040949150623804234, 0.040949150623804234,\n",
       "                    0.040949150623804234, 0.05963623316594643,\n",
       "                    0.05963623316594643, 0.05963623316594643,\n",
       "                    0.05963623316594643, 0.05963623316594643,\n",
       "                    0.05963623316594643, 0.05963623316594643,\n",
       "                    0.05963623316594643, 0.05963623316594643,\n",
       "                    0.05963623316594643, 0.05963623316594643,\n",
       "                    0.05963623316594643, 0.05963623316594643,\n",
       "                    0.05963623316594643, 0.05963623316594643,\n",
       "                    0.05963623316594643, 0.05963623316594643,\n",
       "                    0.05963623316594643, 0.08685113737513521,\n",
       "                    0.08685113737513521, 0.08685113737513521,\n",
       "                    0.08685113737513521, 0.08685113737513521,\n",
       "                    0.08685113737513521, 0.08685113737513521,\n",
       "                    0.08685113737513521, 0.08685113737513521,\n",
       "                    0.08685113737513521, 0.08685113737513521,\n",
       "                    0.08685113737513521, 0.08685113737513521,\n",
       "                    0.08685113737513521, 0.08685113737513521,\n",
       "                    0.08685113737513521, 0.08685113737513521,\n",
       "                    0.08685113737513521, 0.12648552168552957,\n",
       "                    0.12648552168552957, 0.12648552168552957,\n",
       "                    0.12648552168552957, 0.12648552168552957,\n",
       "                    0.12648552168552957, 0.12648552168552957,\n",
       "                    0.12648552168552957, 0.12648552168552957,\n",
       "                    0.12648552168552957, 0.12648552168552957,\n",
       "                    0.12648552168552957, 0.12648552168552957,\n",
       "                    0.12648552168552957, 0.12648552168552957,\n",
       "                    0.12648552168552957, 0.12648552168552957,\n",
       "                    0.12648552168552957, 0.18420699693267145,\n",
       "                    0.18420699693267145, 0.18420699693267145,\n",
       "                    0.18420699693267145, 0.18420699693267145,\n",
       "                    0.18420699693267145, 0.18420699693267145,\n",
       "                    0.18420699693267145, 0.18420699693267145,\n",
       "                    0.18420699693267145, 0.18420699693267145,\n",
       "                    0.18420699693267145, 0.18420699693267145,\n",
       "                    0.18420699693267145, 0.18420699693267145,\n",
       "                    0.18420699693267145, 0.18420699693267145,\n",
       "                    0.18420699693267145, 0.2682695795279725,\n",
       "                    0.2682695795279725, 0.2682695795279725,\n",
       "                    0.2682695795279725, 0.2682695795279725,\n",
       "                    0.2682695795279725, 0.2682695795279725,\n",
       "                    0.2682695795279725, 0.2682695795279725,\n",
       "                    0.2682695795279725, 0.2682695795279725,\n",
       "                    0.2682695795279725, 0.2682695795279725,\n",
       "                    0.2682695795279725, 0.2682695795279725,\n",
       "                    0.2682695795279725, 0.2682695795279725,\n",
       "                    0.2682695795279725, 0.3906939937054613,\n",
       "                    0.3906939937054613, 0.3906939937054613,\n",
       "                    0.3906939937054613, 0.3906939937054613,\n",
       "                    0.3906939937054613, 0.3906939937054613,\n",
       "                    0.3906939937054613, 0.3906939937054613,\n",
       "                    0.3906939937054613, 0.3906939937054613,\n",
       "                    0.3906939937054613, 0.3906939937054613,\n",
       "                    0.3906939937054613, 0.3906939937054613,\n",
       "                    0.3906939937054613, 0.3906939937054613,\n",
       "                    0.3906939937054613, 0.5689866029018293,\n",
       "                    0.5689866029018293, 0.5689866029018293,\n",
       "                    0.5689866029018293, 0.5689866029018293,\n",
       "                    0.5689866029018293, 0.5689866029018293,\n",
       "                    0.5689866029018293, 0.5689866029018293,\n",
       "                    0.5689866029018293, 0.5689866029018293,\n",
       "                    0.5689866029018293, 0.5689866029018293,\n",
       "                    0.5689866029018293, 0.5689866029018293,\n",
       "                    0.5689866029018293, 0.5689866029018293,\n",
       "                    0.5689866029018293, 0.8286427728546842,\n",
       "                    0.8286427728546842, 0.8286427728546842,\n",
       "                    0.8286427728546842, 0.8286427728546842,\n",
       "                    0.8286427728546842, 0.8286427728546842,\n",
       "                    0.8286427728546842, 0.8286427728546842,\n",
       "                    0.8286427728546842, 0.8286427728546842,\n",
       "                    0.8286427728546842, 0.8286427728546842,\n",
       "                    0.8286427728546842, 0.8286427728546842,\n",
       "                    0.8286427728546842, 0.8286427728546842,\n",
       "                    0.8286427728546842, 1.2067926406393288,\n",
       "                    1.2067926406393288, 1.2067926406393288,\n",
       "                    1.2067926406393288, 1.2067926406393288,\n",
       "                    1.2067926406393288, 1.2067926406393288,\n",
       "                    1.2067926406393288, 1.2067926406393288,\n",
       "                    1.2067926406393288, 1.2067926406393288,\n",
       "                    1.2067926406393288, 1.2067926406393288,\n",
       "                    1.2067926406393288, 1.2067926406393288,\n",
       "                    1.2067926406393288, 1.2067926406393288,\n",
       "                    1.2067926406393288, 1.7575106248547894,\n",
       "                    1.7575106248547894, 1.7575106248547894,\n",
       "                    1.7575106248547894, 1.7575106248547894,\n",
       "                    1.7575106248547894, 1.7575106248547894,\n",
       "                    1.7575106248547894, 1.7575106248547894,\n",
       "                    1.7575106248547894, 1.7575106248547894,\n",
       "                    1.7575106248547894, 1.7575106248547894,\n",
       "                    1.7575106248547894, 1.7575106248547894,\n",
       "                    1.7575106248547894, 1.7575106248547894,\n",
       "                    1.7575106248547894, 2.559547922699533,\n",
       "                    2.559547922699533, 2.559547922699533,\n",
       "                    2.559547922699533, 2.559547922699533,\n",
       "                    2.559547922699533, 2.559547922699533,\n",
       "                    2.559547922699533, 2.559547922699533,\n",
       "                    2.559547922699533, 2.559547922699533,\n",
       "                    2.559547922699533, 2.559547922699533,\n",
       "                    2.559547922699533, 2.559547922699533,\n",
       "                    2.559547922699533, 2.559547922699533,\n",
       "                    2.559547922699533, 3.727593720314938,\n",
       "                    3.727593720314938, 3.727593720314938,\n",
       "                    3.727593720314938, 3.727593720314938,\n",
       "                    3.727593720314938, 3.727593720314938,\n",
       "                    3.727593720314938, 3.727593720314938,\n",
       "                    3.727593720314938, 3.727593720314938,\n",
       "                    3.727593720314938, 3.727593720314938,\n",
       "                    3.727593720314938, 3.727593720314938,\n",
       "                    3.727593720314938, 3.727593720314938,\n",
       "                    3.727593720314938, 5.428675439323859,\n",
       "                    5.428675439323859, 5.428675439323859,\n",
       "                    5.428675439323859, 5.428675439323859,\n",
       "                    5.428675439323859, 5.428675439323859,\n",
       "                    5.428675439323859, 5.428675439323859,\n",
       "                    5.428675439323859, 5.428675439323859,\n",
       "                    5.428675439323859, 5.428675439323859,\n",
       "                    5.428675439323859, 5.428675439323859,\n",
       "                    5.428675439323859, 5.428675439323859,\n",
       "                    5.428675439323859, 7.9060432109076855,\n",
       "                    7.9060432109076855, 7.9060432109076855,\n",
       "                    7.9060432109076855, 7.9060432109076855,\n",
       "                    7.9060432109076855, 7.9060432109076855,\n",
       "                    7.9060432109076855, 7.9060432109076855,\n",
       "                    7.9060432109076855, 7.9060432109076855,\n",
       "                    7.9060432109076855, 7.9060432109076855,\n",
       "                    7.9060432109076855, 7.9060432109076855,\n",
       "                    7.9060432109076855, 7.9060432109076855,\n",
       "                    7.9060432109076855, 11.513953993264458,\n",
       "                    11.513953993264458, 11.513953993264458,\n",
       "                    11.513953993264458, 11.513953993264458,\n",
       "                    11.513953993264458, 11.513953993264458,\n",
       "                    11.513953993264458, 11.513953993264458,\n",
       "                    11.513953993264458, 11.513953993264458,\n",
       "                    11.513953993264458, 11.513953993264458,\n",
       "                    11.513953993264458, 11.513953993264458,\n",
       "                    11.513953993264458, 11.513953993264458,\n",
       "                    11.513953993264458, 16.768329368110066,\n",
       "                    16.768329368110066, 16.768329368110066,\n",
       "                    16.768329368110066, 16.768329368110066,\n",
       "                    16.768329368110066, 16.768329368110066,\n",
       "                    16.768329368110066, 16.768329368110066,\n",
       "                    16.768329368110066, 16.768329368110066,\n",
       "                    16.768329368110066, 16.768329368110066,\n",
       "                    16.768329368110066, 16.768329368110066,\n",
       "                    16.768329368110066, 16.768329368110066,\n",
       "                    16.768329368110066, 24.420530945486497,\n",
       "                    24.420530945486497, 24.420530945486497,\n",
       "                    24.420530945486497, 24.420530945486497,\n",
       "                    24.420530945486497, 24.420530945486497,\n",
       "                    24.420530945486497, 24.420530945486497,\n",
       "                    24.420530945486497, 24.420530945486497,\n",
       "                    24.420530945486497, 24.420530945486497,\n",
       "                    24.420530945486497, 24.420530945486497,\n",
       "                    24.420530945486497, 24.420530945486497,\n",
       "                    24.420530945486497, 35.564803062231285,\n",
       "                    35.564803062231285, 35.564803062231285,\n",
       "                    35.564803062231285, 35.564803062231285,\n",
       "                    35.564803062231285, 35.564803062231285,\n",
       "                    35.564803062231285, 35.564803062231285,\n",
       "                    35.564803062231285, 35.564803062231285,\n",
       "                    35.564803062231285, 35.564803062231285,\n",
       "                    35.564803062231285, 35.564803062231285,\n",
       "                    35.564803062231285, 35.564803062231285,\n",
       "                    35.564803062231285, 51.79474679231202,\n",
       "                    51.79474679231202, 51.79474679231202,\n",
       "                    51.79474679231202, 51.79474679231202,\n",
       "                    51.79474679231202, 51.79474679231202,\n",
       "                    51.79474679231202, 51.79474679231202,\n",
       "                    51.79474679231202, 51.79474679231202,\n",
       "                    51.79474679231202, 51.79474679231202,\n",
       "                    51.79474679231202, 51.79474679231202,\n",
       "                    51.79474679231202, 51.79474679231202,\n",
       "                    51.79474679231202, 75.43120063354607,\n",
       "                    75.43120063354607, 75.43120063354607,\n",
       "                    75.43120063354607, 75.43120063354607,\n",
       "                    75.43120063354607, 75.43120063354607,\n",
       "                    75.43120063354607, 75.43120063354607,\n",
       "                    75.43120063354607, 75.43120063354607,\n",
       "                    75.43120063354607, 75.43120063354607,\n",
       "                    75.43120063354607, 75.43120063354607,\n",
       "                    75.43120063354607, 75.43120063354607,\n",
       "                    75.43120063354607, 109.85411419875572,\n",
       "                    109.85411419875572, 109.85411419875572,\n",
       "                    109.85411419875572, 109.85411419875572,\n",
       "                    109.85411419875572, 109.85411419875572,\n",
       "                    109.85411419875572, 109.85411419875572,\n",
       "                    109.85411419875572, 109.85411419875572,\n",
       "                    109.85411419875572, 109.85411419875572,\n",
       "                    109.85411419875572, 109.85411419875572,\n",
       "                    109.85411419875572, 109.85411419875572,\n",
       "                    109.85411419875572, 159.98587196060572,\n",
       "                    159.98587196060572, 159.98587196060572,\n",
       "                    159.98587196060572, 159.98587196060572,\n",
       "                    159.98587196060572, 159.98587196060572,\n",
       "                    159.98587196060572, 159.98587196060572,\n",
       "                    159.98587196060572, 159.98587196060572,\n",
       "                    159.98587196060572, 159.98587196060572,\n",
       "                    159.98587196060572, 159.98587196060572,\n",
       "                    159.98587196060572, 159.98587196060572,\n",
       "                    159.98587196060572, 232.99518105153672,\n",
       "                    232.99518105153672, 232.99518105153672,\n",
       "                    232.99518105153672, 232.99518105153672,\n",
       "                    232.99518105153672, 232.99518105153672,\n",
       "                    232.99518105153672, 232.99518105153672,\n",
       "                    232.99518105153672, 232.99518105153672,\n",
       "                    232.99518105153672, 232.99518105153672,\n",
       "                    232.99518105153672, 232.99518105153672,\n",
       "                    232.99518105153672, 232.99518105153672,\n",
       "                    232.99518105153672, 339.3221771895323,\n",
       "                    339.3221771895323, 339.3221771895323,\n",
       "                    339.3221771895323, 339.3221771895323,\n",
       "                    339.3221771895323, 339.3221771895323,\n",
       "                    339.3221771895323, 339.3221771895323,\n",
       "                    339.3221771895323, 339.3221771895323,\n",
       "                    339.3221771895323, 339.3221771895323,\n",
       "                    339.3221771895323, 339.3221771895323,\n",
       "                    339.3221771895323, 339.3221771895323,\n",
       "                    339.3221771895323, 494.1713361323828,\n",
       "                    494.1713361323828, 494.1713361323828,\n",
       "                    494.1713361323828, 494.1713361323828,\n",
       "                    494.1713361323828, 494.1713361323828,\n",
       "                    494.1713361323828, 494.1713361323828,\n",
       "                    494.1713361323828, 494.1713361323828,\n",
       "                    494.1713361323828, 494.1713361323828,\n",
       "                    494.1713361323828, 494.1713361323828,\n",
       "                    494.1713361323828, 494.1713361323828,\n",
       "                    494.1713361323828, 719.6856730011514,\n",
       "                    719.6856730011514, 719.6856730011514,\n",
       "                    719.6856730011514, 719.6856730011514,\n",
       "                    719.6856730011514, 719.6856730011514,\n",
       "                    719.6856730011514, 719.6856730011514,\n",
       "                    719.6856730011514, 719.6856730011514,\n",
       "                    719.6856730011514, 719.6856730011514,\n",
       "                    719.6856730011514, 719.6856730011514,\n",
       "                    719.6856730011514, 719.6856730011514,\n",
       "                    719.6856730011514, 1048.1131341546852,\n",
       "                    1048.1131341546852, 1048.1131341546852,\n",
       "                    1048.1131341546852, 1048.1131341546852,\n",
       "                    1048.1131341546852, 1048.1131341546852,\n",
       "                    1048.1131341546852, 1048.1131341546852,\n",
       "                    1048.1131341546852, 1048.1131341546852,\n",
       "                    1048.1131341546852, 1048.1131341546852,\n",
       "                    1048.1131341546852, 1048.1131341546852,\n",
       "                    1048.1131341546852, 1048.1131341546852,\n",
       "                    1048.1131341546852, 1526.4179671752302,\n",
       "                    1526.4179671752302, 1526.4179671752302,\n",
       "                    1526.4179671752302, 1526.4179671752302,\n",
       "                    1526.4179671752302, 1526.4179671752302,\n",
       "                    1526.4179671752302, 1526.4179671752302,\n",
       "                    1526.4179671752302, 1526.4179671752302,\n",
       "                    1526.4179671752302, 1526.4179671752302,\n",
       "                    1526.4179671752302, 1526.4179671752302,\n",
       "                    1526.4179671752302, 1526.4179671752302,\n",
       "                    1526.4179671752302, 2222.996482526191,\n",
       "                    2222.996482526191, 2222.996482526191,\n",
       "                    2222.996482526191, 2222.996482526191,\n",
       "                    2222.996482526191, 2222.996482526191,\n",
       "                    2222.996482526191, 2222.996482526191,\n",
       "                    2222.996482526191, 2222.996482526191,\n",
       "                    2222.996482526191, 2222.996482526191,\n",
       "                    2222.996482526191, 2222.996482526191,\n",
       "                    2222.996482526191, 2222.996482526191,\n",
       "                    2222.996482526191, 3237.45754281764, 3237.45754281764,\n",
       "                    3237.45754281764, 3237.45754281764, 3237.45754281764,\n",
       "                    3237.45754281764, 3237.45754281764, 3237.45754281764,\n",
       "                    3237.45754281764, 3237.45754281764, 3237.45754281764,\n",
       "                    3237.45754281764, 3237.45754281764, 3237.45754281764,\n",
       "                    3237.45754281764, 3237.45754281764, 3237.45754281764,\n",
       "                    3237.45754281764, 4714.8663634573895,\n",
       "                    4714.8663634573895, 4714.8663634573895,\n",
       "                    4714.8663634573895, 4714.8663634573895,\n",
       "                    4714.8663634573895, 4714.8663634573895,\n",
       "                    4714.8663634573895, 4714.8663634573895,\n",
       "                    4714.8663634573895, 4714.8663634573895,\n",
       "                    4714.8663634573895, 4714.8663634573895,\n",
       "                    4714.8663634573895, 4714.8663634573895,\n",
       "                    4714.8663634573895, 4714.8663634573895,\n",
       "                    4714.8663634573895, 6866.488450042998,\n",
       "                    6866.488450042998, 6866.488450042998,\n",
       "                    6866.488450042998, 6866.488450042998,\n",
       "                    6866.488450042998, 6866.488450042998,\n",
       "                    6866.488450042998, 6866.488450042998,\n",
       "                    6866.488450042998, 6866.488450042998,\n",
       "                    6866.488450042998, 6866.488450042998,\n",
       "                    6866.488450042998, 6866.488450042998,\n",
       "                    6866.488450042998, 6866.488450042998,\n",
       "                    6866.488450042998, 10000.0, 10000.0, 10000.0, 10000.0,\n",
       "                    10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0,\n",
       "                    10000.0, 10000.0, 10000.0, 10000.0, 10000.0, 10000.0,\n",
       "                    10000.0, 10000.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_iter': masked_array(data=[70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000, 70, 70, 70, 90, 90, 90, 100, 100,\n",
       "                    100, 500, 500, 500, 1000, 1000, 1000, 2000, 2000, 2000,\n",
       "                    70, 70, 70, 90, 90, 90, 100, 100, 100, 500, 500, 500,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 70, 70, 70, 90, 90,\n",
       "                    90, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000,\n",
       "                    2000, 2000, 2000],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_penalty': masked_array(data=['l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet',\n",
       "                    'l1', 'l2', 'elasticnet', 'l1', 'l2', 'elasticnet'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.0001, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.0001, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.0001, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0001, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.0001, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.0001, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0001, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.0001, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.0001, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0001, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.0001, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.0001, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0001, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.0001, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.0001, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0001, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.0001, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.0001, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.00014563484775012445, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.00021209508879201905, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.00030888435964774815, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.0004498432668969444, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.0006551285568595509, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.0009540954763499944, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.0013894954943731374, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.0020235896477251557, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.0029470517025518097, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.004291934260128779, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.0062505519252739694, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.009102981779915217, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.013257113655901081, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.019306977288832496, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.02811768697974228, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.040949150623804234, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.05963623316594643, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.08685113737513521, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.12648552168552957, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.18420699693267145, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.2682695795279725, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.3906939937054613, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.5689866029018293, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 0.8286427728546842, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 1.2067926406393288, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 1.7575106248547894, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 2.559547922699533, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 3.727593720314938, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 5.428675439323859, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 7.9060432109076855, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 11.513953993264458, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 16.768329368110066, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 24.420530945486497, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 35.564803062231285, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 51.79474679231202, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 75.43120063354607, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 109.85411419875572, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 159.98587196060572, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 232.99518105153672, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 339.3221771895323, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 494.1713361323828, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 719.6856730011514, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 1048.1131341546852, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 1526.4179671752302, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 2222.996482526191, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 3237.45754281764, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 4714.8663634573895, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 6866.488450042998, 'max_iter': 2000, 'penalty': 'elasticnet'},\n",
       "  {'C': 10000.0, 'max_iter': 70, 'penalty': 'l1'},\n",
       "  {'C': 10000.0, 'max_iter': 70, 'penalty': 'l2'},\n",
       "  {'C': 10000.0, 'max_iter': 70, 'penalty': 'elasticnet'},\n",
       "  {'C': 10000.0, 'max_iter': 90, 'penalty': 'l1'},\n",
       "  {'C': 10000.0, 'max_iter': 90, 'penalty': 'l2'},\n",
       "  {'C': 10000.0, 'max_iter': 90, 'penalty': 'elasticnet'},\n",
       "  {'C': 10000.0, 'max_iter': 100, 'penalty': 'l1'},\n",
       "  {'C': 10000.0, 'max_iter': 100, 'penalty': 'l2'},\n",
       "  {'C': 10000.0, 'max_iter': 100, 'penalty': 'elasticnet'},\n",
       "  {'C': 10000.0, 'max_iter': 500, 'penalty': 'l1'},\n",
       "  {'C': 10000.0, 'max_iter': 500, 'penalty': 'l2'},\n",
       "  {'C': 10000.0, 'max_iter': 500, 'penalty': 'elasticnet'},\n",
       "  {'C': 10000.0, 'max_iter': 1000, 'penalty': 'l1'},\n",
       "  {'C': 10000.0, 'max_iter': 1000, 'penalty': 'l2'},\n",
       "  {'C': 10000.0, 'max_iter': 1000, 'penalty': 'elasticnet'},\n",
       "  {'C': 10000.0, 'max_iter': 2000, 'penalty': 'l1'},\n",
       "  {'C': 10000.0, 'max_iter': 2000, 'penalty': 'l2'},\n",
       "  {'C': 10000.0, 'max_iter': 2000, 'penalty': 'elasticnet'}],\n",
       " 'split0_test_score': array([       nan, 0.70822868,        nan,        nan, 0.70822868,\n",
       "               nan,        nan, 0.70822868,        nan,        nan,\n",
       "        0.70822868,        nan,        nan, 0.70822868,        nan,\n",
       "               nan, 0.70822868,        nan,        nan, 0.70896128,\n",
       "               nan,        nan, 0.70896128,        nan,        nan,\n",
       "        0.70896128,        nan,        nan, 0.70896128,        nan,\n",
       "               nan, 0.70896128,        nan,        nan, 0.70896128,\n",
       "               nan,        nan, 0.70910518,        nan,        nan,\n",
       "        0.70910518,        nan,        nan, 0.70910518,        nan,\n",
       "               nan, 0.70910518,        nan,        nan, 0.70910518,\n",
       "               nan,        nan, 0.70910518,        nan,        nan,\n",
       "        0.7096023 ,        nan,        nan, 0.7096023 ,        nan,\n",
       "               nan, 0.7096023 ,        nan,        nan, 0.7096023 ,\n",
       "               nan,        nan, 0.7096023 ,        nan,        nan,\n",
       "        0.7096023 ,        nan,        nan, 0.70983778,        nan,\n",
       "               nan, 0.70983778,        nan,        nan, 0.70983778,\n",
       "               nan,        nan, 0.70983778,        nan,        nan,\n",
       "        0.70983778,        nan,        nan, 0.70983778,        nan,\n",
       "               nan, 0.71148613,        nan,        nan, 0.71148613,\n",
       "               nan,        nan, 0.71148613,        nan,        nan,\n",
       "        0.71148613,        nan,        nan, 0.71148613,        nan,\n",
       "               nan, 0.71148613,        nan,        nan, 0.71130298,\n",
       "               nan,        nan, 0.71130298,        nan,        nan,\n",
       "        0.71130298,        nan,        nan, 0.71130298,        nan,\n",
       "               nan, 0.71130298,        nan,        nan, 0.71130298,\n",
       "               nan,        nan, 0.71174778,        nan,        nan,\n",
       "        0.71174778,        nan,        nan, 0.71174778,        nan,\n",
       "               nan, 0.71174778,        nan,        nan, 0.71174778,\n",
       "               nan,        nan, 0.71174778,        nan,        nan,\n",
       "        0.7126112 ,        nan,        nan, 0.7126112 ,        nan,\n",
       "               nan, 0.7126112 ,        nan,        nan, 0.7126112 ,\n",
       "               nan,        nan, 0.7126112 ,        nan,        nan,\n",
       "        0.7126112 ,        nan,        nan, 0.71493982,        nan,\n",
       "               nan, 0.71493982,        nan,        nan, 0.71493982,\n",
       "               nan,        nan, 0.71493982,        nan,        nan,\n",
       "        0.71493982,        nan,        nan, 0.71493982,        nan,\n",
       "               nan, 0.71755625,        nan,        nan, 0.71755625,\n",
       "               nan,        nan, 0.71755625,        nan,        nan,\n",
       "        0.71755625,        nan,        nan, 0.71755625,        nan,\n",
       "               nan, 0.71755625,        nan,        nan, 0.72189953,\n",
       "               nan,        nan, 0.72189953,        nan,        nan,\n",
       "        0.72189953,        nan,        nan, 0.72189953,        nan,\n",
       "               nan, 0.72189953,        nan,        nan, 0.72189953,\n",
       "               nan,        nan, 0.7268315 ,        nan,        nan,\n",
       "        0.7268315 ,        nan,        nan, 0.7268315 ,        nan,\n",
       "               nan, 0.7268315 ,        nan,        nan, 0.7268315 ,\n",
       "               nan,        nan, 0.7268315 ,        nan,        nan,\n",
       "        0.73352957,        nan,        nan, 0.73352957,        nan,\n",
       "               nan, 0.73352957,        nan,        nan, 0.73352957,\n",
       "               nan,        nan, 0.73352957,        nan,        nan,\n",
       "        0.73352957,        nan,        nan, 0.74213762,        nan,\n",
       "               nan, 0.74213762,        nan,        nan, 0.74213762,\n",
       "               nan,        nan, 0.74213762,        nan,        nan,\n",
       "        0.74213762,        nan,        nan, 0.74213762,        nan,\n",
       "               nan, 0.75338828,        nan,        nan, 0.75338828,\n",
       "               nan,        nan, 0.75338828,        nan,        nan,\n",
       "        0.75338828,        nan,        nan, 0.75338828,        nan,\n",
       "               nan, 0.75338828,        nan,        nan, 0.76817111,\n",
       "               nan,        nan, 0.76817111,        nan,        nan,\n",
       "        0.76817111,        nan,        nan, 0.76817111,        nan,\n",
       "               nan, 0.76817111,        nan,        nan, 0.76817111,\n",
       "               nan,        nan, 0.78574045,        nan,        nan,\n",
       "        0.78574045,        nan,        nan, 0.78574045,        nan,\n",
       "               nan, 0.78574045,        nan,        nan, 0.78574045,\n",
       "               nan,        nan, 0.78574045,        nan,        nan,\n",
       "        0.80658032,        nan,        nan, 0.80658032,        nan,\n",
       "               nan, 0.80658032,        nan,        nan, 0.80658032,\n",
       "               nan,        nan, 0.80658032,        nan,        nan,\n",
       "        0.80658032,        nan,        nan, 0.82978807,        nan,\n",
       "               nan, 0.82978807,        nan,        nan, 0.82978807,\n",
       "               nan,        nan, 0.82978807,        nan,        nan,\n",
       "        0.82978807,        nan,        nan, 0.82978807,        nan,\n",
       "               nan, 0.8532967 ,        nan,        nan, 0.8532967 ,\n",
       "               nan,        nan, 0.8532967 ,        nan,        nan,\n",
       "        0.8532967 ,        nan,        nan, 0.8532967 ,        nan,\n",
       "               nan, 0.8532967 ,        nan,        nan, 0.87575877,\n",
       "               nan,        nan, 0.87575877,        nan,        nan,\n",
       "        0.87575877,        nan,        nan, 0.87575877,        nan,\n",
       "               nan, 0.87575877,        nan,        nan, 0.87575877,\n",
       "               nan,        nan, 0.8961146 ,        nan,        nan,\n",
       "        0.8961146 ,        nan,        nan, 0.8961146 ,        nan,\n",
       "               nan, 0.8961146 ,        nan,        nan, 0.8961146 ,\n",
       "               nan,        nan, 0.8961146 ,        nan,        nan,\n",
       "        0.91313448,        nan,        nan, 0.91313448,        nan,\n",
       "               nan, 0.91313448,        nan,        nan, 0.91313448,\n",
       "               nan,        nan, 0.91313448,        nan,        nan,\n",
       "        0.91313448,        nan,        nan, 0.92710623,        nan,\n",
       "               nan, 0.92710623,        nan,        nan, 0.92710623,\n",
       "               nan,        nan, 0.92710623,        nan,        nan,\n",
       "        0.92710623,        nan,        nan, 0.92710623,        nan,\n",
       "               nan, 0.93889325,        nan,        nan, 0.93886709,\n",
       "               nan,        nan, 0.93886709,        nan,        nan,\n",
       "        0.93886709,        nan,        nan, 0.93886709,        nan,\n",
       "               nan, 0.93886709,        nan,        nan, 0.94820774,\n",
       "               nan,        nan, 0.94824699,        nan,        nan,\n",
       "        0.94824699,        nan,        nan, 0.94824699,        nan,\n",
       "               nan, 0.94824699,        nan,        nan, 0.94824699,\n",
       "               nan,        nan, 0.9553506 ,        nan,        nan,\n",
       "        0.95536368,        nan,        nan, 0.95536368,        nan,\n",
       "               nan, 0.95536368,        nan,        nan, 0.95536368,\n",
       "               nan,        nan, 0.95536368,        nan,        nan,\n",
       "        0.96062271,        nan,        nan, 0.96057038,        nan,\n",
       "               nan, 0.96057038,        nan,        nan, 0.96057038,\n",
       "               nan,        nan, 0.96057038,        nan,        nan,\n",
       "        0.96057038,        nan,        nan, 0.96384092,        nan,\n",
       "               nan, 0.96388017,        nan,        nan, 0.96388017,\n",
       "               nan,        nan, 0.96386709,        nan,        nan,\n",
       "        0.96386709,        nan,        nan, 0.96386709,        nan,\n",
       "               nan, 0.96635269,        nan,        nan, 0.96662742,\n",
       "               nan,        nan, 0.96654893,        nan,        nan,\n",
       "        0.96661434,        nan,        nan, 0.96661434,        nan,\n",
       "               nan, 0.96661434,        nan,        nan, 0.96805338,\n",
       "               nan,        nan, 0.968472  ,        nan,        nan,\n",
       "        0.96840659,        nan,        nan, 0.968472  ,        nan,\n",
       "               nan, 0.968472  ,        nan,        nan, 0.968472  ,\n",
       "               nan,        nan, 0.96895604,        nan,        nan,\n",
       "        0.96955782,        nan,        nan, 0.96947933,        nan,\n",
       "               nan, 0.96953166,        nan,        nan, 0.96953166,\n",
       "               nan,        nan, 0.96953166,        nan,        nan,\n",
       "        0.97008111,        nan,        nan, 0.97047357,        nan,\n",
       "               nan, 0.97049974,        nan,        nan, 0.97039508,\n",
       "               nan,        nan, 0.97039508,        nan,        nan,\n",
       "        0.97039508,        nan,        nan, 0.97048666,        nan,\n",
       "               nan, 0.97099686,        nan,        nan, 0.97086604,\n",
       "               nan,        nan, 0.97103611,        nan,        nan,\n",
       "        0.97103611,        nan,        nan, 0.97103611,        nan,\n",
       "               nan, 0.97053898,        nan,        nan, 0.97123234,\n",
       "               nan,        nan, 0.97137624,        nan,        nan,\n",
       "        0.97149398,        nan,        nan, 0.97149398,        nan,\n",
       "               nan, 0.97149398,        nan,        nan, 0.9708922 ,\n",
       "               nan,        nan, 0.97150706,        nan,        nan,\n",
       "        0.97176871,        nan,        nan, 0.97182104,        nan,\n",
       "               nan, 0.97182104,        nan,        nan, 0.97182104,\n",
       "               nan,        nan, 0.9714809 ,        nan,        nan,\n",
       "        0.97063056,        nan,        nan, 0.97163789,        nan,\n",
       "               nan, 0.97217425,        nan,        nan, 0.97217425,\n",
       "               nan,        nan, 0.97217425,        nan,        nan,\n",
       "        0.97132391,        nan,        nan, 0.97112768,        nan,\n",
       "               nan, 0.97167713,        nan,        nan, 0.97231816,\n",
       "               nan,        nan, 0.97231816,        nan,        nan,\n",
       "        0.97231816,        nan,        nan, 0.97165097,        nan,\n",
       "               nan, 0.9719911 ,        nan,        nan, 0.97174254,\n",
       "               nan,        nan, 0.97247514,        nan,        nan,\n",
       "        0.97247514,        nan,        nan, 0.97247514,        nan,\n",
       "               nan, 0.9711146 ,        nan,        nan, 0.97178179,\n",
       "               nan,        nan, 0.97239665,        nan,        nan,\n",
       "        0.97260597,        nan,        nan, 0.97260597,        nan,\n",
       "               nan, 0.97260597,        nan,        nan, 0.97192569,\n",
       "               nan,        nan, 0.97200419,        nan,        nan,\n",
       "        0.97246206,        nan,        nan, 0.97277603,        nan,\n",
       "               nan, 0.97277603,        nan,        nan, 0.97277603,\n",
       "               nan,        nan, 0.97203035,        nan,        nan,\n",
       "        0.97209576,        nan,        nan, 0.9723574 ,        nan,\n",
       "               nan, 0.97290686,        nan,        nan, 0.97290686,\n",
       "               nan,        nan, 0.97290686,        nan,        nan,\n",
       "        0.97205651,        nan,        nan, 0.9718472 ,        nan,\n",
       "               nan, 0.97237049,        nan,        nan, 0.97314233,\n",
       "               nan,        nan, 0.97314233,        nan,        nan,\n",
       "        0.97314233,        nan,        nan, 0.97108844,        nan,\n",
       "               nan, 0.9717033 ,        nan,        nan, 0.97254055,\n",
       "               nan,        nan, 0.9733124 ,        nan,        nan,\n",
       "        0.9733124 ,        nan,        nan, 0.9733124 ,        nan,\n",
       "               nan, 0.97154631,        nan,        nan, 0.97154631,\n",
       "               nan,        nan, 0.97238357,        nan,        nan,\n",
       "        0.97341706,        nan,        nan, 0.97341706,        nan,\n",
       "               nan, 0.97341706,        nan,        nan, 0.97042125,\n",
       "               nan,        nan, 0.97171638,        nan,        nan,\n",
       "        0.97238357,        nan,        nan, 0.97348247,        nan,\n",
       "               nan, 0.97348247,        nan,        nan, 0.97348247,\n",
       "               nan,        nan, 0.9711146 ,        nan,        nan,\n",
       "        0.9720696 ,        nan,        nan, 0.97172946,        nan,\n",
       "               nan, 0.97356096,        nan,        nan, 0.97356096,\n",
       "               nan,        nan, 0.97356096,        nan,        nan,\n",
       "        0.97135008,        nan,        nan, 0.97116693,        nan,\n",
       "               nan, 0.97129775,        nan,        nan, 0.97349555,\n",
       "               nan,        nan, 0.97349555,        nan,        nan,\n",
       "        0.97349555,        nan,        nan, 0.97252747,        nan,\n",
       "               nan, 0.97142857,        nan,        nan, 0.97248823,\n",
       "               nan,        nan, 0.97352172,        nan,        nan,\n",
       "        0.97352172,        nan,        nan, 0.97352172,        nan]),\n",
       " 'split1_test_score': array([       nan, 0.69272782,        nan,        nan, 0.69272782,\n",
       "               nan,        nan, 0.69272782,        nan,        nan,\n",
       "        0.69272782,        nan,        nan, 0.69272782,        nan,\n",
       "               nan, 0.69272782,        nan,        nan, 0.6936206 ,\n",
       "               nan,        nan, 0.6936206 ,        nan,        nan,\n",
       "        0.6936206 ,        nan,        nan, 0.6936206 ,        nan,\n",
       "               nan, 0.6936206 ,        nan,        nan, 0.6936206 ,\n",
       "               nan,        nan, 0.69467092,        nan,        nan,\n",
       "        0.69467092,        nan,        nan, 0.69467092,        nan,\n",
       "               nan, 0.69467092,        nan,        nan, 0.69467092,\n",
       "               nan,        nan, 0.69467092,        nan,        nan,\n",
       "        0.69628579,        nan,        nan, 0.69628579,        nan,\n",
       "               nan, 0.69628579,        nan,        nan, 0.69628579,\n",
       "               nan,        nan, 0.69628579,        nan,        nan,\n",
       "        0.69628579,        nan,        nan, 0.69738863,        nan,\n",
       "               nan, 0.69738863,        nan,        nan, 0.69738863,\n",
       "               nan,        nan, 0.69738863,        nan,        nan,\n",
       "        0.69738863,        nan,        nan, 0.69738863,        nan,\n",
       "               nan, 0.69864902,        nan,        nan, 0.69864902,\n",
       "               nan,        nan, 0.69864902,        nan,        nan,\n",
       "        0.69864902,        nan,        nan, 0.69864902,        nan,\n",
       "               nan, 0.69864902,        nan,        nan, 0.70101225,\n",
       "               nan,        nan, 0.70101225,        nan,        nan,\n",
       "        0.70101225,        nan,        nan, 0.70101225,        nan,\n",
       "               nan, 0.70101225,        nan,        nan, 0.70101225,\n",
       "               nan,        nan, 0.70416322,        nan,        nan,\n",
       "        0.70416322,        nan,        nan, 0.70416322,        nan,\n",
       "               nan, 0.70416322,        nan,        nan, 0.70416322,\n",
       "               nan,        nan, 0.70416322,        nan,        nan,\n",
       "        0.70751113,        nan,        nan, 0.70751113,        nan,\n",
       "               nan, 0.70751113,        nan,        nan, 0.70751113,\n",
       "               nan,        nan, 0.70751113,        nan,        nan,\n",
       "        0.70751113,        nan,        nan, 0.71198813,        nan,\n",
       "               nan, 0.71198813,        nan,        nan, 0.71198813,\n",
       "               nan,        nan, 0.71198813,        nan,        nan,\n",
       "        0.71198813,        nan,        nan, 0.71198813,        nan,\n",
       "               nan, 0.71773865,        nan,        nan, 0.71773865,\n",
       "               nan,        nan, 0.71773865,        nan,        nan,\n",
       "        0.71773865,        nan,        nan, 0.71773865,        nan,\n",
       "               nan, 0.71773865,        nan,        nan, 0.72432943,\n",
       "               nan,        nan, 0.72432943,        nan,        nan,\n",
       "        0.72432943,        nan,        nan, 0.72432943,        nan,\n",
       "               nan, 0.72432943,        nan,        nan, 0.72432943,\n",
       "               nan,        nan, 0.73165544,        nan,        nan,\n",
       "        0.73165544,        nan,        nan, 0.73165544,        nan,\n",
       "               nan, 0.73165544,        nan,        nan, 0.73165544,\n",
       "               nan,        nan, 0.73165544,        nan,        nan,\n",
       "        0.74024184,        nan,        nan, 0.74024184,        nan,\n",
       "               nan, 0.74024184,        nan,        nan, 0.74024184,\n",
       "               nan,        nan, 0.74024184,        nan,        nan,\n",
       "        0.74024184,        nan,        nan, 0.75019365,        nan,\n",
       "               nan, 0.75019365,        nan,        nan, 0.75019365,\n",
       "               nan,        nan, 0.75019365,        nan,        nan,\n",
       "        0.75019365,        nan,        nan, 0.75019365,        nan,\n",
       "               nan, 0.76193102,        nan,        nan, 0.76193102,\n",
       "               nan,        nan, 0.76193102,        nan,        nan,\n",
       "        0.76193102,        nan,        nan, 0.76193102,        nan,\n",
       "               nan, 0.76193102,        nan,        nan, 0.77667494,\n",
       "               nan,        nan, 0.77667494,        nan,        nan,\n",
       "        0.77667494,        nan,        nan, 0.77667494,        nan,\n",
       "               nan, 0.77667494,        nan,        nan, 0.77667494,\n",
       "               nan,        nan, 0.79294183,        nan,        nan,\n",
       "        0.79294183,        nan,        nan, 0.79294183,        nan,\n",
       "               nan, 0.79294183,        nan,        nan, 0.79294183,\n",
       "               nan,        nan, 0.79294183,        nan,        nan,\n",
       "        0.81253036,        nan,        nan, 0.81253036,        nan,\n",
       "               nan, 0.81253036,        nan,        nan, 0.81253036,\n",
       "               nan,        nan, 0.81253036,        nan,        nan,\n",
       "        0.81253036,        nan,        nan, 0.83549306,        nan,\n",
       "               nan, 0.83549306,        nan,        nan, 0.83549306,\n",
       "               nan,        nan, 0.83549306,        nan,        nan,\n",
       "        0.83549306,        nan,        nan, 0.83549306,        nan,\n",
       "               nan, 0.85863957,        nan,        nan, 0.85863957,\n",
       "               nan,        nan, 0.85863957,        nan,        nan,\n",
       "        0.85863957,        nan,        nan, 0.85863957,        nan,\n",
       "               nan, 0.85863957,        nan,        nan, 0.88093269,\n",
       "               nan,        nan, 0.88093269,        nan,        nan,\n",
       "        0.88093269,        nan,        nan, 0.88093269,        nan,\n",
       "               nan, 0.88093269,        nan,        nan, 0.88093269,\n",
       "               nan,        nan, 0.90134835,        nan,        nan,\n",
       "        0.90134835,        nan,        nan, 0.90134835,        nan,\n",
       "               nan, 0.90134835,        nan,        nan, 0.90134835,\n",
       "               nan,        nan, 0.90134835,        nan,        nan,\n",
       "        0.91857366,        nan,        nan, 0.91857366,        nan,\n",
       "               nan, 0.91857366,        nan,        nan, 0.91857366,\n",
       "               nan,        nan, 0.91857366,        nan,        nan,\n",
       "        0.91857366,        nan,        nan, 0.93098061,        nan,\n",
       "               nan, 0.93098061,        nan,        nan, 0.93098061,\n",
       "               nan,        nan, 0.93098061,        nan,        nan,\n",
       "        0.93098061,        nan,        nan, 0.93098061,        nan,\n",
       "               nan, 0.94042039,        nan,        nan, 0.94040726,\n",
       "               nan,        nan, 0.94040726,        nan,        nan,\n",
       "        0.94040726,        nan,        nan, 0.94040726,        nan,\n",
       "               nan, 0.94040726,        nan,        nan, 0.94706369,\n",
       "               nan,        nan, 0.94711621,        nan,        nan,\n",
       "        0.94711621,        nan,        nan, 0.94711621,        nan,\n",
       "               nan, 0.94711621,        nan,        nan, 0.94711621,\n",
       "               nan,        nan, 0.95256476,        nan,        nan,\n",
       "        0.95257789,        nan,        nan, 0.95257789,        nan,\n",
       "               nan, 0.95257789,        nan,        nan, 0.95257789,\n",
       "               nan,        nan, 0.95257789,        nan,        nan,\n",
       "        0.95658225,        nan,        nan, 0.9566085 ,        nan,\n",
       "               nan, 0.95663476,        nan,        nan, 0.9566085 ,\n",
       "               nan,        nan, 0.9566085 ,        nan,        nan,\n",
       "        0.9566085 ,        nan,        nan, 0.95902425,        nan,\n",
       "               nan, 0.95910302,        nan,        nan, 0.95912928,\n",
       "               nan,        nan, 0.95910302,        nan,        nan,\n",
       "        0.95910302,        nan,        nan, 0.95910302,        nan,\n",
       "               nan, 0.96101986,        nan,        nan, 0.96099361,\n",
       "               nan,        nan, 0.96098048,        nan,        nan,\n",
       "        0.96101986,        nan,        nan, 0.96101986,        nan,\n",
       "               nan, 0.96101986,        nan,        nan, 0.9625297 ,\n",
       "               nan,        nan, 0.96239841,        nan,        nan,\n",
       "        0.96242467,        nan,        nan, 0.96238528,        nan,\n",
       "               nan, 0.96238528,        nan,        nan, 0.96238528,\n",
       "               nan,        nan, 0.96360629,        nan,        nan,\n",
       "        0.96351438,        nan,        nan, 0.96348812,        nan,\n",
       "               nan, 0.96344874,        nan,        nan, 0.96344874,\n",
       "               nan,        nan, 0.96344874,        nan,        nan,\n",
       "        0.96431525,        nan,        nan, 0.96452532,        nan,\n",
       "               nan, 0.96419709,        nan,        nan, 0.96427587,\n",
       "               nan,        nan, 0.96427587,        nan,        nan,\n",
       "        0.96427587,        nan,        nan, 0.96553626,        nan,\n",
       "               nan, 0.96501109,        nan,        nan, 0.96495858,\n",
       "               nan,        nan, 0.96497171,        nan,        nan,\n",
       "        0.96497171,        nan,        nan, 0.96497171,        nan,\n",
       "               nan, 0.96482729,        nan,        nan, 0.96554938,\n",
       "               nan,        nan, 0.96551   ,        nan,        nan,\n",
       "        0.96564129,        nan,        nan, 0.96564129,        nan,\n",
       "               nan, 0.96564129,        nan,        nan, 0.96502422,\n",
       "               nan,        nan, 0.96552313,        nan,        nan,\n",
       "        0.96606142,        nan,        nan, 0.9661008 ,        nan,\n",
       "               nan, 0.9661008 ,        nan,        nan, 0.9661008 ,\n",
       "               nan,        nan, 0.96511613,        nan,        nan,\n",
       "        0.96599577,        nan,        nan, 0.96633713,        nan,\n",
       "               nan, 0.96649468,        nan,        nan, 0.96649468,\n",
       "               nan,        nan, 0.96649468,        nan,        nan,\n",
       "        0.96575945,        nan,        nan, 0.96532619,        nan,\n",
       "               nan, 0.9666391 ,        nan,        nan, 0.96675726,\n",
       "               nan,        nan, 0.96675726,        nan,        nan,\n",
       "        0.96675726,        nan,        nan, 0.96564129,        nan,\n",
       "               nan, 0.96596951,        nan,        nan, 0.96675726,\n",
       "               nan,        nan, 0.96694106,        nan,        nan,\n",
       "        0.96694106,        nan,        nan, 0.96694106,        nan,\n",
       "               nan, 0.96583822,        nan,        nan, 0.9656019 ,\n",
       "               nan,        nan, 0.9662321 ,        nan,        nan,\n",
       "        0.96709861,        nan,        nan, 0.96709861,        nan,\n",
       "               nan, 0.96709861,        nan,        nan, 0.96543122,\n",
       "               nan,        nan, 0.96679664,        nan,        nan,\n",
       "        0.96686229,        nan,        nan, 0.96732181,        nan,\n",
       "               nan, 0.96732181,        nan,        nan, 0.96732181,\n",
       "               nan,        nan, 0.96638964,        nan,        nan,\n",
       "        0.9664159 ,        nan,        nan, 0.96598264,        nan,\n",
       "               nan, 0.96741371,        nan,        nan, 0.96741371,\n",
       "               nan,        nan, 0.96741371,        nan,        nan,\n",
       "        0.96602203,        nan,        nan, 0.96470913,        nan,\n",
       "               nan, 0.96572006,        nan,        nan, 0.96750561,\n",
       "               nan,        nan, 0.96750561,        nan,        nan,\n",
       "        0.96750561,        nan,        nan, 0.96442029,        nan,\n",
       "               nan, 0.96775506,        nan,        nan, 0.96649468,\n",
       "               nan,        nan, 0.96767629,        nan,        nan,\n",
       "        0.96767629,        nan,        nan, 0.96767629,        nan,\n",
       "               nan, 0.96452532,        nan,        nan, 0.96627148,\n",
       "               nan,        nan, 0.9662321 ,        nan,        nan,\n",
       "        0.96784697,        nan,        nan, 0.96784697,        nan,\n",
       "               nan, 0.96784697,        nan,        nan, 0.96512926,\n",
       "               nan,        nan, 0.96557564,        nan,        nan,\n",
       "        0.96578571,        nan,        nan, 0.96796513,        nan,\n",
       "               nan, 0.96796513,        nan,        nan, 0.96796513,\n",
       "               nan,        nan, 0.96572006,        nan,        nan,\n",
       "        0.96586448,        nan,        nan, 0.96658658,        nan,\n",
       "               nan, 0.96805703,        nan,        nan, 0.96805703,\n",
       "               nan,        nan, 0.96805703,        nan,        nan,\n",
       "        0.964696  ,        nan,        nan, 0.96493232,        nan,\n",
       "               nan, 0.96640277,        nan,        nan, 0.96810955,\n",
       "               nan,        nan, 0.96810955,        nan,        nan,\n",
       "        0.96810955,        nan,        nan, 0.96572006,        nan,\n",
       "               nan, 0.96544435,        nan,        nan, 0.96590387,\n",
       "               nan,        nan, 0.96818832,        nan,        nan,\n",
       "        0.96818832,        nan,        nan, 0.96818832,        nan]),\n",
       " 'split2_test_score': array([       nan, 0.75498575,        nan,        nan, 0.75498575,\n",
       "               nan,        nan, 0.75498575,        nan,        nan,\n",
       "        0.75498575,        nan,        nan, 0.75498575,        nan,\n",
       "               nan, 0.75498575,        nan,        nan, 0.75591792,\n",
       "               nan,        nan, 0.75591792,        nan,        nan,\n",
       "        0.75591792,        nan,        nan, 0.75591792,        nan,\n",
       "               nan, 0.75591792,        nan,        nan, 0.75591792,\n",
       "               nan,        nan, 0.75698137,        nan,        nan,\n",
       "        0.75698137,        nan,        nan, 0.75698137,        nan,\n",
       "               nan, 0.75698137,        nan,        nan, 0.75698137,\n",
       "               nan,        nan, 0.75698137,        nan,        nan,\n",
       "        0.75838618,        nan,        nan, 0.75838618,        nan,\n",
       "               nan, 0.75838618,        nan,        nan, 0.75838618,\n",
       "               nan,        nan, 0.75838618,        nan,        nan,\n",
       "        0.75838618,        nan,        nan, 0.76060499,        nan,\n",
       "               nan, 0.76060499,        nan,        nan, 0.76060499,\n",
       "               nan,        nan, 0.76060499,        nan,        nan,\n",
       "        0.76060499,        nan,        nan, 0.76060499,        nan,\n",
       "               nan, 0.76325705,        nan,        nan, 0.76325705,\n",
       "               nan,        nan, 0.76325705,        nan,        nan,\n",
       "        0.76325705,        nan,        nan, 0.76325705,        nan,\n",
       "               nan, 0.76325705,        nan,        nan, 0.76644741,\n",
       "               nan,        nan, 0.76644741,        nan,        nan,\n",
       "        0.76644741,        nan,        nan, 0.76644741,        nan,\n",
       "               nan, 0.76644741,        nan,        nan, 0.76644741,\n",
       "               nan,        nan, 0.77017606,        nan,        nan,\n",
       "        0.77017606,        nan,        nan, 0.77017606,        nan,\n",
       "               nan, 0.77017606,        nan,        nan, 0.77017606,\n",
       "               nan,        nan, 0.77017606,        nan,        nan,\n",
       "        0.77431171,        nan,        nan, 0.77431171,        nan,\n",
       "               nan, 0.77431171,        nan,        nan, 0.77431171,\n",
       "               nan,        nan, 0.77431171,        nan,        nan,\n",
       "        0.77431171,        nan,        nan, 0.7775152 ,        nan,\n",
       "               nan, 0.7775152 ,        nan,        nan, 0.7775152 ,\n",
       "               nan,        nan, 0.7775152 ,        nan,        nan,\n",
       "        0.7775152 ,        nan,        nan, 0.7775152 ,        nan,\n",
       "               nan, 0.78162459,        nan,        nan, 0.78162459,\n",
       "               nan,        nan, 0.78162459,        nan,        nan,\n",
       "        0.78162459,        nan,        nan, 0.78162459,        nan,\n",
       "               nan, 0.78162459,        nan,        nan, 0.78755892,\n",
       "               nan,        nan, 0.78755892,        nan,        nan,\n",
       "        0.78755892,        nan,        nan, 0.78755892,        nan,\n",
       "               nan, 0.78755892,        nan,        nan, 0.78755892,\n",
       "               nan,        nan, 0.79466173,        nan,        nan,\n",
       "        0.79466173,        nan,        nan, 0.79466173,        nan,\n",
       "               nan, 0.79466173,        nan,        nan, 0.79466173,\n",
       "               nan,        nan, 0.79466173,        nan,        nan,\n",
       "        0.80330064,        nan,        nan, 0.80330064,        nan,\n",
       "               nan, 0.80330064,        nan,        nan, 0.80330064,\n",
       "               nan,        nan, 0.80330064,        nan,        nan,\n",
       "        0.80330064,        nan,        nan, 0.81347565,        nan,\n",
       "               nan, 0.81347565,        nan,        nan, 0.81347565,\n",
       "               nan,        nan, 0.81347565,        nan,        nan,\n",
       "        0.81347565,        nan,        nan, 0.81347565,        nan,\n",
       "               nan, 0.82447779,        nan,        nan, 0.82447779,\n",
       "               nan,        nan, 0.82447779,        nan,        nan,\n",
       "        0.82447779,        nan,        nan, 0.82447779,        nan,\n",
       "               nan, 0.82447779,        nan,        nan, 0.83842084,\n",
       "               nan,        nan, 0.83842084,        nan,        nan,\n",
       "        0.83842084,        nan,        nan, 0.83842084,        nan,\n",
       "               nan, 0.83842084,        nan,        nan, 0.83842084,\n",
       "               nan,        nan, 0.85477963,        nan,        nan,\n",
       "        0.85477963,        nan,        nan, 0.85477963,        nan,\n",
       "               nan, 0.85477963,        nan,        nan, 0.85477963,\n",
       "               nan,        nan, 0.85477963,        nan,        nan,\n",
       "        0.87250384,        nan,        nan, 0.87250384,        nan,\n",
       "               nan, 0.87250384,        nan,        nan, 0.87250384,\n",
       "               nan,        nan, 0.87250384,        nan,        nan,\n",
       "        0.87250384,        nan,        nan, 0.88961099,        nan,\n",
       "               nan, 0.88961099,        nan,        nan, 0.88961099,\n",
       "               nan,        nan, 0.88961099,        nan,        nan,\n",
       "        0.88961099,        nan,        nan, 0.88961099,        nan,\n",
       "               nan, 0.90713826,        nan,        nan, 0.90713826,\n",
       "               nan,        nan, 0.90713826,        nan,        nan,\n",
       "        0.90713826,        nan,        nan, 0.90713826,        nan,\n",
       "               nan, 0.90713826,        nan,        nan, 0.92480996,\n",
       "               nan,        nan, 0.92480996,        nan,        nan,\n",
       "        0.92480996,        nan,        nan, 0.92480996,        nan,\n",
       "               nan, 0.92480996,        nan,        nan, 0.92480996,\n",
       "               nan,        nan, 0.93841165,        nan,        nan,\n",
       "        0.93841165,        nan,        nan, 0.93841165,        nan,\n",
       "               nan, 0.93841165,        nan,        nan, 0.93841165,\n",
       "               nan,        nan, 0.93841165,        nan,        nan,\n",
       "        0.95024092,        nan,        nan, 0.95024092,        nan,\n",
       "               nan, 0.95024092,        nan,        nan, 0.95024092,\n",
       "               nan,        nan, 0.95024092,        nan,        nan,\n",
       "        0.95024092,        nan,        nan, 0.95945751,        nan,\n",
       "               nan, 0.95948377,        nan,        nan, 0.95948377,\n",
       "               nan,        nan, 0.95948377,        nan,        nan,\n",
       "        0.95948377,        nan,        nan, 0.95948377,        nan,\n",
       "               nan, 0.96540497,        nan,        nan, 0.96541809,\n",
       "               nan,        nan, 0.96541809,        nan,        nan,\n",
       "        0.96541809,        nan,        nan, 0.96541809,        nan,\n",
       "               nan, 0.96541809,        nan,        nan, 0.96977694,\n",
       "               nan,        nan, 0.96976381,        nan,        nan,\n",
       "        0.96976381,        nan,        nan, 0.96976381,        nan,\n",
       "               nan, 0.96976381,        nan,        nan, 0.96976381,\n",
       "               nan,        nan, 0.97307233,        nan,        nan,\n",
       "        0.9731511 ,        nan,        nan, 0.97316423,        nan,\n",
       "               nan, 0.9731511 ,        nan,        nan, 0.9731511 ,\n",
       "               nan,        nan, 0.9731511 ,        nan,        nan,\n",
       "        0.97479223,        nan,        nan, 0.9747791 ,        nan,\n",
       "               nan, 0.9747791 ,        nan,        nan, 0.97475285,\n",
       "               nan,        nan, 0.97475285,        nan,        nan,\n",
       "        0.97475285,        nan,        nan, 0.97621017,        nan,\n",
       "               nan, 0.97611827,        nan,        nan, 0.97614452,\n",
       "               nan,        nan, 0.97614452,        nan,        nan,\n",
       "        0.97614452,        nan,        nan, 0.97614452,        nan,\n",
       "               nan, 0.97697165,        nan,        nan, 0.97681411,\n",
       "               nan,        nan, 0.97684036,        nan,        nan,\n",
       "        0.97681411,        nan,        nan, 0.97681411,        nan,\n",
       "               nan, 0.97681411,        nan,        nan, 0.9769454 ,\n",
       "               nan,        nan, 0.97714233,        nan,        nan,\n",
       "        0.97728675,        nan,        nan, 0.97729988,        nan,\n",
       "               nan, 0.97729988,        nan,        nan, 0.97729988,\n",
       "               nan,        nan, 0.97750995,        nan,        nan,\n",
       "        0.97731301,        nan,        nan, 0.97733927,        nan,\n",
       "               nan, 0.9774443 ,        nan,        nan, 0.9774443 ,\n",
       "               nan,        nan, 0.9774443 ,        nan,        nan,\n",
       "        0.9779432 ,        nan,        nan, 0.97783817,        nan,\n",
       "               nan, 0.97757559,        nan,        nan, 0.97774627,\n",
       "               nan,        nan, 0.97774627,        nan,        nan,\n",
       "        0.97774627,        nan,        nan, 0.97778566,        nan,\n",
       "               nan, 0.97810075,        nan,        nan, 0.97798259,\n",
       "               nan,        nan, 0.97803511,        nan,        nan,\n",
       "        0.97803511,        nan,        nan, 0.97803511,        nan,\n",
       "               nan, 0.97769375,        nan,        nan, 0.97790382,\n",
       "               nan,        nan, 0.97800885,        nan,        nan,\n",
       "        0.97824517,        nan,        nan, 0.97824517,        nan,\n",
       "               nan, 0.97824517,        nan,        nan, 0.97827143,\n",
       "               nan,        nan, 0.97803511,        nan,        nan,\n",
       "        0.97769375,        nan,        nan, 0.97832395,        nan,\n",
       "               nan, 0.97832395,        nan,        nan, 0.97832395,\n",
       "               nan,        nan, 0.97856027,        nan,        nan,\n",
       "        0.97772001,        nan,        nan, 0.97819266,        nan,\n",
       "               nan, 0.97844211,        nan,        nan, 0.97844211,\n",
       "               nan,        nan, 0.97844211,        nan,        nan,\n",
       "        0.97862591,        nan,        nan, 0.97789069,        nan,\n",
       "               nan, 0.97807449,        nan,        nan, 0.97849462,\n",
       "               nan,        nan, 0.97849462,        nan,        nan,\n",
       "        0.97849462,        nan,        nan, 0.97869156,        nan,\n",
       "               nan, 0.97863904,        nan,        nan, 0.97824517,\n",
       "               nan,        nan, 0.97854714,        nan,        nan,\n",
       "        0.97854714,        nan,        nan, 0.97854714,        nan,\n",
       "               nan, 0.97865217,        nan,        nan, 0.97819266,\n",
       "               nan,        nan, 0.97824517,        nan,        nan,\n",
       "        0.97856027,        nan,        nan, 0.97856027,        nan,\n",
       "               nan, 0.97856027,        nan,        nan, 0.97795633,\n",
       "               nan,        nan, 0.97896727,        nan,        nan,\n",
       "        0.97858653,        nan,        nan, 0.97856027,        nan,\n",
       "               nan, 0.97856027,        nan,        nan, 0.97856027,\n",
       "               nan,        nan, 0.97895414,        nan,        nan,\n",
       "        0.97940053,        nan,        nan, 0.97891475,        nan,\n",
       "               nan, 0.97869156,        nan,        nan, 0.97869156,\n",
       "               nan,        nan, 0.97869156,        nan,        nan,\n",
       "        0.97800885,        nan,        nan, 0.97869156,        nan,\n",
       "               nan, 0.97807449,        nan,        nan, 0.97874408,\n",
       "               nan,        nan, 0.97874408,        nan,        nan,\n",
       "        0.97874408,        nan,        nan, 0.97997821,        nan,\n",
       "               nan, 0.97856027,        nan,        nan, 0.97787756,\n",
       "               nan,        nan, 0.97870469,        nan,        nan,\n",
       "        0.97870469,        nan,        nan, 0.97870469,        nan,\n",
       "               nan, 0.97964998,        nan,        nan, 0.97940053,\n",
       "               nan,        nan, 0.97849462,        nan,        nan,\n",
       "        0.97871782,        nan,        nan, 0.97871782,        nan,\n",
       "               nan, 0.97871782,        nan,        nan, 0.98054275,\n",
       "               nan,        nan, 0.97800885,        nan,        nan,\n",
       "        0.97850775,        nan,        nan, 0.9787572 ,        nan,\n",
       "               nan, 0.9787572 ,        nan,        nan, 0.9787572 ,\n",
       "               nan,        nan, 0.97984692,        nan,        nan,\n",
       "        0.97984692,        nan,        nan, 0.97880972,        nan,\n",
       "               nan, 0.97878346,        nan,        nan, 0.97878346,\n",
       "               nan,        nan, 0.97878346,        nan,        nan,\n",
       "        0.98009637,        nan,        nan, 0.98042459,        nan,\n",
       "               nan, 0.98021453,        nan,        nan, 0.97882285,\n",
       "               nan,        nan, 0.97882285,        nan,        nan,\n",
       "        0.97882285,        nan,        nan, 0.97964998,        nan,\n",
       "               nan, 0.98003072,        nan,        nan, 0.97972875,\n",
       "               nan,        nan, 0.97879659,        nan,        nan,\n",
       "        0.97879659,        nan,        nan, 0.97879659,        nan]),\n",
       " 'split3_test_score': array([       nan, 0.71500781,        nan,        nan, 0.71500781,\n",
       "               nan,        nan, 0.71500781,        nan,        nan,\n",
       "        0.71500781,        nan,        nan, 0.71500781,        nan,\n",
       "               nan, 0.71500781,        nan,        nan, 0.71546733,\n",
       "               nan,        nan, 0.71546733,        nan,        nan,\n",
       "        0.71546733,        nan,        nan, 0.71546733,        nan,\n",
       "               nan, 0.71546733,        nan,        nan, 0.71546733,\n",
       "               nan,        nan, 0.71603188,        nan,        nan,\n",
       "        0.71603188,        nan,        nan, 0.71603188,        nan,\n",
       "               nan, 0.71603188,        nan,        nan, 0.71603188,\n",
       "               nan,        nan, 0.71603188,        nan,        nan,\n",
       "        0.7174892 ,        nan,        nan, 0.7174892 ,        nan,\n",
       "               nan, 0.7174892 ,        nan,        nan, 0.7174892 ,\n",
       "               nan,        nan, 0.7174892 ,        nan,        nan,\n",
       "        0.7174892 ,        nan,        nan, 0.71955046,        nan,\n",
       "               nan, 0.71955046,        nan,        nan, 0.71955046,\n",
       "               nan,        nan, 0.71955046,        nan,        nan,\n",
       "        0.71955046,        nan,        nan, 0.71955046,        nan,\n",
       "               nan, 0.72151982,        nan,        nan, 0.72151982,\n",
       "               nan,        nan, 0.72151982,        nan,        nan,\n",
       "        0.72151982,        nan,        nan, 0.72151982,        nan,\n",
       "               nan, 0.72151982,        nan,        nan, 0.72317408,\n",
       "               nan,        nan, 0.72317408,        nan,        nan,\n",
       "        0.72317408,        nan,        nan, 0.72317408,        nan,\n",
       "               nan, 0.72317408,        nan,        nan, 0.72317408,\n",
       "               nan,        nan, 0.72560295,        nan,        nan,\n",
       "        0.72560295,        nan,        nan, 0.72560295,        nan,\n",
       "               nan, 0.72560295,        nan,        nan, 0.72560295,\n",
       "               nan,        nan, 0.72560295,        nan,        nan,\n",
       "        0.7277036 ,        nan,        nan, 0.7277036 ,        nan,\n",
       "               nan, 0.7277036 ,        nan,        nan, 0.7277036 ,\n",
       "               nan,        nan, 0.7277036 ,        nan,        nan,\n",
       "        0.7277036 ,        nan,        nan, 0.7312747 ,        nan,\n",
       "               nan, 0.7312747 ,        nan,        nan, 0.7312747 ,\n",
       "               nan,        nan, 0.7312747 ,        nan,        nan,\n",
       "        0.7312747 ,        nan,        nan, 0.7312747 ,        nan,\n",
       "               nan, 0.7364738 ,        nan,        nan, 0.7364738 ,\n",
       "               nan,        nan, 0.7364738 ,        nan,        nan,\n",
       "        0.7364738 ,        nan,        nan, 0.7364738 ,        nan,\n",
       "               nan, 0.7364738 ,        nan,        nan, 0.74253942,\n",
       "               nan,        nan, 0.74253942,        nan,        nan,\n",
       "        0.74253942,        nan,        nan, 0.74253942,        nan,\n",
       "               nan, 0.74253942,        nan,        nan, 0.74253942,\n",
       "               nan,        nan, 0.75042998,        nan,        nan,\n",
       "        0.75042998,        nan,        nan, 0.75042998,        nan,\n",
       "               nan, 0.75042998,        nan,        nan, 0.75042998,\n",
       "               nan,        nan, 0.75042998,        nan,        nan,\n",
       "        0.75855686,        nan,        nan, 0.75855686,        nan,\n",
       "               nan, 0.75855686,        nan,        nan, 0.75855686,\n",
       "               nan,        nan, 0.75855686,        nan,        nan,\n",
       "        0.75855686,        nan,        nan, 0.76724828,        nan,\n",
       "               nan, 0.76724828,        nan,        nan, 0.76724828,\n",
       "               nan,        nan, 0.76724828,        nan,        nan,\n",
       "        0.76724828,        nan,        nan, 0.76724828,        nan,\n",
       "               nan, 0.778513  ,        nan,        nan, 0.778513  ,\n",
       "               nan,        nan, 0.778513  ,        nan,        nan,\n",
       "        0.778513  ,        nan,        nan, 0.778513  ,        nan,\n",
       "               nan, 0.778513  ,        nan,        nan, 0.79324379,\n",
       "               nan,        nan, 0.79324379,        nan,        nan,\n",
       "        0.79324379,        nan,        nan, 0.79324379,        nan,\n",
       "               nan, 0.79324379,        nan,        nan, 0.79324379,\n",
       "               nan,        nan, 0.80987829,        nan,        nan,\n",
       "        0.80987829,        nan,        nan, 0.80987829,        nan,\n",
       "               nan, 0.80987829,        nan,        nan, 0.80987829,\n",
       "               nan,        nan, 0.80987829,        nan,        nan,\n",
       "        0.83008389,        nan,        nan, 0.83008389,        nan,\n",
       "               nan, 0.83008389,        nan,        nan, 0.83008389,\n",
       "               nan,        nan, 0.83008389,        nan,        nan,\n",
       "        0.83008389,        nan,        nan, 0.85225885,        nan,\n",
       "               nan, 0.85225885,        nan,        nan, 0.85225885,\n",
       "               nan,        nan, 0.85225885,        nan,        nan,\n",
       "        0.85225885,        nan,        nan, 0.85225885,        nan,\n",
       "               nan, 0.87414497,        nan,        nan, 0.87414497,\n",
       "               nan,        nan, 0.87414497,        nan,        nan,\n",
       "        0.87414497,        nan,        nan, 0.87414497,        nan,\n",
       "               nan, 0.87414497,        nan,        nan, 0.89583415,\n",
       "               nan,        nan, 0.89583415,        nan,        nan,\n",
       "        0.89583415,        nan,        nan, 0.89583415,        nan,\n",
       "               nan, 0.89583415,        nan,        nan, 0.89583415,\n",
       "               nan,        nan, 0.91418856,        nan,        nan,\n",
       "        0.91418856,        nan,        nan, 0.91418856,        nan,\n",
       "               nan, 0.91418856,        nan,        nan, 0.91418856,\n",
       "               nan,        nan, 0.91418856,        nan,        nan,\n",
       "        0.92964145,        nan,        nan, 0.92964145,        nan,\n",
       "               nan, 0.92964145,        nan,        nan, 0.92964145,\n",
       "               nan,        nan, 0.92964145,        nan,        nan,\n",
       "        0.92964145,        nan,        nan, 0.94090617,        nan,\n",
       "               nan, 0.94090617,        nan,        nan, 0.94090617,\n",
       "               nan,        nan, 0.94090617,        nan,        nan,\n",
       "        0.94090617,        nan,        nan, 0.94090617,        nan,\n",
       "               nan, 0.9490593 ,        nan,        nan, 0.9490593 ,\n",
       "               nan,        nan, 0.9490593 ,        nan,        nan,\n",
       "        0.9490593 ,        nan,        nan, 0.9490593 ,        nan,\n",
       "               nan, 0.9490593 ,        nan,        nan, 0.95516431,\n",
       "               nan,        nan, 0.95517744,        nan,        nan,\n",
       "        0.95517744,        nan,        nan, 0.95517744,        nan,\n",
       "               nan, 0.95517744,        nan,        nan, 0.95517744,\n",
       "               nan,        nan, 0.95881418,        nan,        nan,\n",
       "        0.95880106,        nan,        nan, 0.95880106,        nan,\n",
       "               nan, 0.95880106,        nan,        nan, 0.95880106,\n",
       "               nan,        nan, 0.95880106,        nan,        nan,\n",
       "        0.96141374,        nan,        nan, 0.96149251,        nan,\n",
       "               nan, 0.96146625,        nan,        nan, 0.96146625,\n",
       "               nan,        nan, 0.96146625,        nan,        nan,\n",
       "        0.96146625,        nan,        nan, 0.96343561,        nan,\n",
       "               nan, 0.96339622,        nan,        nan, 0.96339622,\n",
       "               nan,        nan, 0.96340935,        nan,        nan,\n",
       "        0.96340935,        nan,        nan, 0.96340935,        nan,\n",
       "               nan, 0.96465661,        nan,        nan, 0.96472226,\n",
       "               nan,        nan, 0.964696  ,        nan,        nan,\n",
       "        0.96476164,        nan,        nan, 0.96476164,        nan,\n",
       "               nan, 0.96476164,        nan,        nan, 0.96495858,\n",
       "               nan,        nan, 0.96518177,        nan,        nan,\n",
       "        0.96524742,        nan,        nan, 0.96531306,        nan,\n",
       "               nan, 0.96531306,        nan,        nan, 0.96531306,\n",
       "               nan,        nan, 0.96537871,        nan,        nan,\n",
       "        0.96564129,        nan,        nan, 0.96586448,        nan,\n",
       "               nan, 0.96595639,        nan,        nan, 0.96595639,\n",
       "               nan,        nan, 0.96595639,        nan,        nan,\n",
       "        0.96640277,        nan,        nan, 0.9661008 ,        nan,\n",
       "               nan, 0.96637651,        nan,        nan, 0.96640277,\n",
       "               nan,        nan, 0.96640277,        nan,        nan,\n",
       "        0.96640277,        nan,        nan, 0.96642903,        nan,\n",
       "               nan, 0.96658658,        nan,        nan, 0.96645529,\n",
       "               nan,        nan, 0.96690168,        nan,        nan,\n",
       "        0.96690168,        nan,        nan, 0.96690168,        nan,\n",
       "               nan, 0.96602203,        nan,        nan, 0.96674413,\n",
       "               nan,        nan, 0.96679664,        nan,        nan,\n",
       "        0.96708548,        nan,        nan, 0.96708548,        nan,\n",
       "               nan, 0.96708548,        nan,        nan, 0.96709861,\n",
       "               nan,        nan, 0.96691481,        nan,        nan,\n",
       "        0.96671787,        nan,        nan, 0.96726929,        nan,\n",
       "               nan, 0.96726929,        nan,        nan, 0.96726929,\n",
       "               nan,        nan, 0.96677039,        nan,        nan,\n",
       "        0.96683603,        nan,        nan, 0.966731  ,        nan,\n",
       "               nan, 0.96733494,        nan,        nan, 0.96733494,\n",
       "               nan,        nan, 0.96733494,        nan,        nan,\n",
       "        0.96740058,        nan,        nan, 0.96703297,        nan,\n",
       "               nan, 0.96721677,        nan,        nan, 0.96730868,\n",
       "               nan,        nan, 0.96730868,        nan,        nan,\n",
       "        0.96730868,        nan,        nan, 0.96684916,        nan,\n",
       "               nan, 0.96667848,        nan,        nan, 0.96666535,\n",
       "               nan,        nan, 0.96726929,        nan,        nan,\n",
       "        0.96726929,        nan,        nan, 0.96726929,        nan,\n",
       "               nan, 0.96678352,        nan,        nan, 0.96687542,\n",
       "               nan,        nan, 0.96665222,        nan,        nan,\n",
       "        0.96741371,        nan,        nan, 0.96741371,        nan,\n",
       "               nan, 0.96741371,        nan,        nan, 0.96703297,\n",
       "               nan,        nan, 0.96692793,        nan,        nan,\n",
       "        0.96696732,        nan,        nan, 0.967545  ,        nan,\n",
       "               nan, 0.967545  ,        nan,        nan, 0.967545  ,\n",
       "               nan,        nan, 0.96711174,        nan,        nan,\n",
       "        0.967138  ,        nan,        nan, 0.96677039,        nan,\n",
       "               nan, 0.96770255,        nan,        nan, 0.96770255,\n",
       "               nan,        nan, 0.96770255,        nan,        nan,\n",
       "        0.96633713,        nan,        nan, 0.96638964,        nan,\n",
       "               nan, 0.96675726,        nan,        nan, 0.96765003,\n",
       "               nan,        nan, 0.96765003,        nan,        nan,\n",
       "        0.96765003,        nan,        nan, 0.96684916,        nan,\n",
       "               nan, 0.96715113,        nan,        nan, 0.96709861,\n",
       "               nan,        nan, 0.96765003,        nan,        nan,\n",
       "        0.96765003,        nan,        nan, 0.96765003,        nan,\n",
       "               nan, 0.96566755,        nan,        nan, 0.96628461,\n",
       "               nan,        nan, 0.96652093,        nan,        nan,\n",
       "        0.96753187,        nan,        nan, 0.96753187,        nan,\n",
       "               nan, 0.96753187,        nan,        nan, 0.96598264,\n",
       "               nan,        nan, 0.96699358,        nan,        nan,\n",
       "        0.96627148,        nan,        nan, 0.96759752,        nan,\n",
       "               nan, 0.96759752,        nan,        nan, 0.96759752,\n",
       "               nan,        nan, 0.96721677,        nan,        nan,\n",
       "        0.9666391 ,        nan,        nan, 0.96662597,        nan,\n",
       "               nan, 0.96767629,        nan,        nan, 0.96767629,\n",
       "               nan,        nan, 0.96767629,        nan,        nan,\n",
       "        0.96696732,        nan,        nan, 0.96721677,        nan,\n",
       "               nan, 0.96683603,        nan,        nan, 0.96768942,\n",
       "               nan,        nan, 0.96768942,        nan,        nan,\n",
       "        0.96768942,        nan,        nan, 0.96729555,        nan,\n",
       "               nan, 0.96658658,        nan,        nan, 0.96635026,\n",
       "               nan,        nan, 0.96767629,        nan,        nan,\n",
       "        0.96767629,        nan,        nan, 0.96767629,        nan]),\n",
       " 'split4_test_score': array([       nan, 0.71944543,        nan,        nan, 0.71944543,\n",
       "               nan,        nan, 0.71944543,        nan,        nan,\n",
       "        0.71944543,        nan,        nan, 0.71944543,        nan,\n",
       "               nan, 0.71944543,        nan,        nan, 0.71999685,\n",
       "               nan,        nan, 0.71999685,        nan,        nan,\n",
       "        0.71999685,        nan,        nan, 0.71999685,        nan,\n",
       "               nan, 0.71999685,        nan,        nan, 0.71999685,\n",
       "               nan,        nan, 0.72057453,        nan,        nan,\n",
       "        0.72057453,        nan,        nan, 0.72057453,        nan,\n",
       "               nan, 0.72057453,        nan,        nan, 0.72057453,\n",
       "               nan,        nan, 0.72057453,        nan,        nan,\n",
       "        0.72140166,        nan,        nan, 0.72140166,        nan,\n",
       "               nan, 0.72140166,        nan,        nan, 0.72140166,\n",
       "               nan,        nan, 0.72140166,        nan,        nan,\n",
       "        0.72140166,        nan,        nan, 0.72204498,        nan,\n",
       "               nan, 0.72204498,        nan,        nan, 0.72204498,\n",
       "               nan,        nan, 0.72204498,        nan,        nan,\n",
       "        0.72204498,        nan,        nan, 0.72204498,        nan,\n",
       "               nan, 0.72339727,        nan,        nan, 0.72339727,\n",
       "               nan,        nan, 0.72339727,        nan,        nan,\n",
       "        0.72339727,        nan,        nan, 0.72339727,        nan,\n",
       "               nan, 0.72339727,        nan,        nan, 0.72388305,\n",
       "               nan,        nan, 0.72388305,        nan,        nan,\n",
       "        0.72388305,        nan,        nan, 0.72388305,        nan,\n",
       "               nan, 0.72388305,        nan,        nan, 0.72388305,\n",
       "               nan,        nan, 0.72531411,        nan,        nan,\n",
       "        0.72531411,        nan,        nan, 0.72531411,        nan,\n",
       "               nan, 0.72531411,        nan,        nan, 0.72531411,\n",
       "               nan,        nan, 0.72531411,        nan,        nan,\n",
       "        0.72745415,        nan,        nan, 0.72745415,        nan,\n",
       "               nan, 0.72745415,        nan,        nan, 0.72745415,\n",
       "               nan,        nan, 0.72745415,        nan,        nan,\n",
       "        0.72745415,        nan,        nan, 0.72937099,        nan,\n",
       "               nan, 0.72937099,        nan,        nan, 0.72937099,\n",
       "               nan,        nan, 0.72937099,        nan,        nan,\n",
       "        0.72937099,        nan,        nan, 0.72937099,        nan,\n",
       "               nan, 0.73256135,        nan,        nan, 0.73256135,\n",
       "               nan,        nan, 0.73256135,        nan,        nan,\n",
       "        0.73256135,        nan,        nan, 0.73256135,        nan,\n",
       "               nan, 0.73256135,        nan,        nan, 0.73684141,\n",
       "               nan,        nan, 0.73684141,        nan,        nan,\n",
       "        0.73684141,        nan,        nan, 0.73684141,        nan,\n",
       "               nan, 0.73684141,        nan,        nan, 0.73684141,\n",
       "               nan,        nan, 0.74253942,        nan,        nan,\n",
       "        0.74253942,        nan,        nan, 0.74253942,        nan,\n",
       "               nan, 0.74253942,        nan,        nan, 0.74253942,\n",
       "               nan,        nan, 0.74253942,        nan,        nan,\n",
       "        0.7492221 ,        nan,        nan, 0.7492221 ,        nan,\n",
       "               nan, 0.7492221 ,        nan,        nan, 0.7492221 ,\n",
       "               nan,        nan, 0.7492221 ,        nan,        nan,\n",
       "        0.7492221 ,        nan,        nan, 0.75864876,        nan,\n",
       "               nan, 0.75864876,        nan,        nan, 0.75864876,\n",
       "               nan,        nan, 0.75864876,        nan,        nan,\n",
       "        0.75864876,        nan,        nan, 0.75864876,        nan,\n",
       "               nan, 0.77105571,        nan,        nan, 0.77105571,\n",
       "               nan,        nan, 0.77105571,        nan,        nan,\n",
       "        0.77105571,        nan,        nan, 0.77105571,        nan,\n",
       "               nan, 0.77105571,        nan,        nan, 0.78620662,\n",
       "               nan,        nan, 0.78620662,        nan,        nan,\n",
       "        0.78620662,        nan,        nan, 0.78620662,        nan,\n",
       "               nan, 0.78620662,        nan,        nan, 0.78620662,\n",
       "               nan,        nan, 0.80381267,        nan,        nan,\n",
       "        0.80381267,        nan,        nan, 0.80381267,        nan,\n",
       "               nan, 0.80381267,        nan,        nan, 0.80381267,\n",
       "               nan,        nan, 0.80381267,        nan,        nan,\n",
       "        0.82397889,        nan,        nan, 0.82397889,        nan,\n",
       "               nan, 0.82397889,        nan,        nan, 0.82397889,\n",
       "               nan,        nan, 0.82397889,        nan,        nan,\n",
       "        0.82397889,        nan,        nan, 0.84654772,        nan,\n",
       "               nan, 0.84654772,        nan,        nan, 0.84654772,\n",
       "               nan,        nan, 0.84654772,        nan,        nan,\n",
       "        0.84654772,        nan,        nan, 0.84654772,        nan,\n",
       "               nan, 0.86944477,        nan,        nan, 0.86944477,\n",
       "               nan,        nan, 0.86944477,        nan,        nan,\n",
       "        0.86944477,        nan,        nan, 0.86944477,        nan,\n",
       "               nan, 0.86944477,        nan,        nan, 0.89071383,\n",
       "               nan,        nan, 0.89071383,        nan,        nan,\n",
       "        0.89071383,        nan,        nan, 0.89071383,        nan,\n",
       "               nan, 0.89071383,        nan,        nan, 0.89071383,\n",
       "               nan,        nan, 0.90902884,        nan,        nan,\n",
       "        0.90902884,        nan,        nan, 0.90902884,        nan,\n",
       "               nan, 0.90902884,        nan,        nan, 0.90902884,\n",
       "               nan,        nan, 0.90902884,        nan,        nan,\n",
       "        0.9240616 ,        nan,        nan, 0.9240616 ,        nan,\n",
       "               nan, 0.9240616 ,        nan,        nan, 0.9240616 ,\n",
       "               nan,        nan, 0.9240616 ,        nan,        nan,\n",
       "        0.9240616 ,        nan,        nan, 0.93644229,        nan,\n",
       "               nan, 0.93644229,        nan,        nan, 0.93644229,\n",
       "               nan,        nan, 0.93644229,        nan,        nan,\n",
       "        0.93644229,        nan,        nan, 0.93644229,        nan,\n",
       "               nan, 0.94664356,        nan,        nan, 0.94665669,\n",
       "               nan,        nan, 0.94665669,        nan,        nan,\n",
       "        0.94665669,        nan,        nan, 0.94665669,        nan,\n",
       "               nan, 0.94665669,        nan,        nan, 0.95389079,\n",
       "               nan,        nan, 0.95389079,        nan,        nan,\n",
       "        0.95389079,        nan,        nan, 0.95389079,        nan,\n",
       "               nan, 0.95389079,        nan,        nan, 0.95389079,\n",
       "               nan,        nan, 0.95903738,        nan,        nan,\n",
       "        0.95902425,        nan,        nan, 0.95902425,        nan,\n",
       "               nan, 0.95901112,        nan,        nan, 0.95901112,\n",
       "               nan,        nan, 0.95901112,        nan,        nan,\n",
       "        0.96246406,        nan,        nan, 0.96247719,        nan,\n",
       "               nan, 0.96247719,        nan,        nan, 0.96247719,\n",
       "               nan,        nan, 0.96247719,        nan,        nan,\n",
       "        0.96247719,        nan,        nan, 0.96453845,        nan,\n",
       "               nan, 0.96463035,        nan,        nan, 0.96463035,\n",
       "               nan,        nan, 0.96464348,        nan,        nan,\n",
       "        0.96464348,        nan,        nan, 0.96464348,        nan,\n",
       "               nan, 0.96624522,        nan,        nan, 0.96611393,\n",
       "               nan,        nan, 0.96611393,        nan,        nan,\n",
       "        0.9661008 ,        nan,        nan, 0.9661008 ,        nan,\n",
       "               nan, 0.9661008 ,        nan,        nan, 0.96725616,\n",
       "               nan,        nan, 0.96724303,        nan,        nan,\n",
       "        0.96726929,        nan,        nan, 0.96724303,        nan,\n",
       "               nan, 0.96724303,        nan,        nan, 0.96724303,\n",
       "               nan,        nan, 0.96831961,        nan,        nan,\n",
       "        0.96829336,        nan,        nan, 0.96824084,        nan,\n",
       "               nan, 0.96812268,        nan,        nan, 0.96812268,\n",
       "               nan,        nan, 0.96812268,        nan,        nan,\n",
       "        0.96900232,        nan,        nan, 0.96889729,        nan,\n",
       "               nan, 0.96883165,        nan,        nan, 0.96889729,\n",
       "               nan,        nan, 0.96889729,        nan,        nan,\n",
       "        0.96889729,        nan,        nan, 0.9693962 ,        nan,\n",
       "               nan, 0.96952749,        nan,        nan, 0.96955374,\n",
       "               nan,        nan, 0.96946184,        nan,        nan,\n",
       "        0.96946184,        nan,        nan, 0.96946184,        nan,\n",
       "               nan, 0.969987  ,        nan,        nan, 0.97000013,\n",
       "               nan,        nan, 0.96994762,        nan,        nan,\n",
       "        0.96982945,        nan,        nan, 0.96982945,        nan,\n",
       "               nan, 0.96982945,        nan,        nan, 0.970394  ,\n",
       "               nan,        nan, 0.97053842,        nan,        nan,\n",
       "        0.97003952,        nan,        nan, 0.97002639,        nan,\n",
       "               nan, 0.97002639,        nan,        nan, 0.97002639,\n",
       "               nan,        nan, 0.97049904,        nan,        nan,\n",
       "        0.97026271,        nan,        nan, 0.97069597,        nan,\n",
       "               nan, 0.97026271,        nan,        nan, 0.97026271,\n",
       "               nan,        nan, 0.97026271,        nan,        nan,\n",
       "        0.97048591,        nan,        nan, 0.97110297,        nan,\n",
       "               nan, 0.97089291,        nan,        nan, 0.97043339,\n",
       "               nan,        nan, 0.97043339,        nan,        nan,\n",
       "        0.97043339,        nan,        nan, 0.96833274,        nan,\n",
       "               nan, 0.97064345,        nan,        nan, 0.97118175,\n",
       "               nan,        nan, 0.97065658,        nan,        nan,\n",
       "        0.97065658,        nan,        nan, 0.97065658,        nan,\n",
       "               nan, 0.9703021 ,        nan,        nan, 0.971208  ,\n",
       "               nan,        nan, 0.97105045,        nan,        nan,\n",
       "        0.970801  ,        nan,        nan, 0.970801  ,        nan,\n",
       "               nan, 0.970801  ,        nan,        nan, 0.97099794,\n",
       "               nan,        nan, 0.97068284,        nan,        nan,\n",
       "        0.97072223,        nan,        nan, 0.97090604,        nan,\n",
       "               nan, 0.97090604,        nan,        nan, 0.97090604,\n",
       "               nan,        nan, 0.96996074,        nan,        nan,\n",
       "        0.97059094,        nan,        nan, 0.97157562,        nan,\n",
       "               nan, 0.97086665,        nan,        nan, 0.97086665,\n",
       "               nan,        nan, 0.97086665,        nan,        nan,\n",
       "        0.96915987,        nan,        nan, 0.97007891,        nan,\n",
       "               nan, 0.97065658,        nan,        nan, 0.97085352,\n",
       "               nan,        nan, 0.97085352,        nan,        nan,\n",
       "        0.97085352,        nan,        nan, 0.96956687,        nan,\n",
       "               nan, 0.97056468,        nan,        nan, 0.97157562,\n",
       "               nan,        nan, 0.97091916,        nan,        nan,\n",
       "        0.97091916,        nan,        nan, 0.97091916,        nan,\n",
       "               nan, 0.96993449,        nan,        nan, 0.97090604,\n",
       "               nan,        nan, 0.97063033,        nan,        nan,\n",
       "        0.97094542,        nan,        nan, 0.97094542,        nan,\n",
       "               nan, 0.97094542,        nan,        nan, 0.96843777,\n",
       "               nan,        nan, 0.9694881 ,        nan,        nan,\n",
       "        0.97112923,        nan,        nan, 0.97095855,        nan,\n",
       "               nan, 0.97095855,        nan,        nan, 0.97095855,\n",
       "               nan,        nan, 0.96855594,        nan,        nan,\n",
       "        0.97018394,        nan,        nan, 0.97000013,        nan,\n",
       "               nan, 0.97103733,        nan,        nan, 0.97103733,\n",
       "               nan,        nan, 0.97103733,        nan,        nan,\n",
       "        0.96898919,        nan,        nan, 0.96918613,        nan,\n",
       "               nan, 0.97006578,        nan,        nan, 0.97103733,\n",
       "               nan,        nan, 0.97103733,        nan,        nan,\n",
       "        0.97103733,        nan,        nan, 0.96746623,        nan,\n",
       "               nan, 0.969173  ,        nan,        nan, 0.97038087,\n",
       "               nan,        nan, 0.97108984,        nan,        nan,\n",
       "        0.97108984,        nan,        nan, 0.97108984,        nan]),\n",
       " 'mean_test_score': array([       nan, 0.7180791 ,        nan,        nan, 0.7180791 ,\n",
       "               nan,        nan, 0.7180791 ,        nan,        nan,\n",
       "        0.7180791 ,        nan,        nan, 0.7180791 ,        nan,\n",
       "               nan, 0.7180791 ,        nan,        nan, 0.71879279,\n",
       "               nan,        nan, 0.71879279,        nan,        nan,\n",
       "        0.71879279,        nan,        nan, 0.71879279,        nan,\n",
       "               nan, 0.71879279,        nan,        nan, 0.71879279,\n",
       "               nan,        nan, 0.71947278,        nan,        nan,\n",
       "        0.71947278,        nan,        nan, 0.71947278,        nan,\n",
       "               nan, 0.71947278,        nan,        nan, 0.71947278,\n",
       "               nan,        nan, 0.71947278,        nan,        nan,\n",
       "        0.72063303,        nan,        nan, 0.72063303,        nan,\n",
       "               nan, 0.72063303,        nan,        nan, 0.72063303,\n",
       "               nan,        nan, 0.72063303,        nan,        nan,\n",
       "        0.72063303,        nan,        nan, 0.72188537,        nan,\n",
       "               nan, 0.72188537,        nan,        nan, 0.72188537,\n",
       "               nan,        nan, 0.72188537,        nan,        nan,\n",
       "        0.72188537,        nan,        nan, 0.72188537,        nan,\n",
       "               nan, 0.72366186,        nan,        nan, 0.72366186,\n",
       "               nan,        nan, 0.72366186,        nan,        nan,\n",
       "        0.72366186,        nan,        nan, 0.72366186,        nan,\n",
       "               nan, 0.72366186,        nan,        nan, 0.72516395,\n",
       "               nan,        nan, 0.72516395,        nan,        nan,\n",
       "        0.72516395,        nan,        nan, 0.72516395,        nan,\n",
       "               nan, 0.72516395,        nan,        nan, 0.72516395,\n",
       "               nan,        nan, 0.72740082,        nan,        nan,\n",
       "        0.72740082,        nan,        nan, 0.72740082,        nan,\n",
       "               nan, 0.72740082,        nan,        nan, 0.72740082,\n",
       "               nan,        nan, 0.72740082,        nan,        nan,\n",
       "        0.72991836,        nan,        nan, 0.72991836,        nan,\n",
       "               nan, 0.72991836,        nan,        nan, 0.72991836,\n",
       "               nan,        nan, 0.72991836,        nan,        nan,\n",
       "        0.72991836,        nan,        nan, 0.73301777,        nan,\n",
       "               nan, 0.73301777,        nan,        nan, 0.73301777,\n",
       "               nan,        nan, 0.73301777,        nan,        nan,\n",
       "        0.73301777,        nan,        nan, 0.73301777,        nan,\n",
       "               nan, 0.73719093,        nan,        nan, 0.73719093,\n",
       "               nan,        nan, 0.73719093,        nan,        nan,\n",
       "        0.73719093,        nan,        nan, 0.73719093,        nan,\n",
       "               nan, 0.73719093,        nan,        nan, 0.74263374,\n",
       "               nan,        nan, 0.74263374,        nan,        nan,\n",
       "        0.74263374,        nan,        nan, 0.74263374,        nan,\n",
       "               nan, 0.74263374,        nan,        nan, 0.74263374,\n",
       "               nan,        nan, 0.74922361,        nan,        nan,\n",
       "        0.74922361,        nan,        nan, 0.74922361,        nan,\n",
       "               nan, 0.74922361,        nan,        nan, 0.74922361,\n",
       "               nan,        nan, 0.74922361,        nan,        nan,\n",
       "        0.7569702 ,        nan,        nan, 0.7569702 ,        nan,\n",
       "               nan, 0.7569702 ,        nan,        nan, 0.7569702 ,\n",
       "               nan,        nan, 0.7569702 ,        nan,        nan,\n",
       "        0.7569702 ,        nan,        nan, 0.76634079,        nan,\n",
       "               nan, 0.76634079,        nan,        nan, 0.76634079,\n",
       "               nan,        nan, 0.76634079,        nan,        nan,\n",
       "        0.76634079,        nan,        nan, 0.76634079,        nan,\n",
       "               nan, 0.77787316,        nan,        nan, 0.77787316,\n",
       "               nan,        nan, 0.77787316,        nan,        nan,\n",
       "        0.77787316,        nan,        nan, 0.77787316,        nan,\n",
       "               nan, 0.77787316,        nan,        nan, 0.79254346,\n",
       "               nan,        nan, 0.79254346,        nan,        nan,\n",
       "        0.79254346,        nan,        nan, 0.79254346,        nan,\n",
       "               nan, 0.79254346,        nan,        nan, 0.79254346,\n",
       "               nan,        nan, 0.80943057,        nan,        nan,\n",
       "        0.80943057,        nan,        nan, 0.80943057,        nan,\n",
       "               nan, 0.80943057,        nan,        nan, 0.80943057,\n",
       "               nan,        nan, 0.80943057,        nan,        nan,\n",
       "        0.82913546,        nan,        nan, 0.82913546,        nan,\n",
       "               nan, 0.82913546,        nan,        nan, 0.82913546,\n",
       "               nan,        nan, 0.82913546,        nan,        nan,\n",
       "        0.82913546,        nan,        nan, 0.85073974,        nan,\n",
       "               nan, 0.85073974,        nan,        nan, 0.85073974,\n",
       "               nan,        nan, 0.85073974,        nan,        nan,\n",
       "        0.85073974,        nan,        nan, 0.85073974,        nan,\n",
       "               nan, 0.87253286,        nan,        nan, 0.87253286,\n",
       "               nan,        nan, 0.87253286,        nan,        nan,\n",
       "        0.87253286,        nan,        nan, 0.87253286,        nan,\n",
       "               nan, 0.87253286,        nan,        nan, 0.89360988,\n",
       "               nan,        nan, 0.89360988,        nan,        nan,\n",
       "        0.89360988,        nan,        nan, 0.89360988,        nan,\n",
       "               nan, 0.89360988,        nan,        nan, 0.89360988,\n",
       "               nan,        nan, 0.9118184 ,        nan,        nan,\n",
       "        0.9118184 ,        nan,        nan, 0.9118184 ,        nan,\n",
       "               nan, 0.9118184 ,        nan,        nan, 0.9118184 ,\n",
       "               nan,        nan, 0.9118184 ,        nan,        nan,\n",
       "        0.92713042,        nan,        nan, 0.92713042,        nan,\n",
       "               nan, 0.92713042,        nan,        nan, 0.92713042,\n",
       "               nan,        nan, 0.92713042,        nan,        nan,\n",
       "        0.92713042,        nan,        nan, 0.93897856,        nan,\n",
       "               nan, 0.93898381,        nan,        nan, 0.93898381,\n",
       "               nan,        nan, 0.93898381,        nan,        nan,\n",
       "        0.93898381,        nan,        nan, 0.93898381,        nan,\n",
       "               nan, 0.94808429,        nan,        nan, 0.94808169,\n",
       "               nan,        nan, 0.94808169,        nan,        nan,\n",
       "        0.94808169,        nan,        nan, 0.94808169,        nan,\n",
       "               nan, 0.94808169,        nan,        nan, 0.95482069,\n",
       "               nan,        nan, 0.95483905,        nan,        nan,\n",
       "        0.95483905,        nan,        nan, 0.95483905,        nan,\n",
       "               nan, 0.95483905,        nan,        nan, 0.95483905,\n",
       "               nan,        nan, 0.95976785,        nan,        nan,\n",
       "        0.9597836 ,        nan,        nan, 0.95978622,        nan,\n",
       "               nan, 0.95978097,        nan,        nan, 0.95978097,\n",
       "               nan,        nan, 0.95978097,        nan,        nan,\n",
       "        0.963175  ,        nan,        nan, 0.96318554,        nan,\n",
       "               nan, 0.96318554,        nan,        nan, 0.96317503,\n",
       "               nan,        nan, 0.96317503,        nan,        nan,\n",
       "        0.96317503,        nan,        nan, 0.96540988,        nan,\n",
       "               nan, 0.96542561,        nan,        nan, 0.96543611,\n",
       "               nan,        nan, 0.96543349,        nan,        nan,\n",
       "        0.96543349,        nan,        nan, 0.96543349,        nan,\n",
       "               nan, 0.96704921,        nan,        nan, 0.96705426,\n",
       "               nan,        nan, 0.96703594,        nan,        nan,\n",
       "        0.96706215,        nan,        nan, 0.96706215,        nan,\n",
       "               nan, 0.96706215,        nan,        nan, 0.96794864,\n",
       "               nan,        nan, 0.96808751,        nan,        nan,\n",
       "        0.96812694,        nan,        nan, 0.96814265,        nan,\n",
       "               nan, 0.96814265,        nan,        nan, 0.96814265,\n",
       "               nan,        nan, 0.96875412,        nan,        nan,\n",
       "        0.96886397,        nan,        nan, 0.96888241,        nan,\n",
       "               nan, 0.96890075,        nan,        nan, 0.96890075,\n",
       "               nan,        nan, 0.96890075,        nan,        nan,\n",
       "        0.96954893,        nan,        nan, 0.96956703,        nan,\n",
       "               nan, 0.96949612,        nan,        nan, 0.96954346,\n",
       "               nan,        nan, 0.96954346,        nan,        nan,\n",
       "        0.96954346,        nan,        nan, 0.96992676,        nan,\n",
       "               nan, 0.97004455,        nan,        nan, 0.96996325,\n",
       "               nan,        nan, 0.97008129,        nan,        nan,\n",
       "        0.97008129,        nan,        nan, 0.97008129,        nan,\n",
       "               nan, 0.96981381,        nan,        nan, 0.97028596,\n",
       "               nan,        nan, 0.97032787,        nan,        nan,\n",
       "        0.97045908,        nan,        nan, 0.97045908,        nan,\n",
       "               nan, 0.97045908,        nan,        nan, 0.97033609,\n",
       "               nan,        nan, 0.97050371,        nan,        nan,\n",
       "        0.97045625,        nan,        nan, 0.97070829,        nan,\n",
       "               nan, 0.97070829,        nan,        nan, 0.97070829,\n",
       "               nan,        nan, 0.97048534,        nan,        nan,\n",
       "        0.97028902,        nan,        nan, 0.97071893,        nan,\n",
       "               nan, 0.97094174,        nan,        nan, 0.97094174,\n",
       "               nan,        nan, 0.97094174,        nan,        nan,\n",
       "        0.97071915,        nan,        nan, 0.9704961 ,        nan,\n",
       "               nan, 0.97090008,        nan,        nan, 0.97106242,\n",
       "               nan,        nan, 0.97106242,        nan,        nan,\n",
       "        0.97106242,        nan,        nan, 0.97023314,        nan,\n",
       "               nan, 0.97078432,        nan,        nan, 0.97091841,\n",
       "               nan,        nan, 0.97117784,        nan,        nan,\n",
       "        0.97117784,        nan,        nan, 0.97117784,        nan,\n",
       "               nan, 0.97053812,        nan,        nan, 0.97073195,\n",
       "               nan,        nan, 0.97091532,        nan,        nan,\n",
       "        0.97129591,        nan,        nan, 0.97129591,        nan,\n",
       "               nan, 0.97129591,        nan,        nan, 0.97066883,\n",
       "               nan,        nan, 0.97107578,        nan,        nan,\n",
       "        0.97112009,        nan,        nan, 0.97142183,        nan,\n",
       "               nan, 0.97142183,        nan,        nan, 0.97142183,\n",
       "               nan,        nan, 0.97088932,        nan,        nan,\n",
       "        0.97112823,        nan,        nan, 0.97112016,        nan,\n",
       "               nan, 0.97151626,        nan,        nan, 0.97151626,\n",
       "               nan,        nan, 0.97151626,        nan,        nan,\n",
       "        0.97031688,        nan,        nan, 0.97034329,        nan,\n",
       "               nan, 0.97071578,        nan,        nan, 0.97157911,\n",
       "               nan,        nan, 0.97157911,        nan,        nan,\n",
       "        0.97157911,        nan,        nan, 0.97038059,        nan,\n",
       "               nan, 0.97114689,        nan,        nan, 0.9711174 ,\n",
       "               nan,        nan, 0.97165252,        nan,        nan,\n",
       "        0.97165252,        nan,        nan, 0.97165252,        nan,\n",
       "               nan, 0.97026473,        nan,        nan, 0.97088179,\n",
       "               nan,        nan, 0.97085231,        nan,        nan,\n",
       "        0.97169183,        nan,        nan, 0.97169183,        nan,\n",
       "               nan, 0.97169183,        nan,        nan, 0.97010273,\n",
       "               nan,        nan, 0.97035651,        nan,        nan,\n",
       "        0.97081555,        nan,        nan, 0.97175217,        nan,\n",
       "               nan, 0.97175217,        nan,        nan, 0.97175217,\n",
       "               nan,        nan, 0.97049086,        nan,        nan,\n",
       "        0.97092081,        nan,        nan, 0.97075037,        nan,\n",
       "               nan, 0.97182301,        nan,        nan, 0.97182301,\n",
       "               nan,        nan, 0.97182301,        nan,        nan,\n",
       "        0.97041979,        nan,        nan, 0.97058535,        nan,\n",
       "               nan, 0.97096337,        nan,        nan, 0.97183094,\n",
       "               nan,        nan, 0.97183094,        nan,        nan,\n",
       "        0.97183094,        nan,        nan, 0.97053186,        nan,\n",
       "               nan, 0.97053265,        nan,        nan, 0.9709704 ,\n",
       "               nan,        nan, 0.97185455,        nan,        nan,\n",
       "        0.97185455,        nan,        nan, 0.97185455,        nan]),\n",
       " 'std_test_score': array([       nan, 0.02055782,        nan,        nan, 0.02055782,\n",
       "               nan,        nan, 0.02055782,        nan,        nan,\n",
       "        0.02055782,        nan,        nan, 0.02055782,        nan,\n",
       "               nan, 0.02055782,        nan,        nan, 0.02059651,\n",
       "               nan,        nan, 0.02059651,        nan,        nan,\n",
       "        0.02059651,        nan,        nan, 0.02059651,        nan,\n",
       "               nan, 0.02059651,        nan,        nan, 0.02059651,\n",
       "               nan,        nan, 0.02070057,        nan,        nan,\n",
       "        0.02070057,        nan,        nan, 0.02070057,        nan,\n",
       "               nan, 0.02070057,        nan,        nan, 0.02070057,\n",
       "               nan,        nan, 0.02070057,        nan,        nan,\n",
       "        0.0207376 ,        nan,        nan, 0.0207376 ,        nan,\n",
       "               nan, 0.0207376 ,        nan,        nan, 0.0207376 ,\n",
       "               nan,        nan, 0.0207376 ,        nan,        nan,\n",
       "        0.0207376 ,        nan,        nan, 0.02121283,        nan,\n",
       "               nan, 0.02121283,        nan,        nan, 0.02121283,\n",
       "               nan,        nan, 0.02121283,        nan,        nan,\n",
       "        0.02121283,        nan,        nan, 0.02121283,        nan,\n",
       "               nan, 0.02166254,        nan,        nan, 0.02166254,\n",
       "               nan,        nan, 0.02166254,        nan,        nan,\n",
       "        0.02166254,        nan,        nan, 0.02166254,        nan,\n",
       "               nan, 0.02166254,        nan,        nan, 0.02229509,\n",
       "               nan,        nan, 0.02229509,        nan,        nan,\n",
       "        0.02229509,        nan,        nan, 0.02229509,        nan,\n",
       "               nan, 0.02229509,        nan,        nan, 0.02229509,\n",
       "               nan,        nan, 0.02290114,        nan,        nan,\n",
       "        0.02290114,        nan,        nan, 0.02290114,        nan,\n",
       "               nan, 0.02290114,        nan,        nan, 0.02290114,\n",
       "               nan,        nan, 0.02290114,        nan,        nan,\n",
       "        0.02359394,        nan,        nan, 0.02359394,        nan,\n",
       "               nan, 0.02359394,        nan,        nan, 0.02359394,\n",
       "               nan,        nan, 0.02359394,        nan,        nan,\n",
       "        0.02359394,        nan,        nan, 0.02351772,        nan,\n",
       "               nan, 0.02351772,        nan,        nan, 0.02351772,\n",
       "               nan,        nan, 0.02351772,        nan,        nan,\n",
       "        0.02351772,        nan,        nan, 0.02351772,        nan,\n",
       "               nan, 0.02349557,        nan,        nan, 0.02349557,\n",
       "               nan,        nan, 0.02349557,        nan,        nan,\n",
       "        0.02349557,        nan,        nan, 0.02349557,        nan,\n",
       "               nan, 0.02349557,        nan,        nan, 0.02373515,\n",
       "               nan,        nan, 0.02373515,        nan,        nan,\n",
       "        0.02373515,        nan,        nan, 0.02373515,        nan,\n",
       "               nan, 0.02373515,        nan,        nan, 0.02373515,\n",
       "               nan,        nan, 0.02416942,        nan,        nan,\n",
       "        0.02416942,        nan,        nan, 0.02416942,        nan,\n",
       "               nan, 0.02416942,        nan,        nan, 0.02416942,\n",
       "               nan,        nan, 0.02416942,        nan,        nan,\n",
       "        0.02465101,        nan,        nan, 0.02465101,        nan,\n",
       "               nan, 0.02465101,        nan,        nan, 0.02465101,\n",
       "               nan,        nan, 0.02465101,        nan,        nan,\n",
       "        0.02465101,        nan,        nan, 0.02501283,        nan,\n",
       "               nan, 0.02501283,        nan,        nan, 0.02501283,\n",
       "               nan,        nan, 0.02501283,        nan,        nan,\n",
       "        0.02501283,        nan,        nan, 0.02501283,        nan,\n",
       "               nan, 0.02478927,        nan,        nan, 0.02478927,\n",
       "               nan,        nan, 0.02478927,        nan,        nan,\n",
       "        0.02478927,        nan,        nan, 0.02478927,        nan,\n",
       "               nan, 0.02478927,        nan,        nan, 0.02445895,\n",
       "               nan,        nan, 0.02445895,        nan,        nan,\n",
       "        0.02445895,        nan,        nan, 0.02445895,        nan,\n",
       "               nan, 0.02445895,        nan,        nan, 0.02445895,\n",
       "               nan,        nan, 0.02417188,        nan,        nan,\n",
       "        0.02417188,        nan,        nan, 0.02417188,        nan,\n",
       "               nan, 0.02417188,        nan,        nan, 0.02417188,\n",
       "               nan,        nan, 0.02417188,        nan,        nan,\n",
       "        0.02320676,        nan,        nan, 0.02320676,        nan,\n",
       "               nan, 0.02320676,        nan,        nan, 0.02320676,\n",
       "               nan,        nan, 0.02320676,        nan,        nan,\n",
       "        0.02320676,        nan,        nan, 0.02098708,        nan,\n",
       "               nan, 0.02098708,        nan,        nan, 0.02098708,\n",
       "               nan,        nan, 0.02098708,        nan,        nan,\n",
       "        0.02098708,        nan,        nan, 0.02098708,        nan,\n",
       "               nan, 0.01882935,        nan,        nan, 0.01882935,\n",
       "               nan,        nan, 0.01882935,        nan,        nan,\n",
       "        0.01882935,        nan,        nan, 0.01882935,        nan,\n",
       "               nan, 0.01882935,        nan,        nan, 0.01712398,\n",
       "               nan,        nan, 0.01712398,        nan,        nan,\n",
       "        0.01712398,        nan,        nan, 0.01712398,        nan,\n",
       "               nan, 0.01712398,        nan,        nan, 0.01712398,\n",
       "               nan,        nan, 0.01467536,        nan,        nan,\n",
       "        0.01467536,        nan,        nan, 0.01467536,        nan,\n",
       "               nan, 0.01467536,        nan,        nan, 0.01467536,\n",
       "               nan,        nan, 0.01467536,        nan,        nan,\n",
       "        0.01279783,        nan,        nan, 0.01279783,        nan,\n",
       "               nan, 0.01279783,        nan,        nan, 0.01279783,\n",
       "               nan,        nan, 0.01279783,        nan,        nan,\n",
       "        0.01279783,        nan,        nan, 0.01126459,        nan,\n",
       "               nan, 0.01127414,        nan,        nan, 0.01127414,\n",
       "               nan,        nan, 0.01127414,        nan,        nan,\n",
       "        0.01127414,        nan,        nan, 0.01127414,        nan,\n",
       "               nan, 0.00944714,        nan,        nan, 0.00945878,\n",
       "               nan,        nan, 0.00945878,        nan,        nan,\n",
       "        0.00945878,        nan,        nan, 0.00945878,        nan,\n",
       "               nan, 0.00945878,        nan,        nan, 0.00810647,\n",
       "               nan,        nan, 0.00808529,        nan,        nan,\n",
       "        0.00808529,        nan,        nan, 0.00808529,        nan,\n",
       "               nan, 0.00808529,        nan,        nan, 0.00808529,\n",
       "               nan,        nan, 0.00706893,        nan,        nan,\n",
       "        0.00709493,        nan,        nan, 0.00709988,        nan,\n",
       "               nan, 0.00709521,        nan,        nan, 0.00709521,\n",
       "               nan,        nan, 0.00709521,        nan,        nan,\n",
       "        0.00614078,        nan,        nan, 0.00612985,        nan,\n",
       "               nan, 0.00612569,        nan,        nan, 0.00612138,\n",
       "               nan,        nan, 0.00612138,        nan,        nan,\n",
       "        0.00612138,        nan,        nan, 0.00573657,        nan,\n",
       "               nan, 0.00568238,        nan,        nan, 0.00568643,\n",
       "               nan,        nan, 0.00569168,        nan,        nan,\n",
       "        0.00569168,        nan,        nan, 0.00569168,        nan,\n",
       "               nan, 0.00532286,        nan,        nan, 0.0052629 ,\n",
       "               nan,        nan, 0.00527936,        nan,        nan,\n",
       "        0.00525408,        nan,        nan, 0.00525408,        nan,\n",
       "               nan, 0.00525408,        nan,        nan, 0.00489362,\n",
       "               nan,        nan, 0.00497313,        nan,        nan,\n",
       "        0.00501046,        nan,        nan, 0.0050185 ,        nan,\n",
       "               nan, 0.0050185 ,        nan,        nan, 0.0050185 ,\n",
       "               nan,        nan, 0.0047914 ,        nan,        nan,\n",
       "        0.00471587,        nan,        nan, 0.00470091,        nan,\n",
       "               nan, 0.00474108,        nan,        nan, 0.00474108,\n",
       "               nan,        nan, 0.00474108,        nan,        nan,\n",
       "        0.00465479,        nan,        nan, 0.00462851,        nan,\n",
       "               nan, 0.00457249,        nan,        nan, 0.00460522,\n",
       "               nan,        nan, 0.00460522,        nan,        nan,\n",
       "        0.00460522,        nan,        nan, 0.00433285,        nan,\n",
       "               nan, 0.00454686,        nan,        nan, 0.0045309 ,\n",
       "               nan,        nan, 0.00448946,        nan,        nan,\n",
       "        0.00448946,        nan,        nan, 0.00448946,        nan,\n",
       "               nan, 0.00451425,        nan,        nan, 0.00433564,\n",
       "               nan,        nan, 0.00438001,        nan,        nan,\n",
       "        0.00439734,        nan,        nan, 0.00439734,        nan,\n",
       "               nan, 0.00439734,        nan,        nan, 0.00451618,\n",
       "               nan,        nan, 0.00436844,        nan,        nan,\n",
       "        0.00418492,        nan,        nan, 0.00430713,        nan,\n",
       "               nan, 0.00430713,        nan,        nan, 0.00430713,\n",
       "               nan,        nan, 0.0046653 ,        nan,        nan,\n",
       "        0.00413986,        nan,        nan, 0.00428498,        nan,\n",
       "               nan, 0.00426664,        nan,        nan, 0.00426664,\n",
       "               nan,        nan, 0.00426664,        nan,        nan,\n",
       "        0.00443956,        nan,        nan, 0.00433978,        nan,\n",
       "               nan, 0.00409387,        nan,        nan, 0.00423876,\n",
       "               nan,        nan, 0.00423876,        nan,        nan,\n",
       "        0.00423876,        nan,        nan, 0.00468408,        nan,\n",
       "               nan, 0.00454341,        nan,        nan, 0.00423872,\n",
       "               nan,        nan, 0.00423056,        nan,        nan,\n",
       "        0.00423056,        nan,        nan, 0.00423056,        nan,\n",
       "               nan, 0.00452575,        nan,        nan, 0.00443128,\n",
       "               nan,        nan, 0.00438283,        nan,        nan,\n",
       "        0.00418141,        nan,        nan, 0.00418141,        nan,\n",
       "               nan, 0.00418141,        nan,        nan, 0.00436916,\n",
       "               nan,        nan, 0.00444539,        nan,        nan,\n",
       "        0.00431465,        nan,        nan, 0.00412061,        nan,\n",
       "               nan, 0.00412061,        nan,        nan, 0.00412061,\n",
       "               nan,        nan, 0.00451049,        nan,        nan,\n",
       "        0.00464292,        nan,        nan, 0.00464346,        nan,\n",
       "               nan, 0.00412834,        nan,        nan, 0.00412834,\n",
       "               nan,        nan, 0.00412834,        nan,        nan,\n",
       "        0.00442302,        nan,        nan, 0.00488672,        nan,\n",
       "               nan, 0.00441632,        nan,        nan, 0.00415543,\n",
       "               nan,        nan, 0.00415543,        nan,        nan,\n",
       "        0.00415543,        nan,        nan, 0.00531552,        nan,\n",
       "               nan, 0.00407621,        nan,        nan, 0.00413413,\n",
       "               nan,        nan, 0.00411962,        nan,        nan,\n",
       "        0.00411962,        nan,        nan, 0.00411962,        nan,\n",
       "               nan, 0.00536487,        nan,        nan, 0.00480417,\n",
       "               nan,        nan, 0.00449235,        nan,        nan,\n",
       "        0.00412277,        nan,        nan, 0.00412277,        nan,\n",
       "               nan, 0.00412277,        nan,        nan, 0.00554217,\n",
       "               nan,        nan, 0.00436641,        nan,        nan,\n",
       "        0.00464058,        nan,        nan, 0.00410602,        nan,\n",
       "               nan, 0.00410602,        nan,        nan, 0.00410602,\n",
       "               nan,        nan, 0.00500298,        nan,        nan,\n",
       "        0.00500874,        nan,        nan, 0.00449049,        nan,\n",
       "               nan, 0.00408567,        nan,        nan, 0.00408567,\n",
       "               nan,        nan, 0.00408567,        nan,        nan,\n",
       "        0.00531469,        nan,        nan, 0.00533687,        nan,\n",
       "               nan, 0.00498669,        nan,        nan, 0.00408138,\n",
       "               nan,        nan, 0.00408138,        nan,        nan,\n",
       "        0.00408138,        nan,        nan, 0.00510192,        nan,\n",
       "               nan, 0.00518318,        nan,        nan, 0.00502744,\n",
       "               nan,        nan, 0.00406092,        nan,        nan,\n",
       "        0.00406092,        nan,        nan, 0.00406092,        nan]),\n",
       " 'rank_test_score': array([313, 295, 655, 656, 295, 657, 658, 295, 659, 660, 295, 661, 662,\n",
       "        295, 663, 664, 295, 665, 666, 289, 667, 668, 289, 669, 670, 289,\n",
       "        671, 672, 289, 673, 654, 289, 653, 652, 289, 641, 633, 283, 634,\n",
       "        635, 283, 636, 637, 283, 638, 639, 283, 640, 642, 283, 651, 643,\n",
       "        283, 644, 645, 277, 646, 647, 277, 648, 649, 277, 650, 674, 277,\n",
       "        676, 719, 277, 677, 700, 277, 701, 702, 271, 703, 704, 271, 705,\n",
       "        706, 271, 707, 708, 271, 709, 710, 271, 711, 712, 271, 713, 714,\n",
       "        265, 715, 716, 265, 717, 718, 265, 699, 698, 265, 697, 686, 265,\n",
       "        678, 679, 265, 680, 681, 259, 682, 683, 259, 684, 685, 259, 687,\n",
       "        696, 259, 688, 689, 259, 690, 691, 259, 692, 693, 253, 694, 695,\n",
       "        253, 632, 675, 253, 631, 585, 253, 565, 566, 253, 567, 568, 253,\n",
       "        569, 570, 247, 571, 572, 247, 573, 574, 247, 575, 576, 247, 577,\n",
       "        578, 247, 579, 580, 247, 581, 582, 241, 583, 564, 241, 563, 562,\n",
       "        241, 551, 543, 241, 544, 545, 241, 546, 547, 241, 548, 549, 235,\n",
       "        550, 552, 235, 561, 553, 235, 554, 555, 235, 556, 557, 235, 558,\n",
       "        559, 235, 560, 584, 229, 586, 629, 229, 587, 610, 229, 611, 612,\n",
       "        229, 613, 614, 229, 615, 616, 229, 617, 618, 223, 619, 620, 223,\n",
       "        621, 622, 223, 623, 624, 223, 625, 626, 223, 627, 628, 223, 609,\n",
       "        608, 217, 607, 596, 217, 588, 589, 217, 590, 591, 217, 592, 593,\n",
       "        217, 594, 595, 217, 597, 606, 211, 598, 599, 211, 600, 601, 211,\n",
       "        602, 603, 211, 604, 605, 211, 630, 721, 211, 810, 722, 205, 835,\n",
       "        836, 205, 837, 838, 205, 839, 840, 205, 841, 842, 205, 843, 844,\n",
       "        205, 845, 846, 199, 847, 848, 199, 849, 850, 199, 851, 852, 199,\n",
       "        853, 834, 199, 833, 832, 199, 821, 813, 193, 814, 815, 193, 816,\n",
       "        817, 193, 818, 819, 193, 820, 822, 193, 831, 823, 193, 824, 825,\n",
       "        187, 826, 827, 187, 828, 829, 187, 830, 854, 187, 856, 899, 187,\n",
       "        857, 880, 187, 881, 882, 181, 883, 884, 181, 885, 886, 181, 887,\n",
       "        888, 181, 889, 890, 181, 891, 892, 181, 893, 894, 175, 895, 896,\n",
       "        175, 897, 898, 175, 879, 878, 175, 877, 866, 175, 858, 859, 175,\n",
       "        860, 861, 169, 862, 863, 169, 864, 865, 169, 867, 876, 169, 868,\n",
       "        869, 169, 870, 871, 169, 872, 873, 163, 874, 875, 163, 812, 855,\n",
       "        163, 811, 765, 163, 745, 746, 163, 747, 748, 163, 749, 750, 157,\n",
       "        751, 752, 157, 753, 754, 157, 755, 756, 157, 757, 758, 157, 759,\n",
       "        760, 157, 761, 762, 156, 763, 744, 151, 743, 742, 151, 731, 723,\n",
       "        151, 724, 725, 151, 726, 727, 151, 541, 728, 145, 729, 730, 146,\n",
       "        732, 741, 146, 733, 734, 146, 735, 736, 146, 737, 738, 146, 739,\n",
       "        740, 144, 764, 766, 139, 809, 767, 139, 790, 791, 139, 792, 793,\n",
       "        139, 794, 795, 139, 796, 797, 138, 798, 799, 134, 800, 801, 133,\n",
       "        802, 803, 135, 804, 805, 135, 806, 807, 135, 808, 789, 132, 788,\n",
       "        787, 127, 776, 768, 127, 769, 770, 129, 771, 772, 129, 773, 774,\n",
       "        129, 775, 777, 126, 786, 778, 125, 779, 780, 121, 781, 782, 122,\n",
       "        783, 784, 122, 785, 542, 122, 720, 540, 119, 340, 339, 118, 338,\n",
       "        337, 120, 336, 335, 115, 334, 333, 115, 332, 331, 115, 330, 329,\n",
       "        114, 341, 328, 113, 326, 325, 112, 324, 323, 109, 322, 321, 109,\n",
       "        320, 319, 109, 318, 317, 108, 316, 315, 107, 327, 342, 106, 343,\n",
       "        344, 103, 371, 370, 103, 369, 368, 103, 367, 366,  98, 373, 365,\n",
       "         97, 364, 363, 102, 362, 361,  99, 360, 359,  99, 358, 357,  99,\n",
       "        356, 355,  95, 354, 353,  93, 352, 351,  94, 350, 349,  90, 348,\n",
       "        347,  90, 346, 345,  90, 314, 372,  96, 302, 312,  86, 311, 304,\n",
       "         83, 308, 307,  74, 309, 301,  74, 310, 303,  74, 305, 306,  82,\n",
       "        498, 375,  70, 480, 481,  77, 482, 483,  62, 484, 485,  62, 486,\n",
       "        487,  62, 488, 489,  73, 490, 491,  85, 492, 493,  60, 494, 495,\n",
       "         45, 496, 479,  45, 497, 478,  45, 476, 459,  59, 460, 461,  71,\n",
       "        462, 463,  51, 464, 465,  40, 466, 467,  40, 468, 469,  40, 470,\n",
       "        471,  88, 472, 473,  56, 474, 475,  49, 477, 499,  31, 519, 500,\n",
       "         31, 522, 523,  31, 374, 524,  67, 525, 526,  58, 527, 528,  50,\n",
       "        529, 530,  28, 531, 532,  28, 533, 534,  28, 535, 536,  65, 537,\n",
       "        538,  39, 521, 539,  37, 520, 518,  25, 501, 502,  25, 503, 504,\n",
       "         25, 505, 506,  52, 507, 508,  35, 509, 510,  36, 511, 512,  22,\n",
       "        513, 514,  22, 515, 516,  22, 517, 458,  84, 457, 456,  81, 435,\n",
       "        396,  61, 397, 398,  19, 399, 400,  19, 401, 402,  19, 403, 404,\n",
       "         79, 405, 406,  34, 407, 408,  38, 409, 410,  16, 411, 412,  16,\n",
       "        395, 394,  16, 393, 383,  87, 376, 377,  53, 378, 379,  54, 380,\n",
       "        381,  13, 382, 384,  13, 392, 385,  13, 386, 387,  89, 388, 389,\n",
       "         80, 390, 391,  55, 413, 414,  10, 415, 416,  10, 438, 439,  10,\n",
       "        440, 441,  72, 442, 443,  48, 444, 445,  57, 446, 447,   7, 448,\n",
       "        449,   7, 450, 451,   7, 452, 453,  78, 454, 437,  66, 455, 436,\n",
       "         44, 434, 417,   4, 418, 419,   4, 420, 421,   4, 422, 423,  69,\n",
       "        424, 425,  68, 426, 427,  43, 428, 429,   1, 430, 431,   1, 432,\n",
       "        433,   1, 900]),\n",
       " 'split0_train_score': array([       nan, 0.73633266,        nan,        nan, 0.73633266,\n",
       "               nan,        nan, 0.73633266,        nan,        nan,\n",
       "        0.73633266,        nan,        nan, 0.73633266,        nan,\n",
       "               nan, 0.73633266,        nan,        nan, 0.73728288,\n",
       "               nan,        nan, 0.73728288,        nan,        nan,\n",
       "        0.73728288,        nan,        nan, 0.73728288,        nan,\n",
       "               nan, 0.73728288,        nan,        nan, 0.73728288,\n",
       "               nan,        nan, 0.73842264,        nan,        nan,\n",
       "        0.73842264,        nan,        nan, 0.73842264,        nan,\n",
       "               nan, 0.73842264,        nan,        nan, 0.73842264,\n",
       "               nan,        nan, 0.73842264,        nan,        nan,\n",
       "        0.73936055,        nan,        nan, 0.73936055,        nan,\n",
       "               nan, 0.73936055,        nan,        nan, 0.73936055,\n",
       "               nan,        nan, 0.73936055,        nan,        nan,\n",
       "        0.73936055,        nan,        nan, 0.74035015,        nan,\n",
       "               nan, 0.74035015,        nan,        nan, 0.74035015,\n",
       "               nan,        nan, 0.74035015,        nan,        nan,\n",
       "        0.74035015,        nan,        nan, 0.74035015,        nan,\n",
       "               nan, 0.74172952,        nan,        nan, 0.74172952,\n",
       "               nan,        nan, 0.74172952,        nan,        nan,\n",
       "        0.74172952,        nan,        nan, 0.74172952,        nan,\n",
       "               nan, 0.74172952,        nan,        nan, 0.74339691,\n",
       "               nan,        nan, 0.74339691,        nan,        nan,\n",
       "        0.74339691,        nan,        nan, 0.74339691,        nan,\n",
       "               nan, 0.74339691,        nan,        nan, 0.74339691,\n",
       "               nan,        nan, 0.74572978,        nan,        nan,\n",
       "        0.74572978,        nan,        nan, 0.74572978,        nan,\n",
       "               nan, 0.74572978,        nan,        nan, 0.74572978,\n",
       "               nan,        nan, 0.74572978,        nan,        nan,\n",
       "        0.74889388,        nan,        nan, 0.74889388,        nan,\n",
       "               nan, 0.74889388,        nan,        nan, 0.74889388,\n",
       "               nan,        nan, 0.74889388,        nan,        nan,\n",
       "        0.74889388,        nan,        nan, 0.75301968,        nan,\n",
       "               nan, 0.75301968,        nan,        nan, 0.75301968,\n",
       "               nan,        nan, 0.75301968,        nan,        nan,\n",
       "        0.75301968,        nan,        nan, 0.75301968,        nan,\n",
       "               nan, 0.75809242,        nan,        nan, 0.75809242,\n",
       "               nan,        nan, 0.75809242,        nan,        nan,\n",
       "        0.75809242,        nan,        nan, 0.75809242,        nan,\n",
       "               nan, 0.75809242,        nan,        nan, 0.76429589,\n",
       "               nan,        nan, 0.76429589,        nan,        nan,\n",
       "        0.76429589,        nan,        nan, 0.76429589,        nan,\n",
       "               nan, 0.76429589,        nan,        nan, 0.76429589,\n",
       "               nan,        nan, 0.77155953,        nan,        nan,\n",
       "        0.77155953,        nan,        nan, 0.77155953,        nan,\n",
       "               nan, 0.77155953,        nan,        nan, 0.77155953,\n",
       "               nan,        nan, 0.77155953,        nan,        nan,\n",
       "        0.78052749,        nan,        nan, 0.78052749,        nan,\n",
       "               nan, 0.78052749,        nan,        nan, 0.78052749,\n",
       "               nan,        nan, 0.78052749,        nan,        nan,\n",
       "        0.78052749,        nan,        nan, 0.79152389,        nan,\n",
       "               nan, 0.79152389,        nan,        nan, 0.79152389,\n",
       "               nan,        nan, 0.79152389,        nan,        nan,\n",
       "        0.79152389,        nan,        nan, 0.79152389,        nan,\n",
       "               nan, 0.80470791,        nan,        nan, 0.80470791,\n",
       "               nan,        nan, 0.80470791,        nan,        nan,\n",
       "        0.80470791,        nan,        nan, 0.80470791,        nan,\n",
       "               nan, 0.80470791,        nan,        nan, 0.82168869,\n",
       "               nan,        nan, 0.82168869,        nan,        nan,\n",
       "        0.82168869,        nan,        nan, 0.82168869,        nan,\n",
       "               nan, 0.82168869,        nan,        nan, 0.82168869,\n",
       "               nan,        nan, 0.84202804,        nan,        nan,\n",
       "        0.84202804,        nan,        nan, 0.84202804,        nan,\n",
       "               nan, 0.84202804,        nan,        nan, 0.84202804,\n",
       "               nan,        nan, 0.84202804,        nan,        nan,\n",
       "        0.86526481,        nan,        nan, 0.86526481,        nan,\n",
       "               nan, 0.86526481,        nan,        nan, 0.86526481,\n",
       "               nan,        nan, 0.86526481,        nan,        nan,\n",
       "        0.86526481,        nan,        nan, 0.8897792 ,        nan,\n",
       "               nan, 0.8897792 ,        nan,        nan, 0.8897792 ,\n",
       "               nan,        nan, 0.8897792 ,        nan,        nan,\n",
       "        0.8897792 ,        nan,        nan, 0.8897792 ,        nan,\n",
       "               nan, 0.91455617,        nan,        nan, 0.91455617,\n",
       "               nan,        nan, 0.91455617,        nan,        nan,\n",
       "        0.91455617,        nan,        nan, 0.91455617,        nan,\n",
       "               nan, 0.91455617,        nan,        nan, 0.93784382,\n",
       "               nan,        nan, 0.93784382,        nan,        nan,\n",
       "        0.93784382,        nan,        nan, 0.93784382,        nan,\n",
       "               nan, 0.93784382,        nan,        nan, 0.93784382,\n",
       "               nan,        nan, 0.95713203,        nan,        nan,\n",
       "        0.95713203,        nan,        nan, 0.95713203,        nan,\n",
       "               nan, 0.95713203,        nan,        nan, 0.95713203,\n",
       "               nan,        nan, 0.95713203,        nan,        nan,\n",
       "        0.97166752,        nan,        nan, 0.97166752,        nan,\n",
       "               nan, 0.97166752,        nan,        nan, 0.97166752,\n",
       "               nan,        nan, 0.97166752,        nan,        nan,\n",
       "        0.97166752,        nan,        nan, 0.98140353,        nan,\n",
       "               nan, 0.98140353,        nan,        nan, 0.98140353,\n",
       "               nan,        nan, 0.98140353,        nan,        nan,\n",
       "        0.98140353,        nan,        nan, 0.98140353,        nan,\n",
       "               nan, 0.98811001,        nan,        nan, 0.98811739,\n",
       "               nan,        nan, 0.98811739,        nan,        nan,\n",
       "        0.98811739,        nan,        nan, 0.98811739,        nan,\n",
       "               nan, 0.98811739,        nan,        nan, 0.99323526,\n",
       "               nan,        nan, 0.99322787,        nan,        nan,\n",
       "        0.99322541,        nan,        nan, 0.99322541,        nan,\n",
       "               nan, 0.99322541,        nan,        nan, 0.99322541,\n",
       "               nan,        nan, 0.99699427,        nan,        nan,\n",
       "        0.9969877 ,        nan,        nan, 0.99699017,        nan,\n",
       "               nan, 0.99699017,        nan,        nan, 0.99699017,\n",
       "               nan,        nan, 0.99699017,        nan,        nan,\n",
       "        0.9992221 ,        nan,        nan, 0.99920651,        nan,\n",
       "               nan, 0.99920569,        nan,        nan, 0.99920405,\n",
       "               nan,        nan, 0.99920405,        nan,        nan,\n",
       "        0.99920405,        nan,        nan, 0.99992369,        nan,\n",
       "               nan, 0.99992533,        nan,        nan, 0.99992615,\n",
       "               nan,        nan, 0.99992615,        nan,        nan,\n",
       "        0.99992615,        nan,        nan, 0.99992615,        nan,\n",
       "               nan, 0.99999672,        nan,        nan, 0.9999959 ,\n",
       "               nan,        nan, 0.99999672,        nan,        nan,\n",
       "        0.99999672,        nan,        nan, 0.99999672,        nan,\n",
       "               nan, 0.99999672,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan]),\n",
       " 'split1_train_score': array([       nan, 0.73357961,        nan,        nan, 0.73357961,\n",
       "               nan,        nan, 0.73357961,        nan,        nan,\n",
       "        0.73357961,        nan,        nan, 0.73357961,        nan,\n",
       "               nan, 0.73357961,        nan,        nan, 0.73405757,\n",
       "               nan,        nan, 0.73405757,        nan,        nan,\n",
       "        0.73405757,        nan,        nan, 0.73405757,        nan,\n",
       "               nan, 0.73405757,        nan,        nan, 0.73405757,\n",
       "               nan,        nan, 0.73465523,        nan,        nan,\n",
       "        0.73465523,        nan,        nan, 0.73465523,        nan,\n",
       "               nan, 0.73465523,        nan,        nan, 0.73465523,\n",
       "               nan,        nan, 0.73465523,        nan,        nan,\n",
       "        0.73553737,        nan,        nan, 0.73553737,        nan,\n",
       "               nan, 0.73553737,        nan,        nan, 0.73553737,\n",
       "               nan,        nan, 0.73553737,        nan,        nan,\n",
       "        0.73553737,        nan,        nan, 0.73664004,        nan,\n",
       "               nan, 0.73664004,        nan,        nan, 0.73664004,\n",
       "               nan,        nan, 0.73664004,        nan,        nan,\n",
       "        0.73664004,        nan,        nan, 0.73664004,        nan,\n",
       "               nan, 0.73787634,        nan,        nan, 0.73787634,\n",
       "               nan,        nan, 0.73787634,        nan,        nan,\n",
       "        0.73787634,        nan,        nan, 0.73787634,        nan,\n",
       "               nan, 0.73787634,        nan,        nan, 0.73967587,\n",
       "               nan,        nan, 0.73967587,        nan,        nan,\n",
       "        0.73967587,        nan,        nan, 0.73967587,        nan,\n",
       "               nan, 0.73967587,        nan,        nan, 0.73967587,\n",
       "               nan,        nan, 0.74228703,        nan,        nan,\n",
       "        0.74228703,        nan,        nan, 0.74228703,        nan,\n",
       "               nan, 0.74228703,        nan,        nan, 0.74228703,\n",
       "               nan,        nan, 0.74228703,        nan,        nan,\n",
       "        0.74594184,        nan,        nan, 0.74594184,        nan,\n",
       "               nan, 0.74594184,        nan,        nan, 0.74594184,\n",
       "               nan,        nan, 0.74594184,        nan,        nan,\n",
       "        0.74594184,        nan,        nan, 0.75097068,        nan,\n",
       "               nan, 0.75097068,        nan,        nan, 0.75097068,\n",
       "               nan,        nan, 0.75097068,        nan,        nan,\n",
       "        0.75097068,        nan,        nan, 0.75097068,        nan,\n",
       "               nan, 0.75678984,        nan,        nan, 0.75678984,\n",
       "               nan,        nan, 0.75678984,        nan,        nan,\n",
       "        0.75678984,        nan,        nan, 0.75678984,        nan,\n",
       "               nan, 0.75678984,        nan,        nan, 0.7631591 ,\n",
       "               nan,        nan, 0.7631591 ,        nan,        nan,\n",
       "        0.7631591 ,        nan,        nan, 0.7631591 ,        nan,\n",
       "               nan, 0.7631591 ,        nan,        nan, 0.7631591 ,\n",
       "               nan,        nan, 0.7700301 ,        nan,        nan,\n",
       "        0.7700301 ,        nan,        nan, 0.7700301 ,        nan,\n",
       "               nan, 0.7700301 ,        nan,        nan, 0.7700301 ,\n",
       "               nan,        nan, 0.7700301 ,        nan,        nan,\n",
       "        0.77867768,        nan,        nan, 0.77867768,        nan,\n",
       "               nan, 0.77867768,        nan,        nan, 0.77867768,\n",
       "               nan,        nan, 0.77867768,        nan,        nan,\n",
       "        0.77867768,        nan,        nan, 0.78996347,        nan,\n",
       "               nan, 0.78996347,        nan,        nan, 0.78996347,\n",
       "               nan,        nan, 0.78996347,        nan,        nan,\n",
       "        0.78996347,        nan,        nan, 0.78996347,        nan,\n",
       "               nan, 0.80400635,        nan,        nan, 0.80400635,\n",
       "               nan,        nan, 0.80400635,        nan,        nan,\n",
       "        0.80400635,        nan,        nan, 0.80400635,        nan,\n",
       "               nan, 0.80400635,        nan,        nan, 0.82122033,\n",
       "               nan,        nan, 0.82122033,        nan,        nan,\n",
       "        0.82122033,        nan,        nan, 0.82122033,        nan,\n",
       "               nan, 0.82122033,        nan,        nan, 0.82122033,\n",
       "               nan,        nan, 0.84146359,        nan,        nan,\n",
       "        0.84146359,        nan,        nan, 0.84146359,        nan,\n",
       "               nan, 0.84146359,        nan,        nan, 0.84146359,\n",
       "               nan,        nan, 0.84146359,        nan,        nan,\n",
       "        0.86437458,        nan,        nan, 0.86437458,        nan,\n",
       "               nan, 0.86437458,        nan,        nan, 0.86437458,\n",
       "               nan,        nan, 0.86437458,        nan,        nan,\n",
       "        0.86437458,        nan,        nan, 0.88883177,        nan,\n",
       "               nan, 0.88883177,        nan,        nan, 0.88883177,\n",
       "               nan,        nan, 0.88883177,        nan,        nan,\n",
       "        0.88883177,        nan,        nan, 0.88883177,        nan,\n",
       "               nan, 0.91357017,        nan,        nan, 0.91357017,\n",
       "               nan,        nan, 0.91357017,        nan,        nan,\n",
       "        0.91357017,        nan,        nan, 0.91357017,        nan,\n",
       "               nan, 0.91357017,        nan,        nan, 0.93668119,\n",
       "               nan,        nan, 0.93668119,        nan,        nan,\n",
       "        0.93668119,        nan,        nan, 0.93668119,        nan,\n",
       "               nan, 0.93668119,        nan,        nan, 0.93668119,\n",
       "               nan,        nan, 0.95621284,        nan,        nan,\n",
       "        0.95621284,        nan,        nan, 0.95621284,        nan,\n",
       "               nan, 0.95621284,        nan,        nan, 0.95621284,\n",
       "               nan,        nan, 0.95621284,        nan,        nan,\n",
       "        0.97061727,        nan,        nan, 0.97061727,        nan,\n",
       "               nan, 0.97061727,        nan,        nan, 0.97061727,\n",
       "               nan,        nan, 0.97061727,        nan,        nan,\n",
       "        0.97061727,        nan,        nan, 0.98124637,        nan,\n",
       "               nan, 0.98124637,        nan,        nan, 0.98124637,\n",
       "               nan,        nan, 0.98124637,        nan,        nan,\n",
       "        0.98124637,        nan,        nan, 0.98124637,        nan,\n",
       "               nan, 0.98904542,        nan,        nan, 0.9890446 ,\n",
       "               nan,        nan, 0.9890446 ,        nan,        nan,\n",
       "        0.9890446 ,        nan,        nan, 0.9890446 ,        nan,\n",
       "               nan, 0.9890446 ,        nan,        nan, 0.99424069,\n",
       "               nan,        nan, 0.99424889,        nan,        nan,\n",
       "        0.99424889,        nan,        nan, 0.99424889,        nan,\n",
       "               nan, 0.99424889,        nan,        nan, 0.99424889,\n",
       "               nan,        nan, 0.99772743,        nan,        nan,\n",
       "        0.99774055,        nan,        nan, 0.99773809,        nan,\n",
       "               nan, 0.99773809,        nan,        nan, 0.99773809,\n",
       "               nan,        nan, 0.99773809,        nan,        nan,\n",
       "        0.99949416,        nan,        nan, 0.99951138,        nan,\n",
       "               nan, 0.99951138,        nan,        nan, 0.99951138,\n",
       "               nan,        nan, 0.99951138,        nan,        nan,\n",
       "        0.99951138,        nan,        nan, 0.99997049,        nan,\n",
       "               nan, 0.99997049,        nan,        nan, 0.99997049,\n",
       "               nan,        nan, 0.99997131,        nan,        nan,\n",
       "        0.99997131,        nan,        nan, 0.99997131,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan]),\n",
       " 'split2_train_score': array([       nan, 0.71627298,        nan,        nan, 0.71627298,\n",
       "               nan,        nan, 0.71627298,        nan,        nan,\n",
       "        0.71627298,        nan,        nan, 0.71627298,        nan,\n",
       "               nan, 0.71627298,        nan,        nan, 0.71686408,\n",
       "               nan,        nan, 0.71686408,        nan,        nan,\n",
       "        0.71686408,        nan,        nan, 0.71686408,        nan,\n",
       "               nan, 0.71686408,        nan,        nan, 0.71686408,\n",
       "               nan,        nan, 0.71765112,        nan,        nan,\n",
       "        0.71765112,        nan,        nan, 0.71765112,        nan,\n",
       "               nan, 0.71765112,        nan,        nan, 0.71765112,\n",
       "               nan,        nan, 0.71765112,        nan,        nan,\n",
       "        0.71873248,        nan,        nan, 0.71873248,        nan,\n",
       "               nan, 0.71873248,        nan,        nan, 0.71873248,\n",
       "               nan,        nan, 0.71873248,        nan,        nan,\n",
       "        0.71873248,        nan,        nan, 0.72044756,        nan,\n",
       "               nan, 0.72044756,        nan,        nan, 0.72044756,\n",
       "               nan,        nan, 0.72044756,        nan,        nan,\n",
       "        0.72044756,        nan,        nan, 0.72044756,        nan,\n",
       "               nan, 0.72264143,        nan,        nan, 0.72264143,\n",
       "               nan,        nan, 0.72264143,        nan,        nan,\n",
       "        0.72264143,        nan,        nan, 0.72264143,        nan,\n",
       "               nan, 0.72264143,        nan,        nan, 0.72549444,\n",
       "               nan,        nan, 0.72549444,        nan,        nan,\n",
       "        0.72549444,        nan,        nan, 0.72549444,        nan,\n",
       "               nan, 0.72549444,        nan,        nan, 0.72549444,\n",
       "               nan,        nan, 0.72897216,        nan,        nan,\n",
       "        0.72897216,        nan,        nan, 0.72897216,        nan,\n",
       "               nan, 0.72897216,        nan,        nan, 0.72897216,\n",
       "               nan,        nan, 0.72897216,        nan,        nan,\n",
       "        0.73257942,        nan,        nan, 0.73257942,        nan,\n",
       "               nan, 0.73257942,        nan,        nan, 0.73257942,\n",
       "               nan,        nan, 0.73257942,        nan,        nan,\n",
       "        0.73257942,        nan,        nan, 0.73629325,        nan,\n",
       "               nan, 0.73629325,        nan,        nan, 0.73629325,\n",
       "               nan,        nan, 0.73629325,        nan,        nan,\n",
       "        0.73629325,        nan,        nan, 0.73629325,        nan,\n",
       "               nan, 0.74151065,        nan,        nan, 0.74151065,\n",
       "               nan,        nan, 0.74151065,        nan,        nan,\n",
       "        0.74151065,        nan,        nan, 0.74151065,        nan,\n",
       "               nan, 0.74151065,        nan,        nan, 0.74891864,\n",
       "               nan,        nan, 0.74891864,        nan,        nan,\n",
       "        0.74891864,        nan,        nan, 0.74891864,        nan,\n",
       "               nan, 0.74891864,        nan,        nan, 0.74891864,\n",
       "               nan,        nan, 0.75785152,        nan,        nan,\n",
       "        0.75785152,        nan,        nan, 0.75785152,        nan,\n",
       "               nan, 0.75785152,        nan,        nan, 0.75785152,\n",
       "               nan,        nan, 0.75785152,        nan,        nan,\n",
       "        0.76870116,        nan,        nan, 0.76870116,        nan,\n",
       "               nan, 0.76870116,        nan,        nan, 0.76870116,\n",
       "               nan,        nan, 0.76870116,        nan,        nan,\n",
       "        0.76870116,        nan,        nan, 0.78162497,        nan,\n",
       "               nan, 0.78162497,        nan,        nan, 0.78162497,\n",
       "               nan,        nan, 0.78162497,        nan,        nan,\n",
       "        0.78162497,        nan,        nan, 0.78162497,        nan,\n",
       "               nan, 0.79803224,        nan,        nan, 0.79803224,\n",
       "               nan,        nan, 0.79803224,        nan,        nan,\n",
       "        0.79803224,        nan,        nan, 0.79803224,        nan,\n",
       "               nan, 0.79803224,        nan,        nan, 0.81793281,\n",
       "               nan,        nan, 0.81793281,        nan,        nan,\n",
       "        0.81793281,        nan,        nan, 0.81793281,        nan,\n",
       "               nan, 0.81793281,        nan,        nan, 0.81793281,\n",
       "               nan,        nan, 0.84077822,        nan,        nan,\n",
       "        0.84077822,        nan,        nan, 0.84077822,        nan,\n",
       "               nan, 0.84077822,        nan,        nan, 0.84077822,\n",
       "               nan,        nan, 0.84077822,        nan,        nan,\n",
       "        0.86628807,        nan,        nan, 0.86628807,        nan,\n",
       "               nan, 0.86628807,        nan,        nan, 0.86628807,\n",
       "               nan,        nan, 0.86628807,        nan,        nan,\n",
       "        0.86628807,        nan,        nan, 0.89253331,        nan,\n",
       "               nan, 0.89253331,        nan,        nan, 0.89253331,\n",
       "               nan,        nan, 0.89253331,        nan,        nan,\n",
       "        0.89253331,        nan,        nan, 0.89253331,        nan,\n",
       "               nan, 0.91800545,        nan,        nan, 0.91800545,\n",
       "               nan,        nan, 0.91800545,        nan,        nan,\n",
       "        0.91800545,        nan,        nan, 0.91800545,        nan,\n",
       "               nan, 0.91800545,        nan,        nan, 0.94069181,\n",
       "               nan,        nan, 0.94069181,        nan,        nan,\n",
       "        0.94069181,        nan,        nan, 0.94069181,        nan,\n",
       "               nan, 0.94069181,        nan,        nan, 0.94069181,\n",
       "               nan,        nan, 0.95896829,        nan,        nan,\n",
       "        0.95896829,        nan,        nan, 0.95896829,        nan,\n",
       "               nan, 0.95896829,        nan,        nan, 0.95896829,\n",
       "               nan,        nan, 0.95896829,        nan,        nan,\n",
       "        0.97204869,        nan,        nan, 0.97204869,        nan,\n",
       "               nan, 0.97204869,        nan,        nan, 0.97204869,\n",
       "               nan,        nan, 0.97204869,        nan,        nan,\n",
       "        0.97204869,        nan,        nan, 0.98173253,        nan,\n",
       "               nan, 0.98173089,        nan,        nan, 0.98173089,\n",
       "               nan,        nan, 0.98173089,        nan,        nan,\n",
       "        0.98173089,        nan,        nan, 0.98173089,        nan,\n",
       "               nan, 0.98917659,        nan,        nan, 0.98918561,\n",
       "               nan,        nan, 0.98918561,        nan,        nan,\n",
       "        0.98918561,        nan,        nan, 0.98918561,        nan,\n",
       "               nan, 0.98918561,        nan,        nan, 0.99441531,\n",
       "               nan,        nan, 0.99440548,        nan,        nan,\n",
       "        0.99440548,        nan,        nan, 0.99440548,        nan,\n",
       "               nan, 0.99440548,        nan,        nan, 0.99440548,\n",
       "               nan,        nan, 0.99774383,        nan,        nan,\n",
       "        0.99772825,        nan,        nan, 0.99772579,        nan,\n",
       "               nan, 0.99772743,        nan,        nan, 0.99772743,\n",
       "               nan,        nan, 0.99772743,        nan,        nan,\n",
       "        0.99947285,        nan,        nan, 0.99948597,        nan,\n",
       "               nan, 0.99949089,        nan,        nan, 0.99948925,\n",
       "               nan,        nan, 0.99948925,        nan,        nan,\n",
       "        0.99948925,        nan,        nan, 0.99997377,        nan,\n",
       "               nan, 0.99995901,        nan,        nan, 0.99996065,\n",
       "               nan,        nan, 0.99995983,        nan,        nan,\n",
       "        0.99995983,        nan,        nan, 0.99995983,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan]),\n",
       " 'split3_train_score': array([       nan, 0.71828075,        nan,        nan, 0.71828075,\n",
       "               nan,        nan, 0.71828075,        nan,        nan,\n",
       "        0.71828075,        nan,        nan, 0.71828075,        nan,\n",
       "               nan, 0.71828075,        nan,        nan, 0.71862098,\n",
       "               nan,        nan, 0.71862098,        nan,        nan,\n",
       "        0.71862098,        nan,        nan, 0.71862098,        nan,\n",
       "               nan, 0.71862098,        nan,        nan, 0.71862098,\n",
       "               nan,        nan, 0.71916535,        nan,        nan,\n",
       "        0.71916535,        nan,        nan, 0.71916535,        nan,\n",
       "               nan, 0.71916535,        nan,        nan, 0.71916535,\n",
       "               nan,        nan, 0.71916535,        nan,        nan,\n",
       "        0.72019669,        nan,        nan, 0.72019669,        nan,\n",
       "               nan, 0.72019669,        nan,        nan, 0.72019669,\n",
       "               nan,        nan, 0.72019669,        nan,        nan,\n",
       "        0.72019669,        nan,        nan, 0.72196917,        nan,\n",
       "               nan, 0.72196917,        nan,        nan, 0.72196917,\n",
       "               nan,        nan, 0.72196917,        nan,        nan,\n",
       "        0.72196917,        nan,        nan, 0.72196917,        nan,\n",
       "               nan, 0.72446965,        nan,        nan, 0.72446965,\n",
       "               nan,        nan, 0.72446965,        nan,        nan,\n",
       "        0.72446965,        nan,        nan, 0.72446965,        nan,\n",
       "               nan, 0.72446965,        nan,        nan, 0.72807199,\n",
       "               nan,        nan, 0.72807199,        nan,        nan,\n",
       "        0.72807199,        nan,        nan, 0.72807199,        nan,\n",
       "               nan, 0.72807199,        nan,        nan, 0.72807199,\n",
       "               nan,        nan, 0.73156693,        nan,        nan,\n",
       "        0.73156693,        nan,        nan, 0.73156693,        nan,\n",
       "               nan, 0.73156693,        nan,        nan, 0.73156693,\n",
       "               nan,        nan, 0.73156693,        nan,        nan,\n",
       "        0.73467326,        nan,        nan, 0.73467326,        nan,\n",
       "               nan, 0.73467326,        nan,        nan, 0.73467326,\n",
       "               nan,        nan, 0.73467326,        nan,        nan,\n",
       "        0.73467326,        nan,        nan, 0.73848712,        nan,\n",
       "               nan, 0.73848712,        nan,        nan, 0.73848712,\n",
       "               nan,        nan, 0.73848712,        nan,        nan,\n",
       "        0.73848712,        nan,        nan, 0.73848712,        nan,\n",
       "               nan, 0.74350038,        nan,        nan, 0.74350038,\n",
       "               nan,        nan, 0.74350038,        nan,        nan,\n",
       "        0.74350038,        nan,        nan, 0.74350038,        nan,\n",
       "               nan, 0.74350038,        nan,        nan, 0.75016479,\n",
       "               nan,        nan, 0.75016479,        nan,        nan,\n",
       "        0.75016479,        nan,        nan, 0.75016479,        nan,\n",
       "               nan, 0.75016479,        nan,        nan, 0.75016479,\n",
       "               nan,        nan, 0.7588058 ,        nan,        nan,\n",
       "        0.7588058 ,        nan,        nan, 0.7588058 ,        nan,\n",
       "               nan, 0.7588058 ,        nan,        nan, 0.7588058 ,\n",
       "               nan,        nan, 0.7588058 ,        nan,        nan,\n",
       "        0.7685208 ,        nan,        nan, 0.7685208 ,        nan,\n",
       "               nan, 0.7685208 ,        nan,        nan, 0.7685208 ,\n",
       "               nan,        nan, 0.7685208 ,        nan,        nan,\n",
       "        0.7685208 ,        nan,        nan, 0.78001072,        nan,\n",
       "               nan, 0.78001072,        nan,        nan, 0.78001072,\n",
       "               nan,        nan, 0.78001072,        nan,        nan,\n",
       "        0.78001072,        nan,        nan, 0.78001072,        nan,\n",
       "               nan, 0.79490131,        nan,        nan, 0.79490131,\n",
       "               nan,        nan, 0.79490131,        nan,        nan,\n",
       "        0.79490131,        nan,        nan, 0.79490131,        nan,\n",
       "               nan, 0.79490131,        nan,        nan, 0.81363936,\n",
       "               nan,        nan, 0.81363936,        nan,        nan,\n",
       "        0.81363936,        nan,        nan, 0.81363936,        nan,\n",
       "               nan, 0.81363936,        nan,        nan, 0.81363936,\n",
       "               nan,        nan, 0.83565181,        nan,        nan,\n",
       "        0.83565181,        nan,        nan, 0.83565181,        nan,\n",
       "               nan, 0.83565181,        nan,        nan, 0.83565181,\n",
       "               nan,        nan, 0.83565181,        nan,        nan,\n",
       "        0.86025821,        nan,        nan, 0.86025821,        nan,\n",
       "               nan, 0.86025821,        nan,        nan, 0.86025821,\n",
       "               nan,        nan, 0.86025821,        nan,        nan,\n",
       "        0.86025821,        nan,        nan, 0.88624931,        nan,\n",
       "               nan, 0.88624931,        nan,        nan, 0.88624931,\n",
       "               nan,        nan, 0.88624931,        nan,        nan,\n",
       "        0.88624931,        nan,        nan, 0.88624931,        nan,\n",
       "               nan, 0.91227319,        nan,        nan, 0.91227319,\n",
       "               nan,        nan, 0.91227319,        nan,        nan,\n",
       "        0.91227319,        nan,        nan, 0.91227319,        nan,\n",
       "               nan, 0.91227319,        nan,        nan, 0.93645492,\n",
       "               nan,        nan, 0.93645492,        nan,        nan,\n",
       "        0.93645492,        nan,        nan, 0.93645492,        nan,\n",
       "               nan, 0.93645492,        nan,        nan, 0.93645492,\n",
       "               nan,        nan, 0.95643092,        nan,        nan,\n",
       "        0.95643092,        nan,        nan, 0.95643092,        nan,\n",
       "               nan, 0.95643092,        nan,        nan, 0.95643092,\n",
       "               nan,        nan, 0.95643092,        nan,        nan,\n",
       "        0.97115754,        nan,        nan, 0.97115754,        nan,\n",
       "               nan, 0.97115754,        nan,        nan, 0.97115754,\n",
       "               nan,        nan, 0.97115754,        nan,        nan,\n",
       "        0.97115754,        nan,        nan, 0.98147592,        nan,\n",
       "               nan, 0.98147592,        nan,        nan, 0.98147592,\n",
       "               nan,        nan, 0.98147592,        nan,        nan,\n",
       "        0.98147592,        nan,        nan, 0.98147592,        nan,\n",
       "               nan, 0.98918397,        nan,        nan, 0.98918397,\n",
       "               nan,        nan, 0.98918397,        nan,        nan,\n",
       "        0.98918397,        nan,        nan, 0.98918397,        nan,\n",
       "               nan, 0.98918397,        nan,        nan, 0.99430382,\n",
       "               nan,        nan, 0.99430136,        nan,        nan,\n",
       "        0.99430136,        nan,        nan, 0.99430136,        nan,\n",
       "               nan, 0.99430136,        nan,        nan, 0.99430136,\n",
       "               nan,        nan, 0.99781679,        nan,        nan,\n",
       "        0.99782581,        nan,        nan, 0.99782171,        nan,\n",
       "               nan, 0.99782171,        nan,        nan, 0.99782171,\n",
       "               nan,        nan, 0.99782171,        nan,        nan,\n",
       "        0.99958107,        nan,        nan, 0.9995909 ,        nan,\n",
       "               nan, 0.99958844,        nan,        nan, 0.99958844,\n",
       "               nan,        nan, 0.99958844,        nan,        nan,\n",
       "        0.99958844,        nan,        nan, 0.99996557,        nan,\n",
       "               nan, 0.99996557,        nan,        nan, 0.99996557,\n",
       "               nan,        nan, 0.99996639,        nan,        nan,\n",
       "        0.99996639,        nan,        nan, 0.99996639,        nan,\n",
       "               nan, 0.99999918,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan]),\n",
       " 'split4_train_score': array([       nan, 0.73093566,        nan,        nan, 0.73093566,\n",
       "               nan,        nan, 0.73093566,        nan,        nan,\n",
       "        0.73093566,        nan,        nan, 0.73093566,        nan,\n",
       "               nan, 0.73093566,        nan,        nan, 0.7318096 ,\n",
       "               nan,        nan, 0.7318096 ,        nan,        nan,\n",
       "        0.7318096 ,        nan,        nan, 0.7318096 ,        nan,\n",
       "               nan, 0.7318096 ,        nan,        nan, 0.7318096 ,\n",
       "               nan,        nan, 0.73295244,        nan,        nan,\n",
       "        0.73295244,        nan,        nan, 0.73295244,        nan,\n",
       "               nan, 0.73295244,        nan,        nan, 0.73295244,\n",
       "               nan,        nan, 0.73295244,        nan,        nan,\n",
       "        0.73463719,        nan,        nan, 0.73463719,        nan,\n",
       "               nan, 0.73463719,        nan,        nan, 0.73463719,\n",
       "               nan,        nan, 0.73463719,        nan,        nan,\n",
       "        0.73463719,        nan,        nan, 0.73662774,        nan,\n",
       "               nan, 0.73662774,        nan,        nan, 0.73662774,\n",
       "               nan,        nan, 0.73662774,        nan,        nan,\n",
       "        0.73662774,        nan,        nan, 0.73662774,        nan,\n",
       "               nan, 0.73871093,        nan,        nan, 0.73871093,\n",
       "               nan,        nan, 0.73871093,        nan,        nan,\n",
       "        0.73871093,        nan,        nan, 0.73871093,        nan,\n",
       "               nan, 0.73871093,        nan,        nan, 0.74097694,\n",
       "               nan,        nan, 0.74097694,        nan,        nan,\n",
       "        0.74097694,        nan,        nan, 0.74097694,        nan,\n",
       "               nan, 0.74097694,        nan,        nan, 0.74097694,\n",
       "               nan,        nan, 0.74360778,        nan,        nan,\n",
       "        0.74360778,        nan,        nan, 0.74360778,        nan,\n",
       "               nan, 0.74360778,        nan,        nan, 0.74360778,\n",
       "               nan,        nan, 0.74360778,        nan,        nan,\n",
       "        0.74717076,        nan,        nan, 0.74717076,        nan,\n",
       "               nan, 0.74717076,        nan,        nan, 0.74717076,\n",
       "               nan,        nan, 0.74717076,        nan,        nan,\n",
       "        0.74717076,        nan,        nan, 0.75160523,        nan,\n",
       "               nan, 0.75160523,        nan,        nan, 0.75160523,\n",
       "               nan,        nan, 0.75160523,        nan,        nan,\n",
       "        0.75160523,        nan,        nan, 0.75160523,        nan,\n",
       "               nan, 0.75647174,        nan,        nan, 0.75647174,\n",
       "               nan,        nan, 0.75647174,        nan,        nan,\n",
       "        0.75647174,        nan,        nan, 0.75647174,        nan,\n",
       "               nan, 0.75647174,        nan,        nan, 0.76245733,\n",
       "               nan,        nan, 0.76245733,        nan,        nan,\n",
       "        0.76245733,        nan,        nan, 0.76245733,        nan,\n",
       "               nan, 0.76245733,        nan,        nan, 0.76245733,\n",
       "               nan,        nan, 0.7703441 ,        nan,        nan,\n",
       "        0.7703441 ,        nan,        nan, 0.7703441 ,        nan,\n",
       "               nan, 0.7703441 ,        nan,        nan, 0.7703441 ,\n",
       "               nan,        nan, 0.7703441 ,        nan,        nan,\n",
       "        0.77956883,        nan,        nan, 0.77956883,        nan,\n",
       "               nan, 0.77956883,        nan,        nan, 0.77956883,\n",
       "               nan,        nan, 0.77956883,        nan,        nan,\n",
       "        0.77956883,        nan,        nan, 0.79063901,        nan,\n",
       "               nan, 0.79063901,        nan,        nan, 0.79063901,\n",
       "               nan,        nan, 0.79063901,        nan,        nan,\n",
       "        0.79063901,        nan,        nan, 0.79063901,        nan,\n",
       "               nan, 0.80459335,        nan,        nan, 0.80459335,\n",
       "               nan,        nan, 0.80459335,        nan,        nan,\n",
       "        0.80459335,        nan,        nan, 0.80459335,        nan,\n",
       "               nan, 0.80459335,        nan,        nan, 0.82191473,\n",
       "               nan,        nan, 0.82191473,        nan,        nan,\n",
       "        0.82191473,        nan,        nan, 0.82191473,        nan,\n",
       "               nan, 0.82191473,        nan,        nan, 0.82191473,\n",
       "               nan,        nan, 0.84248592,        nan,        nan,\n",
       "        0.84248592,        nan,        nan, 0.84248592,        nan,\n",
       "               nan, 0.84248592,        nan,        nan, 0.84248592,\n",
       "               nan,        nan, 0.84248592,        nan,        nan,\n",
       "        0.86580027,        nan,        nan, 0.86580027,        nan,\n",
       "               nan, 0.86580027,        nan,        nan, 0.86580027,\n",
       "               nan,        nan, 0.86580027,        nan,        nan,\n",
       "        0.86580027,        nan,        nan, 0.89069033,        nan,\n",
       "               nan, 0.89069033,        nan,        nan, 0.89069033,\n",
       "               nan,        nan, 0.89069033,        nan,        nan,\n",
       "        0.89069033,        nan,        nan, 0.89069033,        nan,\n",
       "               nan, 0.91538937,        nan,        nan, 0.91538937,\n",
       "               nan,        nan, 0.91538937,        nan,        nan,\n",
       "        0.91538937,        nan,        nan, 0.91538937,        nan,\n",
       "               nan, 0.91538937,        nan,        nan, 0.93769614,\n",
       "               nan,        nan, 0.93769614,        nan,        nan,\n",
       "        0.93769614,        nan,        nan, 0.93769614,        nan,\n",
       "               nan, 0.93769614,        nan,        nan, 0.93769614,\n",
       "               nan,        nan, 0.95560453,        nan,        nan,\n",
       "        0.95560453,        nan,        nan, 0.95560453,        nan,\n",
       "               nan, 0.95560453,        nan,        nan, 0.95560453,\n",
       "               nan,        nan, 0.95560453,        nan,        nan,\n",
       "        0.96940064,        nan,        nan, 0.96940064,        nan,\n",
       "               nan, 0.96940064,        nan,        nan, 0.96940064,\n",
       "               nan,        nan, 0.96940064,        nan,        nan,\n",
       "        0.96940064,        nan,        nan, 0.98017157,        nan,\n",
       "               nan, 0.98017157,        nan,        nan, 0.98017157,\n",
       "               nan,        nan, 0.98017157,        nan,        nan,\n",
       "        0.98017157,        nan,        nan, 0.98017157,        nan,\n",
       "               nan, 0.98803703,        nan,        nan, 0.98803129,\n",
       "               nan,        nan, 0.98803129,        nan,        nan,\n",
       "        0.98803129,        nan,        nan, 0.98803129,        nan,\n",
       "               nan, 0.98803129,        nan,        nan, 0.99364467,\n",
       "               nan,        nan, 0.99366681,        nan,        nan,\n",
       "        0.99366681,        nan,        nan, 0.99366681,        nan,\n",
       "               nan, 0.99366681,        nan,        nan, 0.99366681,\n",
       "               nan,        nan, 0.99742819,        nan,        nan,\n",
       "        0.99742409,        nan,        nan, 0.99743147,        nan,\n",
       "               nan, 0.99742901,        nan,        nan, 0.99742901,\n",
       "               nan,        nan, 0.99742901,        nan,        nan,\n",
       "        0.99931544,        nan,        nan, 0.99932856,        nan,\n",
       "               nan, 0.99932856,        nan,        nan, 0.99932856,\n",
       "               nan,        nan, 0.99932856,        nan,        nan,\n",
       "        0.99932856,        nan,        nan, 0.99995081,        nan,\n",
       "               nan, 0.99994507,        nan,        nan, 0.99994589,\n",
       "               nan,        nan, 0.99994507,        nan,        nan,\n",
       "        0.99994507,        nan,        nan, 0.99994507,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan]),\n",
       " 'mean_train_score': array([       nan, 0.72708033,        nan,        nan, 0.72708033,\n",
       "               nan,        nan, 0.72708033,        nan,        nan,\n",
       "        0.72708033,        nan,        nan, 0.72708033,        nan,\n",
       "               nan, 0.72708033,        nan,        nan, 0.72772702,\n",
       "               nan,        nan, 0.72772702,        nan,        nan,\n",
       "        0.72772702,        nan,        nan, 0.72772702,        nan,\n",
       "               nan, 0.72772702,        nan,        nan, 0.72772702,\n",
       "               nan,        nan, 0.72856936,        nan,        nan,\n",
       "        0.72856936,        nan,        nan, 0.72856936,        nan,\n",
       "               nan, 0.72856936,        nan,        nan, 0.72856936,\n",
       "               nan,        nan, 0.72856936,        nan,        nan,\n",
       "        0.72969286,        nan,        nan, 0.72969286,        nan,\n",
       "               nan, 0.72969286,        nan,        nan, 0.72969286,\n",
       "               nan,        nan, 0.72969286,        nan,        nan,\n",
       "        0.72969286,        nan,        nan, 0.73120693,        nan,\n",
       "               nan, 0.73120693,        nan,        nan, 0.73120693,\n",
       "               nan,        nan, 0.73120693,        nan,        nan,\n",
       "        0.73120693,        nan,        nan, 0.73120693,        nan,\n",
       "               nan, 0.73308558,        nan,        nan, 0.73308558,\n",
       "               nan,        nan, 0.73308558,        nan,        nan,\n",
       "        0.73308558,        nan,        nan, 0.73308558,        nan,\n",
       "               nan, 0.73308558,        nan,        nan, 0.73552323,\n",
       "               nan,        nan, 0.73552323,        nan,        nan,\n",
       "        0.73552323,        nan,        nan, 0.73552323,        nan,\n",
       "               nan, 0.73552323,        nan,        nan, 0.73552323,\n",
       "               nan,        nan, 0.73843274,        nan,        nan,\n",
       "        0.73843274,        nan,        nan, 0.73843274,        nan,\n",
       "               nan, 0.73843274,        nan,        nan, 0.73843274,\n",
       "               nan,        nan, 0.73843274,        nan,        nan,\n",
       "        0.74185183,        nan,        nan, 0.74185183,        nan,\n",
       "               nan, 0.74185183,        nan,        nan, 0.74185183,\n",
       "               nan,        nan, 0.74185183,        nan,        nan,\n",
       "        0.74185183,        nan,        nan, 0.74607519,        nan,\n",
       "               nan, 0.74607519,        nan,        nan, 0.74607519,\n",
       "               nan,        nan, 0.74607519,        nan,        nan,\n",
       "        0.74607519,        nan,        nan, 0.74607519,        nan,\n",
       "               nan, 0.75127301,        nan,        nan, 0.75127301,\n",
       "               nan,        nan, 0.75127301,        nan,        nan,\n",
       "        0.75127301,        nan,        nan, 0.75127301,        nan,\n",
       "               nan, 0.75127301,        nan,        nan, 0.75779915,\n",
       "               nan,        nan, 0.75779915,        nan,        nan,\n",
       "        0.75779915,        nan,        nan, 0.75779915,        nan,\n",
       "               nan, 0.75779915,        nan,        nan, 0.75779915,\n",
       "               nan,        nan, 0.76571821,        nan,        nan,\n",
       "        0.76571821,        nan,        nan, 0.76571821,        nan,\n",
       "               nan, 0.76571821,        nan,        nan, 0.76571821,\n",
       "               nan,        nan, 0.76571821,        nan,        nan,\n",
       "        0.77519919,        nan,        nan, 0.77519919,        nan,\n",
       "               nan, 0.77519919,        nan,        nan, 0.77519919,\n",
       "               nan,        nan, 0.77519919,        nan,        nan,\n",
       "        0.77519919,        nan,        nan, 0.78675241,        nan,\n",
       "               nan, 0.78675241,        nan,        nan, 0.78675241,\n",
       "               nan,        nan, 0.78675241,        nan,        nan,\n",
       "        0.78675241,        nan,        nan, 0.78675241,        nan,\n",
       "               nan, 0.80124823,        nan,        nan, 0.80124823,\n",
       "               nan,        nan, 0.80124823,        nan,        nan,\n",
       "        0.80124823,        nan,        nan, 0.80124823,        nan,\n",
       "               nan, 0.80124823,        nan,        nan, 0.81927919,\n",
       "               nan,        nan, 0.81927919,        nan,        nan,\n",
       "        0.81927919,        nan,        nan, 0.81927919,        nan,\n",
       "               nan, 0.81927919,        nan,        nan, 0.81927919,\n",
       "               nan,        nan, 0.84048152,        nan,        nan,\n",
       "        0.84048152,        nan,        nan, 0.84048152,        nan,\n",
       "               nan, 0.84048152,        nan,        nan, 0.84048152,\n",
       "               nan,        nan, 0.84048152,        nan,        nan,\n",
       "        0.86439719,        nan,        nan, 0.86439719,        nan,\n",
       "               nan, 0.86439719,        nan,        nan, 0.86439719,\n",
       "               nan,        nan, 0.86439719,        nan,        nan,\n",
       "        0.86439719,        nan,        nan, 0.88961678,        nan,\n",
       "               nan, 0.88961678,        nan,        nan, 0.88961678,\n",
       "               nan,        nan, 0.88961678,        nan,        nan,\n",
       "        0.88961678,        nan,        nan, 0.88961678,        nan,\n",
       "               nan, 0.91475887,        nan,        nan, 0.91475887,\n",
       "               nan,        nan, 0.91475887,        nan,        nan,\n",
       "        0.91475887,        nan,        nan, 0.91475887,        nan,\n",
       "               nan, 0.91475887,        nan,        nan, 0.93787358,\n",
       "               nan,        nan, 0.93787358,        nan,        nan,\n",
       "        0.93787358,        nan,        nan, 0.93787358,        nan,\n",
       "               nan, 0.93787358,        nan,        nan, 0.93787358,\n",
       "               nan,        nan, 0.95686972,        nan,        nan,\n",
       "        0.95686972,        nan,        nan, 0.95686972,        nan,\n",
       "               nan, 0.95686972,        nan,        nan, 0.95686972,\n",
       "               nan,        nan, 0.95686972,        nan,        nan,\n",
       "        0.97097833,        nan,        nan, 0.97097833,        nan,\n",
       "               nan, 0.97097833,        nan,        nan, 0.97097833,\n",
       "               nan,        nan, 0.97097833,        nan,        nan,\n",
       "        0.97097833,        nan,        nan, 0.98120599,        nan,\n",
       "               nan, 0.98120566,        nan,        nan, 0.98120566,\n",
       "               nan,        nan, 0.98120566,        nan,        nan,\n",
       "        0.98120566,        nan,        nan, 0.98120566,        nan,\n",
       "               nan, 0.98871061,        nan,        nan, 0.98871257,\n",
       "               nan,        nan, 0.98871257,        nan,        nan,\n",
       "        0.98871257,        nan,        nan, 0.98871257,        nan,\n",
       "               nan, 0.98871257,        nan,        nan, 0.99396795,\n",
       "               nan,        nan, 0.99397008,        nan,        nan,\n",
       "        0.99396959,        nan,        nan, 0.99396959,        nan,\n",
       "               nan, 0.99396959,        nan,        nan, 0.99396959,\n",
       "               nan,        nan, 0.9975421 ,        nan,        nan,\n",
       "        0.99754128,        nan,        nan, 0.99754144,        nan,\n",
       "               nan, 0.99754128,        nan,        nan, 0.99754128,\n",
       "               nan,        nan, 0.99754128,        nan,        nan,\n",
       "        0.99941713,        nan,        nan, 0.99942466,        nan,\n",
       "               nan, 0.99942499,        nan,        nan, 0.99942434,\n",
       "               nan,        nan, 0.99942434,        nan,        nan,\n",
       "        0.99942434,        nan,        nan, 0.99995686,        nan,\n",
       "               nan, 0.99995309,        nan,        nan, 0.99995375,\n",
       "               nan,        nan, 0.99995375,        nan,        nan,\n",
       "        0.99995375,        nan,        nan, 0.99995375,        nan,\n",
       "               nan, 0.99999918,        nan,        nan, 0.99999918,\n",
       "               nan,        nan, 0.99999934,        nan,        nan,\n",
       "        0.99999934,        nan,        nan, 0.99999934,        nan,\n",
       "               nan, 0.99999934,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan,\n",
       "               nan, 1.        ,        nan,        nan, 1.        ,\n",
       "               nan,        nan, 1.        ,        nan,        nan,\n",
       "        1.        ,        nan,        nan, 1.        ,        nan]),\n",
       " 'std_train_score': array([           nan, 8.20903424e-03,            nan,            nan,\n",
       "        8.20903424e-03,            nan,            nan, 8.20903424e-03,\n",
       "                   nan,            nan, 8.20903424e-03,            nan,\n",
       "                   nan, 8.20903424e-03,            nan,            nan,\n",
       "        8.20903424e-03,            nan,            nan, 8.35441376e-03,\n",
       "                   nan,            nan, 8.35441376e-03,            nan,\n",
       "                   nan, 8.35441376e-03,            nan,            nan,\n",
       "        8.35441376e-03,            nan,            nan, 8.35441376e-03,\n",
       "                   nan,            nan, 8.35441376e-03,            nan,\n",
       "                   nan, 8.49682089e-03,            nan,            nan,\n",
       "        8.49682089e-03,            nan,            nan, 8.49682089e-03,\n",
       "                   nan,            nan, 8.49682089e-03,            nan,\n",
       "                   nan, 8.49682089e-03,            nan,            nan,\n",
       "        8.49682089e-03,            nan,            nan, 8.51323778e-03,\n",
       "                   nan,            nan, 8.51323778e-03,            nan,\n",
       "                   nan, 8.51323778e-03,            nan,            nan,\n",
       "        8.51323778e-03,            nan,            nan, 8.51323778e-03,\n",
       "                   nan,            nan, 8.51323778e-03,            nan,\n",
       "                   nan, 8.28978424e-03,            nan,            nan,\n",
       "        8.28978424e-03,            nan,            nan, 8.28978424e-03,\n",
       "                   nan,            nan, 8.28978424e-03,            nan,\n",
       "                   nan, 8.28978424e-03,            nan,            nan,\n",
       "        8.28978424e-03,            nan,            nan, 7.90731602e-03,\n",
       "                   nan,            nan, 7.90731602e-03,            nan,\n",
       "                   nan, 7.90731602e-03,            nan,            nan,\n",
       "        7.90731602e-03,            nan,            nan, 7.90731602e-03,\n",
       "                   nan,            nan, 7.90731602e-03,            nan,\n",
       "                   nan, 7.28120746e-03,            nan,            nan,\n",
       "        7.28120746e-03,            nan,            nan, 7.28120746e-03,\n",
       "                   nan,            nan, 7.28120746e-03,            nan,\n",
       "                   nan, 7.28120746e-03,            nan,            nan,\n",
       "        7.28120746e-03,            nan,            nan, 6.80478233e-03,\n",
       "                   nan,            nan, 6.80478233e-03,            nan,\n",
       "                   nan, 6.80478233e-03,            nan,            nan,\n",
       "        6.80478233e-03,            nan,            nan, 6.80478233e-03,\n",
       "                   nan,            nan, 6.80478233e-03,            nan,\n",
       "                   nan, 6.81350288e-03,            nan,            nan,\n",
       "        6.81350288e-03,            nan,            nan, 6.81350288e-03,\n",
       "                   nan,            nan, 6.81350288e-03,            nan,\n",
       "                   nan, 6.81350288e-03,            nan,            nan,\n",
       "        6.81350288e-03,            nan,            nan, 7.15595260e-03,\n",
       "                   nan,            nan, 7.15595260e-03,            nan,\n",
       "                   nan, 7.15595260e-03,            nan,            nan,\n",
       "        7.15595260e-03,            nan,            nan, 7.15595260e-03,\n",
       "                   nan,            nan, 7.15595260e-03,            nan,\n",
       "                   nan, 7.20671735e-03,            nan,            nan,\n",
       "        7.20671735e-03,            nan,            nan, 7.20671735e-03,\n",
       "                   nan,            nan, 7.20671735e-03,            nan,\n",
       "                   nan, 7.20671735e-03,            nan,            nan,\n",
       "        7.20671735e-03,            nan,            nan, 6.77911869e-03,\n",
       "                   nan,            nan, 6.77911869e-03,            nan,\n",
       "                   nan, 6.77911869e-03,            nan,            nan,\n",
       "        6.77911869e-03,            nan,            nan, 6.77911869e-03,\n",
       "                   nan,            nan, 6.77911869e-03,            nan,\n",
       "                   nan, 6.06264898e-03,            nan,            nan,\n",
       "        6.06264898e-03,            nan,            nan, 6.06264898e-03,\n",
       "                   nan,            nan, 6.06264898e-03,            nan,\n",
       "                   nan, 6.06264898e-03,            nan,            nan,\n",
       "        6.06264898e-03,            nan,            nan, 5.41128160e-03,\n",
       "                   nan,            nan, 5.41128160e-03,            nan,\n",
       "                   nan, 5.41128160e-03,            nan,            nan,\n",
       "        5.41128160e-03,            nan,            nan, 5.41128160e-03,\n",
       "                   nan,            nan, 5.41128160e-03,            nan,\n",
       "                   nan, 4.89743872e-03,            nan,            nan,\n",
       "        4.89743872e-03,            nan,            nan, 4.89743872e-03,\n",
       "                   nan,            nan, 4.89743872e-03,            nan,\n",
       "                   nan, 4.89743872e-03,            nan,            nan,\n",
       "        4.89743872e-03,            nan,            nan, 4.03466016e-03,\n",
       "                   nan,            nan, 4.03466016e-03,            nan,\n",
       "                   nan, 4.03466016e-03,            nan,            nan,\n",
       "        4.03466016e-03,            nan,            nan, 4.03466016e-03,\n",
       "                   nan,            nan, 4.03466016e-03,            nan,\n",
       "                   nan, 3.16670893e-03,            nan,            nan,\n",
       "        3.16670893e-03,            nan,            nan, 3.16670893e-03,\n",
       "                   nan,            nan, 3.16670893e-03,            nan,\n",
       "                   nan, 3.16670893e-03,            nan,            nan,\n",
       "        3.16670893e-03,            nan,            nan, 2.48144805e-03,\n",
       "                   nan,            nan, 2.48144805e-03,            nan,\n",
       "                   nan, 2.48144805e-03,            nan,            nan,\n",
       "        2.48144805e-03,            nan,            nan, 2.48144805e-03,\n",
       "                   nan,            nan, 2.48144805e-03,            nan,\n",
       "                   nan, 2.16464587e-03,            nan,            nan,\n",
       "        2.16464587e-03,            nan,            nan, 2.16464587e-03,\n",
       "                   nan,            nan, 2.16464587e-03,            nan,\n",
       "                   nan, 2.16464587e-03,            nan,            nan,\n",
       "        2.16464587e-03,            nan,            nan, 2.08043957e-03,\n",
       "                   nan,            nan, 2.08043957e-03,            nan,\n",
       "                   nan, 2.08043957e-03,            nan,            nan,\n",
       "        2.08043957e-03,            nan,            nan, 2.08043957e-03,\n",
       "                   nan,            nan, 2.08043957e-03,            nan,\n",
       "                   nan, 1.92720022e-03,            nan,            nan,\n",
       "        1.92720022e-03,            nan,            nan, 1.92720022e-03,\n",
       "                   nan,            nan, 1.92720022e-03,            nan,\n",
       "                   nan, 1.92720022e-03,            nan,            nan,\n",
       "        1.92720022e-03,            nan,            nan, 1.51057240e-03,\n",
       "                   nan,            nan, 1.51057240e-03,            nan,\n",
       "                   nan, 1.51057240e-03,            nan,            nan,\n",
       "        1.51057240e-03,            nan,            nan, 1.51057240e-03,\n",
       "                   nan,            nan, 1.51057240e-03,            nan,\n",
       "                   nan, 1.15737307e-03,            nan,            nan,\n",
       "        1.15737307e-03,            nan,            nan, 1.15737307e-03,\n",
       "                   nan,            nan, 1.15737307e-03,            nan,\n",
       "                   nan, 1.15737307e-03,            nan,            nan,\n",
       "        1.15737307e-03,            nan,            nan, 9.24364547e-04,\n",
       "                   nan,            nan, 9.24364547e-04,            nan,\n",
       "                   nan, 9.24364547e-04,            nan,            nan,\n",
       "        9.24364547e-04,            nan,            nan, 9.24364547e-04,\n",
       "                   nan,            nan, 9.24364547e-04,            nan,\n",
       "                   nan, 5.40514415e-04,            nan,            nan,\n",
       "        5.40195261e-04,            nan,            nan, 5.40195261e-04,\n",
       "                   nan,            nan, 5.40195261e-04,            nan,\n",
       "                   nan, 5.40195261e-04,            nan,            nan,\n",
       "        5.40195261e-04,            nan,            nan, 5.23019305e-04,\n",
       "                   nan,            nan, 5.24330411e-04,            nan,\n",
       "                   nan, 5.24330411e-04,            nan,            nan,\n",
       "        5.24330411e-04,            nan,            nan, 5.24330411e-04,\n",
       "                   nan,            nan, 5.24330411e-04,            nan,\n",
       "                   nan, 4.53579226e-04,            nan,            nan,\n",
       "        4.51639285e-04,            nan,            nan, 4.52448724e-04,\n",
       "                   nan,            nan, 4.52448724e-04,            nan,\n",
       "                   nan, 4.52448724e-04,            nan,            nan,\n",
       "        4.52448724e-04,            nan,            nan, 3.04496026e-04,\n",
       "                   nan,            nan, 3.08480152e-04,            nan,\n",
       "                   nan, 3.05680205e-04,            nan,            nan,\n",
       "        3.06057521e-04,            nan,            nan, 3.06057521e-04,\n",
       "                   nan,            nan, 3.06057521e-04,            nan,\n",
       "                   nan, 1.29838185e-04,            nan,            nan,\n",
       "        1.38376027e-04,            nan,            nan, 1.38501532e-04,\n",
       "                   nan,            nan, 1.38867071e-04,            nan,\n",
       "                   nan, 1.38867071e-04,            nan,            nan,\n",
       "        1.38867071e-04,            nan,            nan, 1.83537336e-05,\n",
       "                   nan,            nan, 1.62992471e-05,            nan,\n",
       "                   nan, 1.60679483e-05,            nan,            nan,\n",
       "        1.63827721e-05,            nan,            nan, 1.63827721e-05,\n",
       "                   nan,            nan, 1.63827721e-05,            nan,\n",
       "                   nan, 1.27121439e-06,            nan,            nan,\n",
       "        1.64113067e-06,            nan,            nan, 1.31290454e-06,\n",
       "                   nan,            nan, 1.31290454e-06,            nan,\n",
       "                   nan, 1.31290454e-06,            nan,            nan,\n",
       "        1.31290454e-06,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan,\n",
       "                   nan, 0.00000000e+00,            nan,            nan,\n",
       "        0.00000000e+00,            nan,            nan, 0.00000000e+00,\n",
       "                   nan,            nan, 0.00000000e+00,            nan])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['headline_classifiers_tagalog/LOGREG_GSCV_Classifier.joblib']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, f'headline_classifiers_tagalog/LOGREG_GSCV_Classifier.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2160x432 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_search_results(grid):\n",
    "    \"\"\"\n",
    "    Params: \n",
    "        grid: A trained GridSearchCV object.\n",
    "    \"\"\"\n",
    "    ## Results from grid search\n",
    "    results = grid.cv_results_\n",
    "    means_test = results['mean_test_score']\n",
    "    stds_test = results['std_test_score']\n",
    "    means_train = results['mean_train_score']\n",
    "    stds_train = results['std_train_score']\n",
    "\n",
    "    ## Getting indexes of values per hyper-parameter\n",
    "    masks=[]\n",
    "    masks_names= list(grid.best_params_.keys())\n",
    "    for p_k, p_v in grid.best_params_.items():\n",
    "        masks.append(list(results['param_'+p_k].data==p_v))\n",
    "\n",
    "    params=grid.param_grid\n",
    "\n",
    "    ## Ploting results\n",
    "    fig, ax = plt.subplots(1,len(params),sharex='none', sharey='none',figsize=(30,6))\n",
    "    fig.suptitle('Score per parameter')\n",
    "    fig.text(0.04, 0.5, 'MEAN SCORE', va='center', rotation='vertical')\n",
    "    pram_preformace_in_best = {}\n",
    "    for i, p in enumerate(masks_names):\n",
    "        m = np.stack(masks[:i] + masks[i+1:])\n",
    "        pram_preformace_in_best\n",
    "        best_parms_mask = m.all(axis=0)\n",
    "        best_index = np.where(best_parms_mask)[0]\n",
    "        x = np.array(params[p])\n",
    "        y_1 = np.array(means_test[best_index])\n",
    "        e_1 = np.array(stds_test[best_index])\n",
    "        y_2 = np.array(means_train[best_index])\n",
    "        e_2 = np.array(stds_train[best_index])\n",
    "        ax[i].errorbar(x, y_1, e_1, linestyle='--', marker='o', label='test')\n",
    "        ax[i].errorbar(x, y_2, e_2, linestyle='-', marker='^',label='train' )\n",
    "        ax[i].set_xlabel(p.upper())\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_search_results(grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.2, random_state=45)\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5238, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = create_embedding_matrix(\n",
    "    'glove/glove.6B.100d.txt',\n",
    "    tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          1377000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 96, 128)           64128     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,442,429\n",
      "Trainable params: 1,442,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                116210    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116,221\n",
      "Trainable params: 116,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cvaal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Reshape:0\", shape=(None, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/dense/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 3s 7ms/step - loss: 0.6250 - accuracy: 0.7289 - val_loss: 0.5273 - val_accuracy: 0.8160\n",
      "Epoch 2/100\n",
      "164/164 [==============================] - 1s 6ms/step - loss: 0.3986 - accuracy: 0.8921 - val_loss: 0.4124 - val_accuracy: 0.8321\n",
      "Epoch 3/100\n",
      "164/164 [==============================] - 1s 6ms/step - loss: 0.2470 - accuracy: 0.9414 - val_loss: 0.3695 - val_accuracy: 0.8405\n",
      "Epoch 4/100\n",
      "164/164 [==============================] - 1s 6ms/step - loss: 0.1617 - accuracy: 0.9704 - val_loss: 0.3560 - val_accuracy: 0.8420\n",
      "Epoch 5/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.1108 - accuracy: 0.9845 - val_loss: 0.3569 - val_accuracy: 0.8382\n",
      "Epoch 6/100\n",
      "164/164 [==============================] - 1s 6ms/step - loss: 0.0782 - accuracy: 0.9914 - val_loss: 0.3625 - val_accuracy: 0.8359\n",
      "Epoch 7/100\n",
      "164/164 [==============================] - 1s 6ms/step - loss: 0.0568 - accuracy: 0.9952 - val_loss: 0.3713 - val_accuracy: 0.8351\n",
      "Epoch 8/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0423 - accuracy: 0.9973 - val_loss: 0.3884 - val_accuracy: 0.8328\n",
      "Epoch 9/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0323 - accuracy: 0.9983 - val_loss: 0.3983 - val_accuracy: 0.8351\n",
      "Epoch 10/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0252 - accuracy: 0.9987 - val_loss: 0.4116 - val_accuracy: 0.8321\n",
      "Epoch 11/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0199 - accuracy: 0.9990 - val_loss: 0.4240 - val_accuracy: 0.8313\n",
      "Epoch 12/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 0.0160 - accuracy: 0.9992 - val_loss: 0.4371 - val_accuracy: 0.8305\n",
      "Epoch 13/100\n",
      "164/164 [==============================] - 2s 10ms/step - loss: 0.0130 - accuracy: 0.9996 - val_loss: 0.4500 - val_accuracy: 0.8275\n",
      "Epoch 14/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0107 - accuracy: 0.9994 - val_loss: 0.4607 - val_accuracy: 0.8282\n",
      "Epoch 15/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0089 - accuracy: 0.9996 - val_loss: 0.4745 - val_accuracy: 0.8290\n",
      "Epoch 16/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0075 - accuracy: 0.9996 - val_loss: 0.4867 - val_accuracy: 0.8305\n",
      "Epoch 17/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0063 - accuracy: 0.9996 - val_loss: 0.4976 - val_accuracy: 0.8298\n",
      "Epoch 18/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 0.0055 - accuracy: 0.9996 - val_loss: 0.5100 - val_accuracy: 0.8313\n",
      "Epoch 19/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0047 - accuracy: 0.9998 - val_loss: 0.5219 - val_accuracy: 0.8305\n",
      "Epoch 20/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 0.5316 - val_accuracy: 0.8313\n",
      "Epoch 21/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9998 - val_loss: 0.5433 - val_accuracy: 0.8313\n",
      "Epoch 22/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.5528 - val_accuracy: 0.8321\n",
      "Epoch 23/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 0.0028 - accuracy: 0.9998 - val_loss: 0.5638 - val_accuracy: 0.8321\n",
      "Epoch 24/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 0.5729 - val_accuracy: 0.8305\n",
      "Epoch 25/100\n",
      "164/164 [==============================] - 2s 10ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.5832 - val_accuracy: 0.8290\n",
      "Epoch 26/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.5946 - val_accuracy: 0.8282\n",
      "Epoch 27/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.6032 - val_accuracy: 0.8290\n",
      "Epoch 28/100\n",
      "164/164 [==============================] - 2s 9ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.6147 - val_accuracy: 0.8290\n",
      "Epoch 29/100\n",
      "164/164 [==============================] - 1s 9ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.6229 - val_accuracy: 0.8267\n",
      "Epoch 30/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.6331 - val_accuracy: 0.8275\n",
      "Epoch 31/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.6404 - val_accuracy: 0.8267\n",
      "Epoch 32/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.6498 - val_accuracy: 0.8275\n",
      "Epoch 33/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.6582 - val_accuracy: 0.8267\n",
      "Epoch 34/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 9.5795e-04 - accuracy: 0.9998 - val_loss: 0.6687 - val_accuracy: 0.8252\n",
      "Epoch 35/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 8.7437e-04 - accuracy: 0.9998 - val_loss: 0.6782 - val_accuracy: 0.8244\n",
      "Epoch 36/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 8.0888e-04 - accuracy: 0.9998 - val_loss: 0.6864 - val_accuracy: 0.8244\n",
      "Epoch 37/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 7.5833e-04 - accuracy: 0.9998 - val_loss: 0.6950 - val_accuracy: 0.8229\n",
      "Epoch 38/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 7.1061e-04 - accuracy: 0.9998 - val_loss: 0.7052 - val_accuracy: 0.8214\n",
      "Epoch 39/100\n",
      "164/164 [==============================] - 1s 7ms/step - loss: 6.7175e-04 - accuracy: 0.9998 - val_loss: 0.7136 - val_accuracy: 0.8229\n",
      "Epoch 40/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 6.4940e-04 - accuracy: 0.9996 - val_loss: 0.7198 - val_accuracy: 0.8229\n",
      "Epoch 41/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 6.0274e-04 - accuracy: 0.9998 - val_loss: 0.7308 - val_accuracy: 0.8221\n",
      "Epoch 42/100\n",
      "164/164 [==============================] - 1s 9ms/step - loss: 5.6274e-04 - accuracy: 0.9998 - val_loss: 0.7424 - val_accuracy: 0.8198\n",
      "Epoch 43/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 5.3280e-04 - accuracy: 0.9998 - val_loss: 0.7492 - val_accuracy: 0.8221\n",
      "Epoch 44/100\n",
      "164/164 [==============================] - 2s 10ms/step - loss: 5.0640e-04 - accuracy: 0.9998 - val_loss: 0.7563 - val_accuracy: 0.8221\n",
      "Epoch 45/100\n",
      "164/164 [==============================] - 2s 12ms/step - loss: 4.9094e-04 - accuracy: 0.9998 - val_loss: 0.7637 - val_accuracy: 0.8221\n",
      "Epoch 46/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 4.6936e-04 - accuracy: 0.9998 - val_loss: 0.7730 - val_accuracy: 0.8221\n",
      "Epoch 47/100\n",
      "164/164 [==============================] - 1s 9ms/step - loss: 4.4858e-04 - accuracy: 0.9998 - val_loss: 0.7831 - val_accuracy: 0.8191\n",
      "Epoch 48/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 4.3105e-04 - accuracy: 0.9998 - val_loss: 0.7894 - val_accuracy: 0.8214\n",
      "Epoch 49/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 4.1519e-04 - accuracy: 0.9998 - val_loss: 0.7995 - val_accuracy: 0.8198\n",
      "Epoch 50/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 4.1381e-04 - accuracy: 0.9996 - val_loss: 0.8061 - val_accuracy: 0.8198\n",
      "Epoch 51/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 3.9189e-04 - accuracy: 0.9998 - val_loss: 0.8167 - val_accuracy: 0.8183\n",
      "Epoch 52/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 3.7875e-04 - accuracy: 0.9998 - val_loss: 0.8270 - val_accuracy: 0.8191\n",
      "Epoch 53/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 3.6723e-04 - accuracy: 0.9998 - val_loss: 0.8341 - val_accuracy: 0.8183\n",
      "Epoch 54/100\n",
      "164/164 [==============================] - 1s 9ms/step - loss: 3.5680e-04 - accuracy: 0.9998 - val_loss: 0.8425 - val_accuracy: 0.8176\n",
      "Epoch 55/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 3.4815e-04 - accuracy: 0.9998 - val_loss: 0.8518 - val_accuracy: 0.8191\n",
      "Epoch 56/100\n",
      "164/164 [==============================] - 1s 9ms/step - loss: 3.4103e-04 - accuracy: 0.9998 - val_loss: 0.8572 - val_accuracy: 0.8183\n",
      "Epoch 57/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 3.3753e-04 - accuracy: 0.9998 - val_loss: 0.8652 - val_accuracy: 0.8160\n",
      "Epoch 58/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 3.3294e-04 - accuracy: 0.9998 - val_loss: 0.8727 - val_accuracy: 0.8160\n",
      "Epoch 59/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 3.2480e-04 - accuracy: 0.9998 - val_loss: 0.8834 - val_accuracy: 0.8160\n",
      "Epoch 60/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 3.2205e-04 - accuracy: 0.9998 - val_loss: 0.8899 - val_accuracy: 0.8153\n",
      "Epoch 61/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 3.1525e-04 - accuracy: 0.9998 - val_loss: 0.8996 - val_accuracy: 0.8160\n",
      "Epoch 62/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 3.0993e-04 - accuracy: 0.9998 - val_loss: 0.9082 - val_accuracy: 0.8153\n",
      "Epoch 63/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 3.0814e-04 - accuracy: 0.9998 - val_loss: 0.9153 - val_accuracy: 0.8153\n",
      "Epoch 64/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 3.0299e-04 - accuracy: 0.9998 - val_loss: 0.9242 - val_accuracy: 0.8145\n",
      "Epoch 65/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.9940e-04 - accuracy: 0.9998 - val_loss: 0.9335 - val_accuracy: 0.8145\n",
      "Epoch 66/100\n",
      "164/164 [==============================] - 1s 9ms/step - loss: 2.9522e-04 - accuracy: 0.9998 - val_loss: 0.9418 - val_accuracy: 0.8145\n",
      "Epoch 67/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.9508e-04 - accuracy: 0.9998 - val_loss: 0.9498 - val_accuracy: 0.8145\n",
      "Epoch 68/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.9467e-04 - accuracy: 0.9998 - val_loss: 0.9557 - val_accuracy: 0.8145\n",
      "Epoch 69/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.9060e-04 - accuracy: 0.9998 - val_loss: 0.9648 - val_accuracy: 0.8145\n",
      "Epoch 70/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.8770e-04 - accuracy: 0.9998 - val_loss: 0.9741 - val_accuracy: 0.8145\n",
      "Epoch 71/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.8648e-04 - accuracy: 0.9998 - val_loss: 0.9839 - val_accuracy: 0.8145\n",
      "Epoch 72/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.8362e-04 - accuracy: 0.9998 - val_loss: 0.9924 - val_accuracy: 0.8145\n",
      "Epoch 73/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.8183e-04 - accuracy: 0.9998 - val_loss: 1.0010 - val_accuracy: 0.8145\n",
      "Epoch 74/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.8104e-04 - accuracy: 0.9998 - val_loss: 1.0104 - val_accuracy: 0.8153\n",
      "Epoch 75/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.7932e-04 - accuracy: 0.9998 - val_loss: 1.0196 - val_accuracy: 0.8153\n",
      "Epoch 76/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.7808e-04 - accuracy: 0.9998 - val_loss: 1.0268 - val_accuracy: 0.8153\n",
      "Epoch 77/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.7658e-04 - accuracy: 0.9998 - val_loss: 1.0347 - val_accuracy: 0.8153\n",
      "Epoch 78/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.7597e-04 - accuracy: 0.9998 - val_loss: 1.0435 - val_accuracy: 0.8153\n",
      "Epoch 79/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.7489e-04 - accuracy: 0.9998 - val_loss: 1.0528 - val_accuracy: 0.8145\n",
      "Epoch 80/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.7423e-04 - accuracy: 0.9998 - val_loss: 1.0594 - val_accuracy: 0.8153\n",
      "Epoch 81/100\n",
      "164/164 [==============================] - 2s 10ms/step - loss: 2.7333e-04 - accuracy: 0.9998 - val_loss: 1.0689 - val_accuracy: 0.8145\n",
      "Epoch 82/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.7282e-04 - accuracy: 0.9998 - val_loss: 1.0759 - val_accuracy: 0.8145\n",
      "Epoch 83/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.7248e-04 - accuracy: 0.9998 - val_loss: 1.0837 - val_accuracy: 0.8145\n",
      "Epoch 84/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.7146e-04 - accuracy: 0.9998 - val_loss: 1.0918 - val_accuracy: 0.8137\n",
      "Epoch 85/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.7207e-04 - accuracy: 0.9998 - val_loss: 1.0993 - val_accuracy: 0.8122\n",
      "Epoch 86/100\n",
      "164/164 [==============================] - 1s 9ms/step - loss: 2.7183e-04 - accuracy: 0.9998 - val_loss: 1.1111 - val_accuracy: 0.8153\n",
      "Epoch 87/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.7063e-04 - accuracy: 0.9998 - val_loss: 1.1187 - val_accuracy: 0.8145\n",
      "Epoch 88/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.7240e-04 - accuracy: 0.9998 - val_loss: 1.1252 - val_accuracy: 0.8137\n",
      "Epoch 89/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.7000e-04 - accuracy: 0.9998 - val_loss: 1.1336 - val_accuracy: 0.8130\n",
      "Epoch 90/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.6894e-04 - accuracy: 0.9998 - val_loss: 1.1427 - val_accuracy: 0.8130\n",
      "Epoch 91/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.6919e-04 - accuracy: 0.9998 - val_loss: 1.1506 - val_accuracy: 0.8122\n",
      "Epoch 92/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.6950e-04 - accuracy: 0.9998 - val_loss: 1.1561 - val_accuracy: 0.8122\n",
      "Epoch 93/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.6874e-04 - accuracy: 0.9998 - val_loss: 1.1658 - val_accuracy: 0.8122\n",
      "Epoch 94/100\n",
      "164/164 [==============================] - 1s 9ms/step - loss: 2.6853e-04 - accuracy: 0.9998 - val_loss: 1.1746 - val_accuracy: 0.8122\n",
      "Epoch 95/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.6825e-04 - accuracy: 0.9998 - val_loss: 1.1842 - val_accuracy: 0.8130\n",
      "Epoch 96/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.6740e-04 - accuracy: 0.9998 - val_loss: 1.1922 - val_accuracy: 0.8137\n",
      "Epoch 97/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.6779e-04 - accuracy: 0.9998 - val_loss: 1.2000 - val_accuracy: 0.8130\n",
      "Epoch 98/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.6762e-04 - accuracy: 0.9998 - val_loss: 1.2094 - val_accuracy: 0.8130\n",
      "Epoch 99/100\n",
      "164/164 [==============================] - 1s 9ms/step - loss: 2.6741e-04 - accuracy: 0.9998 - val_loss: 1.2163 - val_accuracy: 0.8122\n",
      "Epoch 100/100\n",
      "164/164 [==============================] - 1s 8ms/step - loss: 2.6703e-04 - accuracy: 0.9998 - val_loss: 1.2250 - val_accuracy: 0.8122\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9998090863227844\n",
      "Testing Accuracy:  0.8122137188911438\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(f\"Training Accuracy: {accuracy}\")\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(f\"Testing Accuracy:  {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Church of England Declares ‘Jesus Is Non-Binary’\n"
     ]
    }
   ],
   "source": [
    "print(sentences_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[484, 4, 932, 506, 10, 1112]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           688500    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 50)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                510       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 689,021\n",
      "Trainable params: 689,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "524/524 [==============================] - 5s 8ms/step - loss: 0.6134 - accuracy: 0.7003 - val_loss: 0.4681 - val_accuracy: 0.8023\n",
      "Epoch 2/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.3042 - accuracy: 0.8881 - val_loss: 0.3346 - val_accuracy: 0.8534\n",
      "Epoch 3/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.1335 - accuracy: 0.9586 - val_loss: 0.3388 - val_accuracy: 0.8603\n",
      "Epoch 4/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0532 - accuracy: 0.9878 - val_loss: 0.3775 - val_accuracy: 0.8466\n",
      "Epoch 5/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0196 - accuracy: 0.9981 - val_loss: 0.4128 - val_accuracy: 0.8389\n",
      "Epoch 6/20\n",
      "524/524 [==============================] - 4s 9ms/step - loss: 0.0085 - accuracy: 0.9994 - val_loss: 0.4433 - val_accuracy: 0.8389\n",
      "Epoch 7/20\n",
      "524/524 [==============================] - 5s 9ms/step - loss: 0.0040 - accuracy: 0.9996 - val_loss: 0.4853 - val_accuracy: 0.8435\n",
      "Epoch 8/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.5122 - val_accuracy: 0.8389\n",
      "Epoch 9/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.5397 - val_accuracy: 0.8382\n",
      "Epoch 10/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.5548 - val_accuracy: 0.8389\n",
      "Epoch 11/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.5714 - val_accuracy: 0.8397\n",
      "Epoch 12/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.5880 - val_accuracy: 0.8366\n",
      "Epoch 13/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 8.8158e-04 - accuracy: 0.9998 - val_loss: 0.6066 - val_accuracy: 0.8389\n",
      "Epoch 14/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 8.5662e-04 - accuracy: 0.9998 - val_loss: 0.6380 - val_accuracy: 0.8328\n",
      "Epoch 15/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.6332 - val_accuracy: 0.8374\n",
      "Epoch 16/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.6384 - val_accuracy: 0.8382\n",
      "Epoch 17/20\n",
      "524/524 [==============================] - 3s 6ms/step - loss: 7.2638e-04 - accuracy: 0.9998 - val_loss: 0.6564 - val_accuracy: 0.8366\n",
      "Epoch 18/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.6866 - val_accuracy: 0.8305\n",
      "Epoch 19/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.6705 - val_accuracy: 0.8389\n",
      "Epoch 20/20\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 6.9663e-04 - accuracy: 0.9998 - val_loss: 0.6790 - val_accuracy: 0.8374\n",
      "Training Accuracy: 0.9998\n",
      "Testing Accuracy:  0.8374\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          1377000   \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 100)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                1010      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,378,021\n",
      "Trainable params: 1,378,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen, \n",
    "                           trainable=True))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "524/524 [==============================] - 4s 6ms/step - loss: 0.6752 - accuracy: 0.5836 - val_loss: 0.6106 - val_accuracy: 0.7252\n",
      "Epoch 2/50\n",
      "524/524 [==============================] - 3s 6ms/step - loss: 0.5121 - accuracy: 0.7591 - val_loss: 0.4503 - val_accuracy: 0.7863\n",
      "Epoch 3/50\n",
      "524/524 [==============================] - 3s 6ms/step - loss: 0.3502 - accuracy: 0.8511 - val_loss: 0.4069 - val_accuracy: 0.8038\n",
      "Epoch 4/50\n",
      "524/524 [==============================] - 3s 6ms/step - loss: 0.2377 - accuracy: 0.9049 - val_loss: 0.3776 - val_accuracy: 0.8252\n",
      "Epoch 5/50\n",
      "524/524 [==============================] - 4s 7ms/step - loss: 0.1546 - accuracy: 0.9488 - val_loss: 0.3853 - val_accuracy: 0.8313\n",
      "Epoch 6/50\n",
      "524/524 [==============================] - 4s 7ms/step - loss: 0.0933 - accuracy: 0.9727 - val_loss: 0.4012 - val_accuracy: 0.8382\n",
      "Epoch 7/50\n",
      "524/524 [==============================] - 3s 7ms/step - loss: 0.0561 - accuracy: 0.9863 - val_loss: 0.4263 - val_accuracy: 0.8382\n",
      "Epoch 8/50\n",
      "524/524 [==============================] - 4s 7ms/step - loss: 0.0301 - accuracy: 0.9939 - val_loss: 0.4756 - val_accuracy: 0.8298\n",
      "Epoch 9/50\n",
      "524/524 [==============================] - 4s 7ms/step - loss: 0.0164 - accuracy: 0.9979 - val_loss: 0.4876 - val_accuracy: 0.8305\n",
      "Epoch 10/50\n",
      "524/524 [==============================] - 3s 7ms/step - loss: 0.0095 - accuracy: 0.9992 - val_loss: 0.5246 - val_accuracy: 0.8275\n",
      "Epoch 11/50\n",
      "524/524 [==============================] - 4s 7ms/step - loss: 0.0057 - accuracy: 0.9996 - val_loss: 0.5452 - val_accuracy: 0.8351\n",
      "Epoch 12/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0033 - accuracy: 0.9996 - val_loss: 0.5720 - val_accuracy: 0.8359\n",
      "Epoch 13/50\n",
      "524/524 [==============================] - 4s 7ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.5945 - val_accuracy: 0.8374\n",
      "Epoch 14/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.6481 - val_accuracy: 0.8275\n",
      "Epoch 15/50\n",
      "524/524 [==============================] - 4s 7ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.6582 - val_accuracy: 0.8412\n",
      "Epoch 16/50\n",
      "524/524 [==============================] - 3s 6ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.6661 - val_accuracy: 0.8298\n",
      "Epoch 17/50\n",
      "524/524 [==============================] - 3s 6ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.6759 - val_accuracy: 0.8351\n",
      "Epoch 18/50\n",
      "524/524 [==============================] - 4s 7ms/step - loss: 8.0044e-04 - accuracy: 0.9998 - val_loss: 0.7006 - val_accuracy: 0.8420\n",
      "Epoch 19/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.7172 - val_accuracy: 0.8397\n",
      "Epoch 20/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 8.4509e-04 - accuracy: 0.9998 - val_loss: 0.7411 - val_accuracy: 0.8313\n",
      "Epoch 21/50\n",
      "524/524 [==============================] - 4s 7ms/step - loss: 8.2911e-04 - accuracy: 0.9998 - val_loss: 0.7580 - val_accuracy: 0.8298\n",
      "Epoch 22/50\n",
      "524/524 [==============================] - 5s 9ms/step - loss: 8.6121e-04 - accuracy: 0.9998 - val_loss: 0.7708 - val_accuracy: 0.8267\n",
      "Epoch 23/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.7789 - val_accuracy: 0.8298\n",
      "Epoch 24/50\n",
      "524/524 [==============================] - 5s 10ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.7854 - val_accuracy: 0.8344\n",
      "Epoch 25/50\n",
      "524/524 [==============================] - 5s 9ms/step - loss: 7.0640e-04 - accuracy: 0.9998 - val_loss: 0.7904 - val_accuracy: 0.8344\n",
      "Epoch 26/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.8041 - val_accuracy: 0.8366\n",
      "Epoch 27/50\n",
      "524/524 [==============================] - 6s 11ms/step - loss: 7.6467e-04 - accuracy: 0.9998 - val_loss: 0.8171 - val_accuracy: 0.8344\n",
      "Epoch 28/50\n",
      "524/524 [==============================] - 8s 16ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.8316 - val_accuracy: 0.8359\n",
      "Epoch 29/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 7.4102e-04 - accuracy: 0.9998 - val_loss: 0.8295 - val_accuracy: 0.8328\n",
      "Epoch 30/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.8633 - val_accuracy: 0.8405\n",
      "Epoch 31/50\n",
      "524/524 [==============================] - 6s 12ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.8414 - val_accuracy: 0.8420\n",
      "Epoch 32/50\n",
      "524/524 [==============================] - 5s 9ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.8582 - val_accuracy: 0.8412\n",
      "Epoch 33/50\n",
      "524/524 [==============================] - 5s 9ms/step - loss: 7.4515e-04 - accuracy: 0.9998 - val_loss: 0.8791 - val_accuracy: 0.8359\n",
      "Epoch 34/50\n",
      "524/524 [==============================] - 5s 9ms/step - loss: 7.4653e-04 - accuracy: 0.9998 - val_loss: 0.8589 - val_accuracy: 0.8443\n",
      "Epoch 35/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.8703 - val_accuracy: 0.8374\n",
      "Epoch 36/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 9.3836e-04 - accuracy: 0.9994 - val_loss: 1.0329 - val_accuracy: 0.8244\n",
      "Epoch 37/50\n",
      "524/524 [==============================] - 4s 7ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.8863 - val_accuracy: 0.8382\n",
      "Epoch 38/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 6.7394e-04 - accuracy: 0.9998 - val_loss: 0.9952 - val_accuracy: 0.8244\n",
      "Epoch 39/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 8.7004e-04 - accuracy: 0.9998 - val_loss: 1.0313 - val_accuracy: 0.8260\n",
      "Epoch 40/50\n",
      "524/524 [==============================] - 6s 11ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.9006 - val_accuracy: 0.8389\n",
      "Epoch 41/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 7.5371e-04 - accuracy: 0.9998 - val_loss: 0.9083 - val_accuracy: 0.8351\n",
      "Epoch 42/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 6.4848e-04 - accuracy: 0.9998 - val_loss: 0.9270 - val_accuracy: 0.8374\n",
      "Epoch 43/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 8.4693e-04 - accuracy: 0.9998 - val_loss: 1.1729 - val_accuracy: 0.8115\n",
      "Epoch 44/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 1.1971 - val_accuracy: 0.8107\n",
      "Epoch 45/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 7.6540e-04 - accuracy: 0.9998 - val_loss: 0.9586 - val_accuracy: 0.8313\n",
      "Epoch 46/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.9954 - val_accuracy: 0.8305\n",
      "Epoch 47/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 7.1456e-04 - accuracy: 0.9998 - val_loss: 1.0483 - val_accuracy: 0.8282\n",
      "Epoch 48/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.9754 - val_accuracy: 0.8359\n",
      "Epoch 49/50\n",
      "524/524 [==============================] - 4s 8ms/step - loss: 6.7121e-04 - accuracy: 0.9998 - val_loss: 1.0040 - val_accuracy: 0.8313\n",
      "Epoch 50/50\n",
      "524/524 [==============================] - 4s 7ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 1.0982 - val_accuracy: 0.8206\n",
      "Training Accuracy: 0.9998\n",
      "Testing Accuracy:  0.8206\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
