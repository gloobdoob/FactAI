{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "domain = \"ajc.com\"\n",
    "\n",
    "def get_pagerank(domain):\n",
    "    headers = {'API-OPR':'4ok88wgckg8o0cgcswo4gc4kkc0sgocw0woww4o0'}\n",
    "    url = 'https://openpagerank.com/api/v1.0/getPageRank?domains%5B0%5D=' + domain\n",
    "    request = requests.get(url, headers=headers)\n",
    "    if request.status_code == 200:\n",
    "        result = request.json()\n",
    "        if result['status_code'] == 200:\n",
    "            pr_int = result['response'][0]['page_rank_integer']\n",
    "            pr_dec = result['response'][0]['page_rank_decimal']\n",
    "            rank = result['response'][0]['rank']\n",
    "\n",
    "            return pr_int, pr_dec, rank\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import whois\n",
    "\n",
    "def whois_checker(domain):\n",
    "    w = whois.whois(domain)\n",
    "    domain_registrar = w.get('registrar')\n",
    "    postal_code = w.get('registrant_postal_code')\n",
    "    country = w.get('country')\n",
    "\n",
    "    return  domain_registrar, postal_code, country\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import virustotal_python\n",
    "from pprint import pprint\n",
    "from base64 import urlsafe_b64encode\n",
    "import json\n",
    "\n",
    "\n",
    "def virus_scan(domain):\n",
    "    with virustotal_python.Virustotal(\"242305dedab935c6a39736d0749ae88f2b68f1e2f0c8e82c81d8955c5fd194d3\") as vtotal:\n",
    "        resp = vtotal.request(f\"domains/{domain}\")\n",
    "        virus_data = resp.json()['data']['attributes']['last_analysis_stats']\n",
    "        return virus_data['harmless'], virus_data['malicious'], virus_data['suspicious'], virus_data['undetected'], virus_data['timeout']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "#try except outside this func\n",
    "def link_checker(domain):\n",
    "    session = HTMLSession()\n",
    "\n",
    "    r = session.get(\"https://\"+domain)\n",
    "    unique_netlocs = Counter(urlparse(link).netloc for link in r.html.absolute_links)\n",
    "    outbound_n = 0\n",
    "    local_n = 0\n",
    "    links = []\n",
    "    for link in unique_netlocs:\n",
    "        # print(link, unique_netlocs[link])\n",
    "        links.append(link)\n",
    "        if domain.lower() in link:\n",
    "            local_n += unique_netlocs[link]\n",
    "        else:\n",
    "            outbound_n += unique_netlocs[link]\n",
    "\n",
    "    return local_n, outbound_n, local_n + outbound_n, links\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subscribe.ajc.com',\n",
       " 'www.ajc.com',\n",
       " 'ajc.zendesk.com',\n",
       " 'legislativenavigator.ajc.com',\n",
       " 'twitter.com',\n",
       " 'events.ajc.com',\n",
       " 'classifieds.ajc.com',\n",
       " 'live.ajc.com',\n",
       " 'editions.ajc.com',\n",
       " 'facebook.com',\n",
       " 'doctors.ajc.com',\n",
       " 'myaccount.ajc.com',\n",
       " 'play.google.com',\n",
       " 'ajc.com',\n",
       " 'www.facebook.com',\n",
       " 'www.pinterest.com',\n",
       " 'www.nieonline.com',\n",
       " 'apps.apple.com',\n",
       " 'jobs.coxenterprises.com',\n",
       " 'secure.qgiv.com',\n",
       " 'www.reddit.com',\n",
       " 'games.ajc.com']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "Options = Options()\n",
    "Options.headless = True \n",
    "\n",
    "domain = 'ralblaw.com'\n",
    "def check_login(domain, driver):\n",
    "    wp_xpath = \"//a[starts-with(@href, 'https://wordpress.org')]\"\n",
    "    try:\n",
    "        displayed = driver.find_element('xpath', wp_xpath).is_displayed()\n",
    "\n",
    "        return displayed\n",
    "    except:\n",
    "\n",
    "        return False\n",
    "\n",
    "def check_page_source(domain, driver):\n",
    "    try:\n",
    "        driver.get(f\"https://{domain}/wp-admin\")\n",
    "        wp_pg = \"wp-content\" in driver.page_source\n",
    "        return wp_pg\n",
    "    except:\n",
    "\n",
    "        return False\n",
    "\n",
    "def check_wp(domain):\n",
    "\n",
    "    service_obj = Service(executable_path=\"C:/Users/cvaal/chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(options=Options, service=service_obj)\n",
    "    driver.implicitly_wait(1)\n",
    "    driver.get(f\"https://{domain}/wp-admin\")\n",
    "    login = check_login(domain, driver)\n",
    "    pg_source = check_page_source(domain, driver)\n",
    "    wp = None\n",
    "    if login == False and pg_source == False:\n",
    "        wp = False\n",
    "    else:\n",
    "        wp = True\n",
    "\n",
    "    driver.close()\n",
    "    return wp\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FakeNews_URLs.txt', 'r') as f:\n",
    "    fakenews_sites = f.read().split('\\n')\n",
    "    fakenews_sites = [site.strip() for site in fakenews_sites]\n",
    "\n",
    "\n",
    "def get_site_data(domain):\n",
    "    if requests.head('http://'+domain, allow_redirects=True).status_code == 200:\n",
    "        pr_int, pr_dec, rank = get_pagerank(domain)\n",
    "        domain_registrar, postal_code, country = whois_checker(domain)\n",
    "        harmless, malicious, suspicious, undetected, timeout = virus_scan(domain)\n",
    "        local_n, outbound_links, total_links, links = link_checker(domain)\n",
    "        wp_check = check_wp(domain)\n",
    "\n",
    "        return domain, pr_int, pr_dec, rank, domain_registrar, postal_code, country, harmless, malicious, suspicious, undetected, timeout, local_n, outbound_links, total_links, links, wp_check \n",
    "    else:\n",
    "        return 'Site down'\n",
    "\n",
    "# domain = 'reddit.com'\n",
    "\n",
    "columns = ['Domain', 'Page rank integer', 'Page rank decimal', 'Site Rank', 'Domain registrar', 'Postal code', 'Country of origin', 'Harmless', 'Malicious', 'Suspicious', 'Undetected', 'Timeout', 'No. of Local links', 'No. of Outbound links', 'Total links', 'Links to fake news site?', 'Wordpress?']\n",
    "\n",
    "site_df_dict = {}\n",
    "\n",
    "for i, domain in enumerate(fakenews_sites):\n",
    "    if i == 3:\n",
    "        break\n",
    "    \n",
    "    site_data = get_site_data(domain)\n",
    "    if type(site_data) == tuple:\n",
    "        for column, data in zip(columns, site_data):\n",
    "            if type(data) != list:\n",
    "                site_df_dict[column] = data\n",
    "            else:\n",
    "                if any(site in data for site in fakenews_sites):\n",
    "                    site_df_dict[column] = True\n",
    "                else:\n",
    "                    site_df_dict[column] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Site down'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a51e5855e04901d00966621b96dcfa06fb6a1bfd2a37e82f444787cca980996"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
