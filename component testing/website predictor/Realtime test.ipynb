{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cvaal\\AppData\\Local\\Temp\\ipykernel_16352\\3156865653.py:20: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n",
      "  options.headless = True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import whois\n",
    "import virustotal_python\n",
    "from requests_html import HTMLSession\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from urllib3 import disable_warnings\n",
    "\n",
    "disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    " \n",
    "from collections import defaultdict\n",
    "from urllib.request import urlopen, URLError\n",
    "import socket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'reddit.com'\n",
    "\n",
    "def get_pagerank(domain):\n",
    "    headers = {'API-OPR':'4ok88wgckg8o0cgcswo4gc4kkc0sgocw0woww4o0'}\n",
    "    url = 'https://openpagerank.com/api/v1.0/getPageRank?domains%5B0%5D=' + domain\n",
    "    request = requests.get(url, headers=headers)\n",
    "    if request.status_code == 200:\n",
    "        result = request.json()\n",
    "        if result['status_code'] == 200:\n",
    "            pr_dec = result['response'][0]['page_rank_decimal']\n",
    "            rank = result['response'][0]['rank']\n",
    "\n",
    "            return pr_dec, rank\n",
    "\n",
    "def whois_checker(domain):\n",
    "    w = whois.whois(domain)\n",
    "    domain_registrar = w.get('registrar')\n",
    "    postal_code = w.get('registrant_postal_code')\n",
    "    country = w.get('country')\n",
    "\n",
    "    return  domain_registrar, postal_code, country\n",
    "\n",
    "def virus_scan(domain):\n",
    "    with virustotal_python.Virustotal(\"242305dedab935c6a39736d0749ae88f2b68f1e2f0c8e82c81d8955c5fd194d3\") as vtotal:\n",
    "        resp = vtotal.request(f\"domains/{domain}\")\n",
    "        virus_data = resp.json()['data']['attributes']['last_analysis_stats']\n",
    "        return virus_data['malicious']\n",
    "\n",
    "\n",
    "def link_checker(domain):\n",
    "    session = HTMLSession()\n",
    "    \n",
    "    r = session.get(\"https://\"+domain, verify=False)\n",
    "    unique_netlocs = Counter(urlparse(link).netloc for link in r.html.absolute_links)\n",
    "    outbound_n = 0\n",
    "    local_n = 0\n",
    "\n",
    "    for link in unique_netlocs:\n",
    "        # print(link, unique_netlocs[link])\n",
    "        if domain.lower() in link:\n",
    "            local_n += unique_netlocs[link]\n",
    "        else:\n",
    "            outbound_n += unique_netlocs[link]\n",
    "\n",
    "    return local_n, local_n + outbound_n\n",
    "\n",
    "\n",
    "\n",
    "def check_login(domain, driver):\n",
    "    wp_xpath = \"//a[starts-with(@href, 'https://wordpress.org')]\"\n",
    "    try:\n",
    "        displayed = driver.find_element('xpath', wp_xpath).is_displayed()\n",
    "\n",
    "        return displayed\n",
    "    except:\n",
    "\n",
    "        return False\n",
    "\n",
    "def check_page_source(domain, driver):\n",
    "    try:\n",
    "        driver.get(f\"https://{domain}/wp-admin\")\n",
    "        wp_pg = \"wp-content\" in driver.page_source\n",
    "        return wp_pg\n",
    "    except:\n",
    "\n",
    "        return False\n",
    "\n",
    "def check_wp(domain):\n",
    "\n",
    "    service_obj = Service(executable_path=\"C:/Users/cvaal/chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(options=options, service=service_obj)\n",
    "    driver.implicitly_wait(1)\n",
    "    driver.get(f\"https://{domain}/wp-admin\")\n",
    "    login = check_login(domain, driver)\n",
    "    pg_source = check_page_source(domain, driver)\n",
    "    wp = None\n",
    "    if login == False and pg_source == False:\n",
    "        wp = False\n",
    "    else:\n",
    "        wp = True\n",
    "\n",
    "    driver.close()\n",
    "    return wp\n",
    "\n",
    "\n",
    "def check_url( url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'}\n",
    "    response = requests.get('https://'+url, timeout=5, headers=headers)\n",
    "   \n",
    "    return response.status_code\n",
    "\n",
    "\n",
    "def get_site_data(domain):\n",
    "    # try:\n",
    "    # print(check_url(domain))\n",
    "    # if check_url(domain) == True:\n",
    "    # try:\n",
    "    pr_dec, rank = get_pagerank(domain)\n",
    "    domain_registrar, postal_code, country = whois_checker(domain)\n",
    "    malicious = virus_scan(domain)\n",
    "    local_n, total_links= link_checker(domain)\n",
    "    wp_check = check_wp(domain)\n",
    "    return domain, pr_dec, rank, domain_registrar, postal_code, country, malicious, local_n, total_links, wp_check\n",
    "# except:\n",
    "    #     return('site down')\n",
    "    # else:\n",
    "    #     return 'Site down'\n",
    "    # # except:\n",
    "    # #     return 'Site down'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stackoverflow.com'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "urlparse('https://stackoverflow.com/questions/44113335/extract-domain-from-url-in-python').netloc.strip(\"www.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FakeNews_URLs.txt', 'r') as f:\n",
    "    fakenews_sites = f.read().split('\\n')\n",
    "    fakenews_sites = [site.strip() for site in fakenews_sites]\n",
    "\n",
    "\n",
    "def get_and_organize_data(domain_list):\n",
    "    columns = ['Domain', 'Page rank decimal', 'Site Rank', 'Domain registrar', 'Postal code', 'Country of origin', 'Malicious', 'No. of Local links','Total links','Wordpress?']\n",
    "    site_df_dict = defaultdict(list)\n",
    "\n",
    "    for domain in domain_list:\n",
    "        if 'http' in domain:\n",
    "            domain = urlparse(domain).netloc.strip(\"www.\")\n",
    "        print(domain)\n",
    "        try:\n",
    "            site_data = get_site_data(domain.lower())\n",
    "            if type(site_data) == tuple:\n",
    "                for column, data in zip(columns, site_data):\n",
    "                    site_df_dict[column].append(data)                \n",
    "            else:\n",
    "                print('site down')\n",
    "        except:\n",
    "            print('site down')\n",
    "            continue\n",
    "            \n",
    "\n",
    "    return pd.DataFrame(site_df_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;standard_scaler&#x27;,\n",
       "                                                  StandardScaler(),\n",
       "                                                  [&#x27;Page rank decimal&#x27;,\n",
       "                                                   &#x27;Site Rank&#x27;,\n",
       "                                                   &#x27;Total links&#x27;]),\n",
       "                                                 (&#x27;one_hot_encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse=False),\n",
       "                                                  [&#x27;Postal code&#x27;,\n",
       "                                                   &#x27;Country of origin&#x27;]),\n",
       "                                                 (&#x27;label_encoder&#x27;,\n",
       "                                                  OrdinalEncoder(),\n",
       "                                                  [&#x27;Wordpress?&#x27;,\n",
       "                                                   &#x27;Malicious&#x27;])])),\n",
       "                (&#x27;XGBoost&#x27;,\n",
       "                 XGBClassifier(base_score...\n",
       "                               gamma=0.3, gpu_id=-1, importance_type=None,\n",
       "                               interaction_constraints=&#x27;&#x27;, learning_rate=0.01,\n",
       "                               max_delta_step=0, max_depth=3, min_child_weigh=3,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "                               n_jobs=6, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=1, subsample=1,\n",
       "                               tree_method=&#x27;exact&#x27;, validate_parameters=1,\n",
       "                               verbosity=None))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;standard_scaler&#x27;,\n",
       "                                                  StandardScaler(),\n",
       "                                                  [&#x27;Page rank decimal&#x27;,\n",
       "                                                   &#x27;Site Rank&#x27;,\n",
       "                                                   &#x27;Total links&#x27;]),\n",
       "                                                 (&#x27;one_hot_encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse=False),\n",
       "                                                  [&#x27;Postal code&#x27;,\n",
       "                                                   &#x27;Country of origin&#x27;]),\n",
       "                                                 (&#x27;label_encoder&#x27;,\n",
       "                                                  OrdinalEncoder(),\n",
       "                                                  [&#x27;Wordpress?&#x27;,\n",
       "                                                   &#x27;Malicious&#x27;])])),\n",
       "                (&#x27;XGBoost&#x27;,\n",
       "                 XGBClassifier(base_score...\n",
       "                               gamma=0.3, gpu_id=-1, importance_type=None,\n",
       "                               interaction_constraints=&#x27;&#x27;, learning_rate=0.01,\n",
       "                               max_delta_step=0, max_depth=3, min_child_weigh=3,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "                               n_jobs=6, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=1, subsample=1,\n",
       "                               tree_method=&#x27;exact&#x27;, validate_parameters=1,\n",
       "                               verbosity=None))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;standard_scaler&#x27;, StandardScaler(),\n",
       "                                 [&#x27;Page rank decimal&#x27;, &#x27;Site Rank&#x27;,\n",
       "                                  &#x27;Total links&#x27;]),\n",
       "                                (&#x27;one_hot_encoder&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                               sparse=False),\n",
       "                                 [&#x27;Postal code&#x27;, &#x27;Country of origin&#x27;]),\n",
       "                                (&#x27;label_encoder&#x27;, OrdinalEncoder(),\n",
       "                                 [&#x27;Wordpress?&#x27;, &#x27;Malicious&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">standard_scaler</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Page rank decimal&#x27;, &#x27;Site Rank&#x27;, &#x27;Total links&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">one_hot_encoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Postal code&#x27;, &#x27;Country of origin&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">label_encoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Wordpress?&#x27;, &#x27;Malicious&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OrdinalEncoder</label><div class=\"sk-toggleable__content\"><pre>OrdinalEncoder()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.4,\n",
       "              enable_categorical=False, gamma=0.3, gpu_id=-1,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
       "              min_child_weigh=3, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=6,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=&#x27;exact&#x27;, validate_parameters=1, verbosity=None)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('standard_scaler',\n",
       "                                                  StandardScaler(),\n",
       "                                                  ['Page rank decimal',\n",
       "                                                   'Site Rank',\n",
       "                                                   'Total links']),\n",
       "                                                 ('one_hot_encoder',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                sparse=False),\n",
       "                                                  ['Postal code',\n",
       "                                                   'Country of origin']),\n",
       "                                                 ('label_encoder',\n",
       "                                                  OrdinalEncoder(),\n",
       "                                                  ['Wordpress?',\n",
       "                                                   'Malicious'])])),\n",
       "                ('XGBoost',\n",
       "                 XGBClassifier(base_score...\n",
       "                               gamma=0.3, gpu_id=-1, importance_type=None,\n",
       "                               interaction_constraints='', learning_rate=0.01,\n",
       "                               max_delta_step=0, max_depth=3, min_child_weigh=3,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=6, num_parallel_tree=1, predictor='auto',\n",
       "                               random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                               scale_pos_weight=1, subsample=1,\n",
       "                               tree_method='exact', validate_parameters=1,\n",
       "                               verbosity=None))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "ct = joblib.load('SitePreprocessor.joblib')\n",
    "clf = joblib.load('XGBoostClassifier.joblib')\n",
    "\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "\n",
    "ct = joblib.load('SitePreprocessor.joblib')\n",
    "clf = joblib.load('XGBoostClassifier.joblib')\n",
    "\n",
    "def predict(domain_list):\n",
    "    print('scanning sites:' , domain_list)\n",
    "    data = get_and_organize_data(domain_list) #dataframe obj\n",
    "    print(data)\n",
    "    domains = data['Domain'].values\n",
    "    features_df = data.drop('Domain', axis=1)\n",
    "    \n",
    "    \n",
    "    preds = clf.predict(features_df)\n",
    "    predictions = [round(value) for value in preds]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "#idea: create instance of organizing object everytime the system is ran, collects all the links and returns a full dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FilipinoRealNews.txt', 'r') as f:\n",
    "    fil_real_sites = f.read().split('\\n')\n",
    "    fil_real_sites= [site.strip() for site in fil_real_sites]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict(fil_real_sites[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_fake_sites = ['https://balita.definitelyfilipino.com', 'https://pinoytrendingnews.net', 'https://www.thedailysentry.net', 'https://brigadanews.ph', 'https://bandera.inquirer.net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning sites: ['https://balita.definitelyfilipino.com', 'https://pinoytrendingnews.net', 'https://www.thedailysentry.net', 'https://brigadanews.ph', 'https://bandera.inquirer.net']\n",
      "balita.definitelyfilipino.com\n",
      "pinoytrendingnews.net\n",
      "thedailysentry.net\n",
      "brigadanews.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "bandera.inquirer.net\n",
      "                          Domain  Page rank decimal Site Rank   \n",
      "0  balita.definitelyfilipino.com               2.83  34229397  \\\n",
      "1          pinoytrendingnews.net               3.15  15781332   \n",
      "2             thedailysentry.net               4.10   3340028   \n",
      "3                 brigadanews.ph               3.29  11566328   \n",
      "4           bandera.inquirer.net               4.91    445655   \n",
      "\n",
      "         Domain registrar Postal code Country of origin  Malicious   \n",
      "0         Domain.com, LLC       32256                US          0  \\\n",
      "1           NAMECHEAP INC         101                IS          0   \n",
      "2           NAMECHEAP INC         101                IS          0   \n",
      "3                    None        None              None          0   \n",
      "4  Network Solutions, LLC        None              None          0   \n",
      "\n",
      "   No. of Local links  Total links  Wordpress?  \n",
      "0                  31           33        True  \n",
      "1                  19           20        True  \n",
      "2                  26           66       False  \n",
      "3                  99          107        True  \n",
      "4                  31           32        True  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(fil_fake_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning sites: reddit.com\n",
      "r\n",
      "site down\n",
      "e\n",
      "site down\n",
      "d\n",
      "site down\n",
      "d\n",
      "site down\n",
      "i\n",
      "site down\n",
      "t\n",
      "site down\n",
      ".\n",
      "site down\n",
      "c\n",
      "site down\n",
      "o\n",
      "site down\n",
      "m\n",
      "site down\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Domain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predict(\u001b[39m'\u001b[39;49m\u001b[39mreddit.com\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(domain_list)\u001b[0m\n\u001b[0;32m     18\u001b[0m data \u001b[39m=\u001b[39m get_and_organize_data(domain_list) \u001b[39m#dataframe obj\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(data)\n\u001b[1;32m---> 20\u001b[0m domains \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39mDomain\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m     21\u001b[0m features_df \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mDomain\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m preds \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(features_df)\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\pt-tf-py3.10\\lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\cvaal\\anaconda3\\envs\\pt-tf-py3.10\\lib\\site-packages\\pandas\\core\\indexes\\range.py:349\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m    350\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    351\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Domain'"
     ]
    }
   ],
   "source": [
    "predict('reddit.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning:businessmirror.com.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "('businessmirror.com.ph', 5.6, '16363', None, None, None, 0, 457, 465, True)\n",
      "<class 'numpy.ndarray'>\n",
      "businessmirror.com.ph : reliable site : 1\n",
      "scanning:bworldonline.com\n",
      "('bworldonline.com', 5.38, '29569', 'Network Solutions, LLC', None, None, 0, 151, 168, False)\n",
      "<class 'numpy.ndarray'>\n",
      "bworldonline.com : reliable site : 1\n",
      "scanning:tribune.net.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "('tribune.net.ph', 5.15, '78947', None, None, None, 0, 149, 157, True)\n",
      "<class 'numpy.ndarray'>\n",
      "tribune.net.ph : reliable site : 1\n",
      "scanning:malaya.com.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "('malaya.com.ph', 5.06, '124301', None, None, None, 0, 260, 265, True)\n",
      "<class 'numpy.ndarray'>\n",
      "malaya.com.ph : reliable site : 1\n",
      "scanning:mb.com.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "('mb.com.ph', 5.48, '22037', None, None, None, 0, 33, 34, False)\n",
      "<class 'numpy.ndarray'>\n",
      "mb.com.ph : reliable site : 1\n",
      "scanning:manilastandard.net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Service.__del__ at 0x000001E7DA9C76D0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\cvaal\\anaconda3\\envs\\pt-tf-py3.10\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 184, in __del__\n",
      "    self.stop()\n",
      "  File \"c:\\Users\\cvaal\\anaconda3\\envs\\pt-tf-py3.10\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 145, in stop\n",
      "    self.send_remote_shutdown_command()\n",
      "  File \"c:\\Users\\cvaal\\anaconda3\\envs\\pt-tf-py3.10\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 129, in send_remote_shutdown_command\n",
      "    if not self.is_connectable():\n",
      "  File \"c:\\Users\\cvaal\\anaconda3\\envs\\pt-tf-py3.10\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\", line 118, in is_connectable\n",
      "    return utils.is_connectable(self.port)\n",
      "  File \"c:\\Users\\cvaal\\anaconda3\\envs\\pt-tf-py3.10\\lib\\site-packages\\selenium\\webdriver\\common\\utils.py\", line 102, in is_connectable\n",
      "    socket_ = socket.create_connection((host, port), 1)\n",
      "  File \"c:\\Users\\cvaal\\anaconda3\\envs\\pt-tf-py3.10\\lib\\socket.py\", line 833, in create_connection\n",
      "    sock.connect(sa)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('manilastandard.net', 5.22, '58179', 'DOMAINPEOPLE, INC.', 'REDACTED FOR PRIVACY', 'PH', 0, 323, 334, True)\n",
      "<class 'numpy.ndarray'>\n",
      "manilastandard.net : reliable site : 1\n",
      "scanning:manilatimes.net\n",
      "('manilatimes.net', 5.74, '13046', 'Network Solutions, LLC', None, None, 0, 77, 96, False)\n",
      "<class 'numpy.ndarray'>\n",
      "manilatimes.net : reliable site : 1\n",
      "scanning:journal.com.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "('journal.com.ph', 5.07, '117292', None, None, None, 0, 125, 128, True)\n",
      "<class 'numpy.ndarray'>\n",
      "journal.com.ph : reliable site : 1\n",
      "scanning:philstar.com\n",
      "('philstar.com', 5.63, '15414', 'Network Solutions, LLC', None, None, 0, 196, 272, False)\n",
      "<class 'numpy.ndarray'>\n",
      "philstar.com : reliable site : 1\n",
      "scanning:officialgazette.gov.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "('officialgazette.gov.ph', 5.27, '46290', None, None, None, 0, 71, 85, True)\n",
      "<class 'numpy.ndarray'>\n",
      "officialgazette.gov.ph : Unreliable site : 0\n",
      "scanning:verafiles.org\n",
      "('verafiles.org', 5.01, '172118', 'NameCheap, Inc.', 'REDACTED FOR PRIVACY', 'IS', 0, 102, 112, False)\n",
      "<class 'numpy.ndarray'>\n",
      "verafiles.org : reliable site : 1\n",
      "scanning:gmanetwork.com\n",
      "('gmanetwork.com', 5.81, '11929', 'Network Solutions, LLC', None, None, 0, 0, 0, False)\n",
      "<class 'numpy.ndarray'>\n",
      "gmanetwork.com : reliable site : 1\n",
      "scanning:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m site \u001b[39min\u001b[39;00m fil_real_sites:\n\u001b[1;32m----> 2\u001b[0m     predict(site)\n",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(domain)\u001b[0m\n\u001b[0;32m     18\u001b[0m     domain \u001b[39m=\u001b[39m urlparse(domain)\u001b[39m.\u001b[39mnetloc\u001b[39m.\u001b[39mstrip(\u001b[39m\"\u001b[39m\u001b[39mwww.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mscanning:\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m domain)\n\u001b[1;32m---> 21\u001b[0m data \u001b[39m=\u001b[39m get_and_organize_data(domain) \u001b[39m#dataframe obj\u001b[39;00m\n\u001b[0;32m     22\u001b[0m domains \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mDomain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m     23\u001b[0m features_df \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mDomain\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36mget_and_organize_data\u001b[1;34m(domain)\u001b[0m\n\u001b[0;32m      7\u001b[0m columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mDomain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPage rank decimal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSite Rank\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDomain registrar\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPostal code\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCountry of origin\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMalicious\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNo. of Local links\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mTotal links\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mWordpress?\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m site_df_dict \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m site_data \u001b[39m=\u001b[39m get_site_data(domain\u001b[39m.\u001b[39;49mlower())\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(site_data)\n\u001b[0;32m     11\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(site_data) \u001b[39m==\u001b[39m \u001b[39mtuple\u001b[39m:\n",
      "Cell \u001b[1;32mIn[2], line 98\u001b[0m, in \u001b[0;36mget_site_data\u001b[1;34m(domain)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_site_data\u001b[39m(domain):\n\u001b[0;32m     94\u001b[0m     \u001b[39m# try:\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[39m# print(check_url(domain))\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[39m# if check_url(domain) == True:\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[39m# try:\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     pr_dec, rank \u001b[39m=\u001b[39m get_pagerank(domain)\n\u001b[0;32m     99\u001b[0m     domain_registrar, postal_code, country \u001b[39m=\u001b[39m whois_checker(domain)\n\u001b[0;32m    100\u001b[0m     malicious \u001b[39m=\u001b[39m virus_scan(domain)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "for site in fil_real_sites:\n",
    "    predict(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning:officialgazette.gov.ph\n",
      "Error trying to connect to socket: closing socket - [Errno 11001] getaddrinfo failed\n",
      "('officialgazette.gov.ph', 4.58, '62421', None, None, None, 0, 59, 72, False)\n",
      "officialgazette.gov.ph : Unreliable site\n"
     ]
    }
   ],
   "source": [
    "predict('https://www.officialgazette.gov.ph/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-93084fb2a806>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msite_df_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "site_df_dict = defaultdict(list)\n",
    "\n",
    "for rating, domains in site_dict.items():\n",
    "    for i, domain in enumerate(domains):\n",
    "        print(f'Analyzing Site {i+1}: {domain} of rating: {rating}')\n",
    "        site_data = get_site_data(domain.lower(), rating)\n",
    "        if type(site_data) == tuple:\n",
    "            for column, data in zip(columns, site_data):\n",
    "                if type(data) != list:\n",
    "                    site_df_dict[column].append(data)\n",
    "                else:\n",
    "                    if any(site in data for site in fakenews_sites):\n",
    "                        site_df_dict[column].append(True)\n",
    "                    else:\n",
    "                        site_df_dict[column].append(False)\n",
    "            print('Data Gathered')\n",
    "        else:\n",
    "            print('site down')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a51e5855e04901d00966621b96dcfa06fb6a1bfd2a37e82f444787cca980996"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
